{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'non-COVID']\n",
      "{'train': 1985, 'test': 497}\n",
      "cuda:1\n",
      "{0: 'COVID', 1: 'non-COVID'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19_CT'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(240),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 50\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14863946\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "                                 #('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.11825432701100046, Val accuracy: 0.9738430583501007\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.12331190 Acc: 0.95365239\n",
      "test Loss: 0.12487701 Acc: 0.97183099\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.13372765 Acc: 0.93803526\n",
      "test Loss: 0.12339640 Acc: 0.97585513\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.12777692 Acc: 0.94256927\n",
      "test Loss: 0.13130460 Acc: 0.97384306\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.13213252 Acc: 0.93954660\n",
      "test Loss: 0.12756284 Acc: 0.97384306\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.12579533 Acc: 0.94659950\n",
      "test Loss: 0.13468619 Acc: 0.97183099\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.13209418 Acc: 0.94911839\n",
      "test Loss: 0.11986892 Acc: 0.97384306\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.11610619 Acc: 0.95465995\n",
      "test Loss: 0.11738393 Acc: 0.97384306\n",
      "New best model found!\n",
      "New record loss: 0.1173839307684354, previous record loss: 0.11825432701100046\n",
      "New record loss is SAVED: 0.1173839307684354\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.12946132 Acc: 0.94408060\n",
      "test Loss: 0.12980268 Acc: 0.97183099\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.12988715 Acc: 0.94659950\n",
      "test Loss: 0.11984058 Acc: 0.97384306\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.12177649 Acc: 0.95062972\n",
      "test Loss: 0.11983230 Acc: 0.97384306\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.12679641 Acc: 0.94962217\n",
      "test Loss: 0.12790270 Acc: 0.97183099\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.13005638 Acc: 0.94659950\n",
      "test Loss: 0.12710159 Acc: 0.97384306\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.12201719 Acc: 0.94861461\n",
      "test Loss: 0.11895507 Acc: 0.97384306\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.13568367 Acc: 0.94256927\n",
      "test Loss: 0.12024564 Acc: 0.97384306\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.14870080 Acc: 0.94055416\n",
      "test Loss: 0.12169384 Acc: 0.97384306\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.11863308 Acc: 0.95264484\n",
      "test Loss: 0.12411790 Acc: 0.97384306\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.13074323 Acc: 0.94609572\n",
      "test Loss: 0.12347361 Acc: 0.97384306\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.12063568 Acc: 0.95264484\n",
      "test Loss: 0.12539772 Acc: 0.97384306\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.12946863 Acc: 0.94256927\n",
      "test Loss: 0.12391382 Acc: 0.97384306\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.13707441 Acc: 0.93702771\n",
      "test Loss: 0.12633989 Acc: 0.97384306\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.14065399 Acc: 0.94357683\n",
      "test Loss: 0.12115468 Acc: 0.97384306\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.11817646 Acc: 0.94911839\n",
      "test Loss: 0.12361180 Acc: 0.97384306\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.13280224 Acc: 0.94357683\n",
      "test Loss: 0.13059428 Acc: 0.97384306\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.12814564 Acc: 0.94659950\n",
      "test Loss: 0.13393051 Acc: 0.97384306\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.12617620 Acc: 0.94408060\n",
      "test Loss: 0.12755961 Acc: 0.97384306\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.14719407 Acc: 0.93551637\n",
      "test Loss: 0.12170157 Acc: 0.97384306\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.13417746 Acc: 0.94508816\n",
      "test Loss: 0.12306948 Acc: 0.97384306\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.14291759 Acc: 0.93904282\n",
      "test Loss: 0.12794329 Acc: 0.97384306\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.13079455 Acc: 0.94659950\n",
      "test Loss: 0.13000577 Acc: 0.97786720\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.13829669 Acc: 0.93954660\n",
      "test Loss: 0.11779933 Acc: 0.97384306\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.12922823 Acc: 0.95062972\n",
      "test Loss: 0.12251754 Acc: 0.97183099\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.14588740 Acc: 0.94256927\n",
      "test Loss: 0.12505524 Acc: 0.97384306\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.13265418 Acc: 0.94458438\n",
      "test Loss: 0.12200852 Acc: 0.97384306\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.12388363 Acc: 0.94911839\n",
      "test Loss: 0.12037471 Acc: 0.97384306\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.14853529 Acc: 0.94156171\n",
      "test Loss: 0.11981430 Acc: 0.97585513\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.11666919 Acc: 0.95717884\n",
      "test Loss: 0.12262361 Acc: 0.97384306\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.13358375 Acc: 0.94559194\n",
      "test Loss: 0.12436905 Acc: 0.97384306\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.13383877 Acc: 0.94559194\n",
      "test Loss: 0.12274597 Acc: 0.97384306\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.15043971 Acc: 0.93753149\n",
      "test Loss: 0.12504414 Acc: 0.97384306\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.12769589 Acc: 0.94408060\n",
      "test Loss: 0.12557188 Acc: 0.97384306\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.13037756 Acc: 0.95264484\n",
      "test Loss: 0.12894293 Acc: 0.97585513\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.12967177 Acc: 0.94760705\n",
      "test Loss: 0.12025066 Acc: 0.97384306\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.14404615 Acc: 0.94307305\n",
      "test Loss: 0.12840939 Acc: 0.97183099\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.14153390 Acc: 0.94156171\n",
      "test Loss: 0.12004775 Acc: 0.97384306\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.11738149 Acc: 0.95264484\n",
      "test Loss: 0.12568714 Acc: 0.97384306\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.13345509 Acc: 0.94307305\n",
      "test Loss: 0.12427967 Acc: 0.97384306\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.13423864 Acc: 0.94156171\n",
      "test Loss: 0.12212838 Acc: 0.97585513\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.13286942 Acc: 0.94710327\n",
      "test Loss: 0.12005238 Acc: 0.97384306\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.13158893 Acc: 0.94458438\n",
      "test Loss: 0.12764603 Acc: 0.97786720\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.13490640 Acc: 0.94005038\n",
      "test Loss: 0.11719522 Acc: 0.97384306\n",
      "New best model found!\n",
      "New record loss: 0.11719522414582836, previous record loss: 0.1173839307684354\n",
      "New record loss is SAVED: 0.11719522414582836\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.12571703 Acc: 0.94811083\n",
      "test Loss: 0.12314570 Acc: 0.97384306\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.12572624 Acc: 0.95062972\n",
      "test Loss: 0.12822662 Acc: 0.97585513\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.12059032 Acc: 0.95012594\n",
      "test Loss: 0.12055747 Acc: 0.97183099\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.12063684 Acc: 0.95113350\n",
      "test Loss: 0.12655126 Acc: 0.97384306\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.13514925 Acc: 0.94710327\n",
      "test Loss: 0.12107990 Acc: 0.97384306\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.13463786 Acc: 0.94811083\n",
      "test Loss: 0.12425966 Acc: 0.97183099\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.12823422 Acc: 0.94659950\n",
      "test Loss: 0.12770676 Acc: 0.97585513\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.11622601 Acc: 0.95012594\n",
      "test Loss: 0.12099155 Acc: 0.97384306\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.12552732 Acc: 0.95062972\n",
      "test Loss: 0.12862778 Acc: 0.97183099\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.12170481 Acc: 0.94911839\n",
      "test Loss: 0.12496808 Acc: 0.97384306\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.13059414 Acc: 0.94458438\n",
      "test Loss: 0.13253311 Acc: 0.97384306\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.11435054 Acc: 0.94861461\n",
      "test Loss: 0.12424423 Acc: 0.97384306\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.12892488 Acc: 0.94508816\n",
      "test Loss: 0.12468488 Acc: 0.97384306\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.13381098 Acc: 0.94609572\n",
      "test Loss: 0.12519860 Acc: 0.97384306\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.12581305 Acc: 0.95415617\n",
      "test Loss: 0.12340129 Acc: 0.97384306\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.13379861 Acc: 0.94256927\n",
      "test Loss: 0.12912102 Acc: 0.97384306\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.12616637 Acc: 0.94760705\n",
      "test Loss: 0.12129669 Acc: 0.97384306\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.12016451 Acc: 0.95163728\n",
      "test Loss: 0.12735888 Acc: 0.97384306\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.12539787 Acc: 0.95465995\n",
      "test Loss: 0.12336862 Acc: 0.97384306\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.11849898 Acc: 0.94861461\n",
      "test Loss: 0.12355957 Acc: 0.97384306\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.14191065 Acc: 0.93904282\n",
      "test Loss: 0.12363665 Acc: 0.97384306\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.14523742 Acc: 0.94307305\n",
      "test Loss: 0.12750001 Acc: 0.97384306\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.13392954 Acc: 0.94105793\n",
      "test Loss: 0.13062797 Acc: 0.97384306\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.12421554 Acc: 0.94861461\n",
      "test Loss: 0.12997415 Acc: 0.97384306\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.12206724 Acc: 0.94962217\n",
      "test Loss: 0.12638919 Acc: 0.97384306\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.13464462 Acc: 0.94357683\n",
      "test Loss: 0.12633685 Acc: 0.97384306\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.14770552 Acc: 0.94256927\n",
      "test Loss: 0.13403044 Acc: 0.97183099\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.14950433 Acc: 0.93450882\n",
      "test Loss: 0.12013696 Acc: 0.97384306\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.12753355 Acc: 0.94811083\n",
      "test Loss: 0.13020646 Acc: 0.97384306\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.14787050 Acc: 0.93904282\n",
      "test Loss: 0.13215257 Acc: 0.97384306\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.11630511 Acc: 0.94911839\n",
      "test Loss: 0.13086130 Acc: 0.97384306\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.12895970 Acc: 0.94609572\n",
      "test Loss: 0.12531953 Acc: 0.97384306\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.13215793 Acc: 0.93954660\n",
      "test Loss: 0.12277368 Acc: 0.97384306\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.13470492 Acc: 0.93954660\n",
      "test Loss: 0.12139728 Acc: 0.97384306\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.12750811 Acc: 0.94710327\n",
      "test Loss: 0.12734708 Acc: 0.97384306\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.11761428 Acc: 0.94508816\n",
      "test Loss: 0.12432819 Acc: 0.97384306\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.11990110 Acc: 0.95012594\n",
      "test Loss: 0.12360362 Acc: 0.97183099\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.12771764 Acc: 0.95062972\n",
      "test Loss: 0.12007781 Acc: 0.97585513\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.12577346 Acc: 0.94911839\n",
      "test Loss: 0.11989373 Acc: 0.97384306\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.13025570 Acc: 0.94508816\n",
      "test Loss: 0.11890332 Acc: 0.97384306\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.13869983 Acc: 0.94760705\n",
      "test Loss: 0.13038306 Acc: 0.97183099\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.14077286 Acc: 0.94156171\n",
      "test Loss: 0.12823088 Acc: 0.97585513\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.13207162 Acc: 0.94357683\n",
      "test Loss: 0.12439686 Acc: 0.97384306\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.14123382 Acc: 0.94055416\n",
      "test Loss: 0.12790151 Acc: 0.97987928\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.14485684 Acc: 0.93602015\n",
      "test Loss: 0.12700256 Acc: 0.97585513\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.12833475 Acc: 0.94458438\n",
      "test Loss: 0.12916237 Acc: 0.97786720\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.13123970 Acc: 0.94962217\n",
      "test Loss: 0.12394890 Acc: 0.97384306\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.12731727 Acc: 0.94357683\n",
      "test Loss: 0.12645895 Acc: 0.97384306\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.13319429 Acc: 0.94055416\n",
      "test Loss: 0.12262921 Acc: 0.97585513\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.12884929 Acc: 0.94307305\n",
      "test Loss: 0.12913159 Acc: 0.97384306\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.13867382 Acc: 0.94307305\n",
      "test Loss: 0.12851612 Acc: 0.97384306\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.12674664 Acc: 0.94609572\n",
      "test Loss: 0.12415997 Acc: 0.97384306\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.12515257 Acc: 0.94811083\n",
      "test Loss: 0.12695812 Acc: 0.97384306\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.12887586 Acc: 0.94710327\n",
      "test Loss: 0.13166311 Acc: 0.97183099\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.13269828 Acc: 0.94659950\n",
      "test Loss: 0.12623500 Acc: 0.97183099\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.13133679 Acc: 0.94760705\n",
      "test Loss: 0.12364407 Acc: 0.97384306\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.13398523 Acc: 0.94559194\n",
      "test Loss: 0.12216133 Acc: 0.97384306\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.13386220 Acc: 0.94710327\n",
      "test Loss: 0.12462933 Acc: 0.97384306\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.11989176 Acc: 0.95365239\n",
      "test Loss: 0.12804191 Acc: 0.97384306\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.11678751 Acc: 0.95667506\n",
      "test Loss: 0.12799475 Acc: 0.97585513\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.13370442 Acc: 0.94609572\n",
      "test Loss: 0.13061378 Acc: 0.97384306\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.14465744 Acc: 0.94105793\n",
      "test Loss: 0.12293769 Acc: 0.97384306\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.12720448 Acc: 0.94911839\n",
      "test Loss: 0.12158533 Acc: 0.97384306\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.12826992 Acc: 0.94256927\n",
      "test Loss: 0.12663156 Acc: 0.97384306\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.11633904 Acc: 0.95113350\n",
      "test Loss: 0.12552281 Acc: 0.97384306\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.12369616 Acc: 0.94962217\n",
      "test Loss: 0.12553874 Acc: 0.97786720\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.13079908 Acc: 0.94256927\n",
      "test Loss: 0.12405027 Acc: 0.97585513\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.13520787 Acc: 0.94357683\n",
      "test Loss: 0.12281980 Acc: 0.97384306\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.13541824 Acc: 0.94206549\n",
      "test Loss: 0.12574167 Acc: 0.97384306\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.12174738 Acc: 0.95012594\n",
      "test Loss: 0.12693052 Acc: 0.97786720\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.13506368 Acc: 0.93954660\n",
      "test Loss: 0.12817350 Acc: 0.97384306\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.13070195 Acc: 0.94811083\n",
      "test Loss: 0.11826510 Acc: 0.97384306\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.14139207 Acc: 0.94307305\n",
      "test Loss: 0.11393755 Acc: 0.97585513\n",
      "New best model found!\n",
      "New record loss: 0.11393755149673408, previous record loss: 0.11719522414582836\n",
      "New record loss is SAVED: 0.11393755149673408\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.15090681 Acc: 0.93551637\n",
      "test Loss: 0.11983674 Acc: 0.97384306\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.13658579 Acc: 0.94609572\n",
      "test Loss: 0.11803017 Acc: 0.97585513\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.12390594 Acc: 0.94811083\n",
      "test Loss: 0.13489239 Acc: 0.97384306\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.13310550 Acc: 0.94156171\n",
      "test Loss: 0.12688230 Acc: 0.97183099\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.11987886 Acc: 0.95264484\n",
      "test Loss: 0.12383787 Acc: 0.97384306\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.14418176 Acc: 0.94005038\n",
      "test Loss: 0.12357658 Acc: 0.97384306\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.13989617 Acc: 0.93753149\n",
      "test Loss: 0.12162304 Acc: 0.97183099\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.14364656 Acc: 0.93954660\n",
      "test Loss: 0.13209444 Acc: 0.97384306\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.14552487 Acc: 0.94105793\n",
      "test Loss: 0.12822950 Acc: 0.97183099\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.14130046 Acc: 0.93904282\n",
      "test Loss: 0.12036498 Acc: 0.97384306\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.13066621 Acc: 0.94811083\n",
      "test Loss: 0.12403215 Acc: 0.97384306\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.12847418 Acc: 0.94710327\n",
      "test Loss: 0.12102636 Acc: 0.97384306\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.11536742 Acc: 0.95415617\n",
      "test Loss: 0.12685240 Acc: 0.97384306\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.13132357 Acc: 0.93803526\n",
      "test Loss: 0.12345956 Acc: 0.97384306\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.12645995 Acc: 0.94559194\n",
      "test Loss: 0.12341497 Acc: 0.97585513\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.14202131 Acc: 0.94206549\n",
      "test Loss: 0.12523714 Acc: 0.97585513\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.15103164 Acc: 0.93047859\n",
      "test Loss: 0.12390403 Acc: 0.97585513\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.14397192 Acc: 0.93904282\n",
      "test Loss: 0.12225419 Acc: 0.97384306\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.14437100 Acc: 0.94156171\n",
      "test Loss: 0.12689380 Acc: 0.97384306\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.13560566 Acc: 0.94156171\n",
      "test Loss: 0.12161355 Acc: 0.97384306\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.13053484 Acc: 0.94559194\n",
      "test Loss: 0.12088848 Acc: 0.97384306\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.12974514 Acc: 0.94609572\n",
      "test Loss: 0.12672597 Acc: 0.97585513\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.12416358 Acc: 0.94811083\n",
      "test Loss: 0.11627187 Acc: 0.97384306\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.13438114 Acc: 0.94458438\n",
      "test Loss: 0.12197507 Acc: 0.97183099\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.13245379 Acc: 0.94408060\n",
      "test Loss: 0.12112380 Acc: 0.97384306\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.12180644 Acc: 0.94710327\n",
      "test Loss: 0.11788390 Acc: 0.97384306\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.13711642 Acc: 0.94408060\n",
      "test Loss: 0.12651789 Acc: 0.97987928\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.14519078 Acc: 0.94156171\n",
      "test Loss: 0.12779446 Acc: 0.97585513\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.15316453 Acc: 0.93652393\n",
      "test Loss: 0.12493306 Acc: 0.97384306\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.13142380 Acc: 0.94256927\n",
      "test Loss: 0.12326715 Acc: 0.97384306\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.11937125 Acc: 0.95113350\n",
      "test Loss: 0.12491125 Acc: 0.97585513\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.15051384 Acc: 0.94357683\n",
      "test Loss: 0.12270140 Acc: 0.97384306\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.15977141 Acc: 0.93702771\n",
      "test Loss: 0.12265155 Acc: 0.97183099\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.12989608 Acc: 0.94911839\n",
      "test Loss: 0.12242558 Acc: 0.97384306\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.12251279 Acc: 0.94811083\n",
      "test Loss: 0.12432799 Acc: 0.97384306\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.13061511 Acc: 0.94861461\n",
      "test Loss: 0.12442656 Acc: 0.97183099\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.13375412 Acc: 0.94408060\n",
      "test Loss: 0.11776553 Acc: 0.97384306\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.13049303 Acc: 0.94408060\n",
      "test Loss: 0.13140419 Acc: 0.97384306\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.13110402 Acc: 0.94609572\n",
      "test Loss: 0.12926250 Acc: 0.97384306\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.11939755 Acc: 0.95113350\n",
      "test Loss: 0.12452194 Acc: 0.97384306\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.14290722 Acc: 0.94256927\n",
      "test Loss: 0.13011536 Acc: 0.97183099\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.12649652 Acc: 0.95314861\n",
      "test Loss: 0.12459326 Acc: 0.97384306\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.13158723 Acc: 0.94760705\n",
      "test Loss: 0.13148352 Acc: 0.97585513\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.15361263 Acc: 0.93602015\n",
      "test Loss: 0.12004707 Acc: 0.97384306\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.15183685 Acc: 0.94005038\n",
      "test Loss: 0.12663580 Acc: 0.97384306\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.12956645 Acc: 0.94659950\n",
      "test Loss: 0.12324273 Acc: 0.97384306\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.13052807 Acc: 0.94408060\n",
      "test Loss: 0.12397842 Acc: 0.97183099\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.15173755 Acc: 0.93904282\n",
      "test Loss: 0.12729869 Acc: 0.97585513\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.14313661 Acc: 0.94105793\n",
      "test Loss: 0.12534871 Acc: 0.97384306\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.12742814 Acc: 0.94811083\n",
      "test Loss: 0.12449053 Acc: 0.97384306\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.12190179 Acc: 0.95062972\n",
      "test Loss: 0.12705970 Acc: 0.97384306\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.12058150 Acc: 0.94559194\n",
      "test Loss: 0.12846562 Acc: 0.97384306\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.12667580 Acc: 0.94055416\n",
      "test Loss: 0.12570007 Acc: 0.97384306\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.14650069 Acc: 0.93904282\n",
      "test Loss: 0.12143394 Acc: 0.97384306\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.14103665 Acc: 0.93904282\n",
      "test Loss: 0.12138633 Acc: 0.97384306\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.14320739 Acc: 0.93803526\n",
      "test Loss: 0.12564021 Acc: 0.97384306\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.12577897 Acc: 0.95214106\n",
      "test Loss: 0.12403903 Acc: 0.97384306\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.15088047 Acc: 0.93702771\n",
      "test Loss: 0.13219218 Acc: 0.97585513\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.12780617 Acc: 0.94357683\n",
      "test Loss: 0.12648703 Acc: 0.97384306\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.14285145 Acc: 0.93954660\n",
      "test Loss: 0.12575932 Acc: 0.97384306\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.13296182 Acc: 0.94408060\n",
      "test Loss: 0.12514545 Acc: 0.97585513\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.12343226 Acc: 0.95365239\n",
      "test Loss: 0.12671256 Acc: 0.97384306\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.11932616 Acc: 0.94962217\n",
      "test Loss: 0.12531191 Acc: 0.97384306\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.13980633 Acc: 0.94307305\n",
      "test Loss: 0.12068577 Acc: 0.97384306\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.12707614 Acc: 0.95314861\n",
      "test Loss: 0.12108502 Acc: 0.97384306\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.12518128 Acc: 0.95062972\n",
      "test Loss: 0.12635835 Acc: 0.97384306\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.15336456 Acc: 0.93803526\n",
      "test Loss: 0.12118655 Acc: 0.97585513\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.12411659 Acc: 0.94811083\n",
      "test Loss: 0.12983397 Acc: 0.97585513\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.13393272 Acc: 0.94357683\n",
      "test Loss: 0.12267918 Acc: 0.97384306\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.13032401 Acc: 0.94559194\n",
      "test Loss: 0.12414934 Acc: 0.97585513\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.12868686 Acc: 0.95062972\n",
      "test Loss: 0.12598363 Acc: 0.97384306\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.12237025 Acc: 0.95566751\n",
      "test Loss: 0.12777517 Acc: 0.97384306\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.14229894 Acc: 0.94055416\n",
      "test Loss: 0.12502463 Acc: 0.97384306\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.13641263 Acc: 0.94760705\n",
      "test Loss: 0.12430657 Acc: 0.97384306\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.12596466 Acc: 0.94559194\n",
      "test Loss: 0.11921004 Acc: 0.97384306\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.13839451 Acc: 0.94055416\n",
      "test Loss: 0.12584928 Acc: 0.97384306\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.13366135 Acc: 0.94055416\n",
      "test Loss: 0.12795401 Acc: 0.97384306\n",
      "\n",
      "Training complete in 84m 27s\n",
      "Best val Acc: 0.97585513 Best val loss: 0.11393755\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19_CT/weights/EfficientNet_B3_Covid-19.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 200,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
