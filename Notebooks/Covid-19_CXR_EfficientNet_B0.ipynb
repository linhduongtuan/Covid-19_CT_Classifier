{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 12884, 'val': 2758}\n",
      "cuda:0\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19/CXR_20200630'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 100\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 7919391\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([#('fc1', nn.Linear(1280, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=70, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.15218442639450305, Val accuracy: 0.9566787003610108\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.17389431 Acc: 0.93076684\n",
      "val Loss: 0.16298832 Acc: 0.94742567\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.17312811 Acc: 0.93317293\n",
      "val Loss: 0.16237223 Acc: 0.94742567\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.17472732 Acc: 0.93534617\n",
      "val Loss: 0.16273782 Acc: 0.94670051\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.16462006 Acc: 0.93868364\n",
      "val Loss: 0.16277093 Acc: 0.94742567\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.17087805 Acc: 0.93550140\n",
      "val Loss: 0.16201506 Acc: 0.94778825\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.17739049 Acc: 0.93208631\n",
      "val Loss: 0.16551591 Acc: 0.94778825\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.16759657 Acc: 0.93612232\n",
      "val Loss: 0.16394449 Acc: 0.94778825\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.17054474 Acc: 0.93581186\n",
      "val Loss: 0.16199154 Acc: 0.94851342\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.17263139 Acc: 0.93542378\n",
      "val Loss: 0.16312662 Acc: 0.94778825\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.17093916 Acc: 0.93619994\n",
      "val Loss: 0.16265505 Acc: 0.94742567\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.17308727 Acc: 0.93340577\n",
      "val Loss: 0.16527748 Acc: 0.94742567\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.16989701 Acc: 0.93581186\n",
      "val Loss: 0.16202582 Acc: 0.94742567\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.17259456 Acc: 0.93526855\n",
      "val Loss: 0.16301783 Acc: 0.94742567\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.16000367 Acc: 0.93969264\n",
      "val Loss: 0.16476699 Acc: 0.94742567\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.16813110 Acc: 0.93751940\n",
      "val Loss: 0.16336721 Acc: 0.94887600\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.16500242 Acc: 0.93705371\n",
      "val Loss: 0.16325826 Acc: 0.94778825\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.17163962 Acc: 0.93542378\n",
      "val Loss: 0.16219243 Acc: 0.94815083\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.16963420 Acc: 0.93542378\n",
      "val Loss: 0.16100047 Acc: 0.94778825\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.16311991 Acc: 0.93977026\n",
      "val Loss: 0.16218243 Acc: 0.94851342\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.17115841 Acc: 0.93557901\n",
      "val Loss: 0.16113078 Acc: 0.94815083\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.16436317 Acc: 0.93883887\n",
      "val Loss: 0.16321471 Acc: 0.94778825\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.17228792 Acc: 0.93612232\n",
      "val Loss: 0.16569023 Acc: 0.94887600\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.16691818 Acc: 0.93744179\n",
      "val Loss: 0.16244954 Acc: 0.94887600\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.16686926 Acc: 0.93782987\n",
      "val Loss: 0.16259266 Acc: 0.94851342\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.16724057 Acc: 0.93705371\n",
      "val Loss: 0.16322801 Acc: 0.94815083\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.16658860 Acc: 0.93651040\n",
      "val Loss: 0.16282487 Acc: 0.94778825\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.17143506 Acc: 0.93557901\n",
      "val Loss: 0.16442286 Acc: 0.94742567\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.17313856 Acc: 0.93534617\n",
      "val Loss: 0.16263499 Acc: 0.94923858\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.16970955 Acc: 0.93612232\n",
      "val Loss: 0.16216768 Acc: 0.94887600\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.16823324 Acc: 0.93744179\n",
      "val Loss: 0.16475981 Acc: 0.94851342\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.16376330 Acc: 0.93852841\n",
      "val Loss: 0.16197323 Acc: 0.94778825\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.17124451 Acc: 0.93689848\n",
      "val Loss: 0.16417061 Acc: 0.94887600\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.16197917 Acc: 0.93961503\n",
      "val Loss: 0.16136902 Acc: 0.94887600\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.16529805 Acc: 0.93612232\n",
      "val Loss: 0.16242463 Acc: 0.94887600\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.16901835 Acc: 0.93651040\n",
      "val Loss: 0.16276420 Acc: 0.94851342\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.16781022 Acc: 0.93619994\n",
      "val Loss: 0.16061688 Acc: 0.94815083\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.17374199 Acc: 0.93682086\n",
      "val Loss: 0.16089752 Acc: 0.94923858\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.16470145 Acc: 0.93666563\n",
      "val Loss: 0.16132767 Acc: 0.94778825\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.16485758 Acc: 0.93914933\n",
      "val Loss: 0.16077169 Acc: 0.94778825\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.17094699 Acc: 0.93627755\n",
      "val Loss: 0.16143350 Acc: 0.94742567\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.16858298 Acc: 0.93588948\n",
      "val Loss: 0.16254757 Acc: 0.94815083\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.16589563 Acc: 0.93782987\n",
      "val Loss: 0.16277552 Acc: 0.94742567\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.16460643 Acc: 0.93658802\n",
      "val Loss: 0.16287486 Acc: 0.94851342\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.16727333 Acc: 0.93635517\n",
      "val Loss: 0.16568031 Acc: 0.94815083\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.16296753 Acc: 0.93744179\n",
      "val Loss: 0.16300784 Acc: 0.94887600\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.17098844 Acc: 0.93790748\n",
      "val Loss: 0.16360084 Acc: 0.94887600\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.16470463 Acc: 0.93635517\n",
      "val Loss: 0.16154135 Acc: 0.94815083\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.16742796 Acc: 0.93503570\n",
      "val Loss: 0.16265455 Acc: 0.94742567\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.16895734 Acc: 0.93658802\n",
      "val Loss: 0.16324445 Acc: 0.94851342\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.17025345 Acc: 0.93557901\n",
      "val Loss: 0.16465524 Acc: 0.94815083\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.16871080 Acc: 0.93534617\n",
      "val Loss: 0.16341462 Acc: 0.94851342\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.16785544 Acc: 0.93526855\n",
      "val Loss: 0.16369159 Acc: 0.94815083\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.17368983 Acc: 0.93410432\n",
      "val Loss: 0.16066071 Acc: 0.94778825\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.16817399 Acc: 0.93759702\n",
      "val Loss: 0.16187901 Acc: 0.94815083\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.16137925 Acc: 0.93651040\n",
      "val Loss: 0.16188173 Acc: 0.94778825\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.15834486 Acc: 0.94077926\n",
      "val Loss: 0.16182515 Acc: 0.94778825\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.16269979 Acc: 0.93759702\n",
      "val Loss: 0.16053338 Acc: 0.94923858\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.16821183 Acc: 0.93534617\n",
      "val Loss: 0.16281031 Acc: 0.94706309\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.16804802 Acc: 0.93658802\n",
      "val Loss: 0.16251661 Acc: 0.94706309\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.16699395 Acc: 0.93790748\n",
      "val Loss: 0.16490746 Acc: 0.94923858\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.16793739 Acc: 0.93705371\n",
      "val Loss: 0.16324770 Acc: 0.94851342\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.16634116 Acc: 0.93720894\n",
      "val Loss: 0.16239487 Acc: 0.94778825\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.17311139 Acc: 0.93371624\n",
      "val Loss: 0.16180344 Acc: 0.94851342\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.16451005 Acc: 0.93837318\n",
      "val Loss: 0.16314878 Acc: 0.94742567\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.16376619 Acc: 0.93775225\n",
      "val Loss: 0.16363953 Acc: 0.94851342\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.16091642 Acc: 0.93829556\n",
      "val Loss: 0.16329631 Acc: 0.94778825\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.16333407 Acc: 0.93798510\n",
      "val Loss: 0.16499951 Acc: 0.94742567\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.16787475 Acc: 0.93728656\n",
      "val Loss: 0.16185095 Acc: 0.94851342\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.16929707 Acc: 0.93418193\n",
      "val Loss: 0.16063819 Acc: 0.94815083\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.17029016 Acc: 0.93581186\n",
      "val Loss: 0.16184931 Acc: 0.94851342\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.16834587 Acc: 0.93495809\n",
      "val Loss: 0.16577297 Acc: 0.94778825\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.16445507 Acc: 0.93899410\n",
      "val Loss: 0.16239220 Acc: 0.94851342\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.16702055 Acc: 0.93689848\n",
      "val Loss: 0.16113959 Acc: 0.94851342\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.16383063 Acc: 0.93782987\n",
      "val Loss: 0.16239702 Acc: 0.94815083\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.16481851 Acc: 0.93814033\n",
      "val Loss: 0.16125466 Acc: 0.94887600\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.16468909 Acc: 0.93705371\n",
      "val Loss: 0.16421603 Acc: 0.94778825\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.15773111 Acc: 0.94023595\n",
      "val Loss: 0.16202000 Acc: 0.94815083\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.16880403 Acc: 0.93775225\n",
      "val Loss: 0.16292334 Acc: 0.94815083\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.16079596 Acc: 0.93945980\n",
      "val Loss: 0.16295886 Acc: 0.94778825\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.16448745 Acc: 0.93666563\n",
      "val Loss: 0.16225146 Acc: 0.94778825\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.17983478 Acc: 0.93239677\n",
      "val Loss: 0.16370000 Acc: 0.94887600\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.16252280 Acc: 0.93876125\n",
      "val Loss: 0.16211900 Acc: 0.94815083\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.16564876 Acc: 0.93542378\n",
      "val Loss: 0.16325283 Acc: 0.94851342\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.16399188 Acc: 0.93666563\n",
      "val Loss: 0.16213350 Acc: 0.94778825\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.16943478 Acc: 0.93736417\n",
      "val Loss: 0.16131322 Acc: 0.94670051\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.16617662 Acc: 0.93720894\n",
      "val Loss: 0.16379761 Acc: 0.94815083\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.17070783 Acc: 0.93457001\n",
      "val Loss: 0.16283316 Acc: 0.94887600\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.16902001 Acc: 0.93573424\n",
      "val Loss: 0.16353472 Acc: 0.94742567\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.16438026 Acc: 0.93759702\n",
      "val Loss: 0.16140539 Acc: 0.94778825\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.16366753 Acc: 0.93713133\n",
      "val Loss: 0.16060308 Acc: 0.94851342\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.16386440 Acc: 0.93635517\n",
      "val Loss: 0.16247701 Acc: 0.94923858\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.16572783 Acc: 0.93837318\n",
      "val Loss: 0.16274953 Acc: 0.94887600\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.16694263 Acc: 0.93495809\n",
      "val Loss: 0.16061746 Acc: 0.94778825\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.15836962 Acc: 0.93922695\n",
      "val Loss: 0.16260859 Acc: 0.94887600\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.17177483 Acc: 0.93278485\n",
      "val Loss: 0.16154762 Acc: 0.94887600\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.16086830 Acc: 0.93868364\n",
      "val Loss: 0.16445318 Acc: 0.94815083\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.17016277 Acc: 0.93674325\n",
      "val Loss: 0.16291579 Acc: 0.94778825\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.16223978 Acc: 0.93759702\n",
      "val Loss: 0.16278440 Acc: 0.94778825\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.16277338 Acc: 0.93845079\n",
      "val Loss: 0.16125238 Acc: 0.94815083\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.16479576 Acc: 0.93907172\n",
      "val Loss: 0.16374569 Acc: 0.94815083\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.16648783 Acc: 0.93806271\n",
      "val Loss: 0.16331094 Acc: 0.94815083\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.16899897 Acc: 0.93410432\n",
      "val Loss: 0.16186602 Acc: 0.94923858\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.16545090 Acc: 0.93744179\n",
      "val Loss: 0.16117592 Acc: 0.94815083\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.16627486 Acc: 0.93651040\n",
      "val Loss: 0.16474491 Acc: 0.94923858\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.17215449 Acc: 0.93371624\n",
      "val Loss: 0.16587942 Acc: 0.94851342\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.16292171 Acc: 0.93705371\n",
      "val Loss: 0.16211989 Acc: 0.94923858\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.16231589 Acc: 0.93596709\n",
      "val Loss: 0.16332344 Acc: 0.94851342\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.17021665 Acc: 0.93713133\n",
      "val Loss: 0.16328559 Acc: 0.94923858\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.16599636 Acc: 0.93782987\n",
      "val Loss: 0.16342498 Acc: 0.94960116\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.16424832 Acc: 0.93977026\n",
      "val Loss: 0.16269834 Acc: 0.94887600\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.16495763 Acc: 0.93744179\n",
      "val Loss: 0.16100796 Acc: 0.94778825\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.16343620 Acc: 0.93961503\n",
      "val Loss: 0.16306905 Acc: 0.94923858\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.16425198 Acc: 0.93782987\n",
      "val Loss: 0.16222575 Acc: 0.94670051\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.16293727 Acc: 0.93775225\n",
      "val Loss: 0.16287408 Acc: 0.94960116\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.16715557 Acc: 0.93581186\n",
      "val Loss: 0.16309635 Acc: 0.94778825\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.17227410 Acc: 0.93464762\n",
      "val Loss: 0.16274124 Acc: 0.94851342\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.16330411 Acc: 0.93697609\n",
      "val Loss: 0.16445716 Acc: 0.94815083\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.16424626 Acc: 0.93845079\n",
      "val Loss: 0.16468968 Acc: 0.94996374\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.16858644 Acc: 0.93627755\n",
      "val Loss: 0.16408046 Acc: 0.94778825\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.16831392 Acc: 0.93573424\n",
      "val Loss: 0.16207549 Acc: 0.94778825\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.16223910 Acc: 0.93775225\n",
      "val Loss: 0.16399586 Acc: 0.94815083\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.16297555 Acc: 0.93736417\n",
      "val Loss: 0.16155605 Acc: 0.94815083\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.16747895 Acc: 0.93682086\n",
      "val Loss: 0.16333700 Acc: 0.94923858\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.16769896 Acc: 0.93627755\n",
      "val Loss: 0.16169054 Acc: 0.94923858\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.16347303 Acc: 0.93806271\n",
      "val Loss: 0.16163175 Acc: 0.94887600\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.16850576 Acc: 0.93821794\n",
      "val Loss: 0.16315491 Acc: 0.94815083\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.17331613 Acc: 0.93441478\n",
      "val Loss: 0.16222990 Acc: 0.94815083\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.15955546 Acc: 0.94171065\n",
      "val Loss: 0.16369929 Acc: 0.94887600\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.17289500 Acc: 0.93612232\n",
      "val Loss: 0.16239253 Acc: 0.94996374\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.17200511 Acc: 0.93325054\n",
      "val Loss: 0.16262447 Acc: 0.94742567\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.16865678 Acc: 0.93534617\n",
      "val Loss: 0.16274017 Acc: 0.94706309\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.16173608 Acc: 0.93876125\n",
      "val Loss: 0.16397098 Acc: 0.94851342\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.16483515 Acc: 0.93977026\n",
      "val Loss: 0.16198644 Acc: 0.94923858\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.16560352 Acc: 0.93689848\n",
      "val Loss: 0.16134570 Acc: 0.94670051\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.16345302 Acc: 0.93627755\n",
      "val Loss: 0.16111752 Acc: 0.94778825\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.16612598 Acc: 0.93682086\n",
      "val Loss: 0.16323509 Acc: 0.94851342\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.15870060 Acc: 0.94023595\n",
      "val Loss: 0.16347012 Acc: 0.94633793\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.15916318 Acc: 0.93953741\n",
      "val Loss: 0.16382990 Acc: 0.94815083\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.16222132 Acc: 0.94023595\n",
      "val Loss: 0.16370180 Acc: 0.94742567\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.17250147 Acc: 0.93488047\n",
      "val Loss: 0.16248829 Acc: 0.94923858\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.16498718 Acc: 0.93790748\n",
      "val Loss: 0.16217067 Acc: 0.94887600\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.16047648 Acc: 0.93868364\n",
      "val Loss: 0.16131799 Acc: 0.94815083\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.16272254 Acc: 0.93651040\n",
      "val Loss: 0.16116318 Acc: 0.94887600\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.16710711 Acc: 0.93697609\n",
      "val Loss: 0.16267973 Acc: 0.94815083\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.16882647 Acc: 0.93480286\n",
      "val Loss: 0.16251811 Acc: 0.94742567\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.16305649 Acc: 0.93992549\n",
      "val Loss: 0.16276999 Acc: 0.94778825\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.17377312 Acc: 0.93332816\n",
      "val Loss: 0.16114705 Acc: 0.94851342\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.17327343 Acc: 0.93294008\n",
      "val Loss: 0.16183030 Acc: 0.94742567\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.16851307 Acc: 0.93682086\n",
      "val Loss: 0.16218740 Acc: 0.94815083\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.16957876 Acc: 0.93542378\n",
      "val Loss: 0.16278836 Acc: 0.94887600\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.17084645 Acc: 0.93433716\n",
      "val Loss: 0.16147312 Acc: 0.94851342\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.16324832 Acc: 0.93914933\n",
      "val Loss: 0.16226902 Acc: 0.94815083\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.16195499 Acc: 0.93814033\n",
      "val Loss: 0.16233476 Acc: 0.94778825\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.17169242 Acc: 0.93449239\n",
      "val Loss: 0.16190514 Acc: 0.94706309\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.16582676 Acc: 0.93736417\n",
      "val Loss: 0.16394288 Acc: 0.94815083\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.17040159 Acc: 0.93604471\n",
      "val Loss: 0.16651985 Acc: 0.94851342\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.15954667 Acc: 0.93907172\n",
      "val Loss: 0.16154305 Acc: 0.94778825\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.15606177 Acc: 0.94171065\n",
      "val Loss: 0.16178307 Acc: 0.94923858\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.16577292 Acc: 0.93503570\n",
      "val Loss: 0.16296214 Acc: 0.94778825\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.16557729 Acc: 0.93945980\n",
      "val Loss: 0.16192186 Acc: 0.94923858\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.17102759 Acc: 0.93410432\n",
      "val Loss: 0.16180707 Acc: 0.94633793\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.16712812 Acc: 0.93790748\n",
      "val Loss: 0.16146397 Acc: 0.94887600\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.17280341 Acc: 0.93449239\n",
      "val Loss: 0.16213837 Acc: 0.94815083\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.16809196 Acc: 0.93705371\n",
      "val Loss: 0.16243644 Acc: 0.94887600\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.17273361 Acc: 0.93534617\n",
      "val Loss: 0.16072560 Acc: 0.94887600\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.16735020 Acc: 0.93472524\n",
      "val Loss: 0.16142214 Acc: 0.94887600\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.16441957 Acc: 0.93767464\n",
      "val Loss: 0.16214271 Acc: 0.94778825\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.16568844 Acc: 0.93651040\n",
      "val Loss: 0.16289278 Acc: 0.94923858\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.16538685 Acc: 0.93814033\n",
      "val Loss: 0.16200781 Acc: 0.94815083\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.17188816 Acc: 0.93270723\n",
      "val Loss: 0.16345536 Acc: 0.94851342\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.16637668 Acc: 0.93526855\n",
      "val Loss: 0.16201569 Acc: 0.94778825\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.16818995 Acc: 0.93604471\n",
      "val Loss: 0.16144939 Acc: 0.94851342\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.17375653 Acc: 0.93216392\n",
      "val Loss: 0.16546864 Acc: 0.94815083\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.16209570 Acc: 0.93969264\n",
      "val Loss: 0.16292668 Acc: 0.94851342\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.16666534 Acc: 0.93689848\n",
      "val Loss: 0.16103756 Acc: 0.94815083\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.16738519 Acc: 0.93581186\n",
      "val Loss: 0.16367731 Acc: 0.94670051\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.17052045 Acc: 0.93550140\n",
      "val Loss: 0.16338017 Acc: 0.94960116\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.16438926 Acc: 0.93728656\n",
      "val Loss: 0.16148431 Acc: 0.94815083\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.16899689 Acc: 0.93604471\n",
      "val Loss: 0.16495461 Acc: 0.94778825\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.17007861 Acc: 0.93596709\n",
      "val Loss: 0.16252001 Acc: 0.94851342\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.16880227 Acc: 0.93821794\n",
      "val Loss: 0.16326963 Acc: 0.94742567\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.16696971 Acc: 0.93713133\n",
      "val Loss: 0.16239457 Acc: 0.94851342\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.16894406 Acc: 0.93643278\n",
      "val Loss: 0.16074114 Acc: 0.94742567\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.16640103 Acc: 0.93782987\n",
      "val Loss: 0.16163059 Acc: 0.94887600\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.16275166 Acc: 0.93876125\n",
      "val Loss: 0.16157995 Acc: 0.94742567\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.16760191 Acc: 0.93759702\n",
      "val Loss: 0.16454264 Acc: 0.94851342\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.16349984 Acc: 0.93759702\n",
      "val Loss: 0.16384841 Acc: 0.94778825\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.16650223 Acc: 0.93356101\n",
      "val Loss: 0.16201449 Acc: 0.94706309\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.17008995 Acc: 0.93565663\n",
      "val Loss: 0.16162644 Acc: 0.94815083\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.17743977 Acc: 0.93332816\n",
      "val Loss: 0.16129001 Acc: 0.94742567\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.17252083 Acc: 0.93534617\n",
      "val Loss: 0.16100630 Acc: 0.94633793\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.17345007 Acc: 0.93356101\n",
      "val Loss: 0.16216159 Acc: 0.94815083\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.16976998 Acc: 0.93651040\n",
      "val Loss: 0.16287600 Acc: 0.94815083\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.15918868 Acc: 0.94077926\n",
      "val Loss: 0.16117521 Acc: 0.94960116\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.16601274 Acc: 0.93557901\n",
      "val Loss: 0.16094297 Acc: 0.94960116\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.17548012 Acc: 0.93542378\n",
      "val Loss: 0.16261895 Acc: 0.94815083\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.17527552 Acc: 0.93309531\n",
      "val Loss: 0.16253512 Acc: 0.94742567\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.16503785 Acc: 0.93658802\n",
      "val Loss: 0.16101525 Acc: 0.94815083\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.16603869 Acc: 0.93876125\n",
      "val Loss: 0.16107696 Acc: 0.94778825\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.16422936 Acc: 0.93503570\n",
      "val Loss: 0.16101072 Acc: 0.94960116\n",
      "\n",
      "Training complete in 792m 59s\n",
      "Best val Acc: 0.95667870 Best val loss: 0.15218443\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CXR/EfficientNet_B0_Dataset20200630.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 200,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
