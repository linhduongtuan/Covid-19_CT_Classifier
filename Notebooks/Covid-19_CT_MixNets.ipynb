{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'NonCOVID-19']\n",
      "{'train': 639, 'val': 75}\n",
      "cuda:0\n",
      "{0: 'COVID-19', 1: 'NonCOVID-19'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([44, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_dir = '/home/linh/Downloads/Covid-19/CT/Soares'\n",
    "\n",
    "data_dir = '/home/linh/Downloads/Covid-19/CT/Yang'\n",
    "\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 44\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('mixnet_xl', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14015482\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=70, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.1626083121697108, Val accuracy: 0.9466666666666668\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.25679381 Acc: 0.89514867\n",
      "val Loss: 0.15934705 Acc: 0.94666667\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.26655616 Acc: 0.88106416\n",
      "val Loss: 0.15551488 Acc: 0.94666667\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.25025364 Acc: 0.89514867\n",
      "val Loss: 0.16698318 Acc: 0.94666667\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.22820096 Acc: 0.91862285\n",
      "val Loss: 0.16737210 Acc: 0.94666667\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.27227353 Acc: 0.88575900\n",
      "val Loss: 0.14835829 Acc: 0.94666667\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.29883659 Acc: 0.87323944\n",
      "val Loss: 0.16318206 Acc: 0.96000000\n",
      "New best model found!\n",
      "New record ACC: 0.9600000000000001, previous record acc: 0.9466666666666668\n",
      "New record acc is SAVED: 0.9600000000000001\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.25728646 Acc: 0.87949922\n",
      "val Loss: 0.16184770 Acc: 0.93333333\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.24703119 Acc: 0.88888889\n",
      "val Loss: 0.16793537 Acc: 0.96000000\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.24014209 Acc: 0.90140845\n",
      "val Loss: 0.15638127 Acc: 0.96000000\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.25929680 Acc: 0.89358372\n",
      "val Loss: 0.17810166 Acc: 0.94666667\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.26939188 Acc: 0.87167449\n",
      "val Loss: 0.16524653 Acc: 0.94666667\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.22185828 Acc: 0.91236307\n",
      "val Loss: 0.16469241 Acc: 0.94666667\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.23512945 Acc: 0.89045383\n",
      "val Loss: 0.16933799 Acc: 0.94666667\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.23073937 Acc: 0.90140845\n",
      "val Loss: 0.17053469 Acc: 0.94666667\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.24217717 Acc: 0.89045383\n",
      "val Loss: 0.16354320 Acc: 0.94666667\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.23008307 Acc: 0.90610329\n",
      "val Loss: 0.17216666 Acc: 0.94666667\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.26736671 Acc: 0.87323944\n",
      "val Loss: 0.18460789 Acc: 0.94666667\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.27147699 Acc: 0.87323944\n",
      "val Loss: 0.17238739 Acc: 0.94666667\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.22575269 Acc: 0.89827856\n",
      "val Loss: 0.16657304 Acc: 0.94666667\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.24107150 Acc: 0.89514867\n",
      "val Loss: 0.17171288 Acc: 0.94666667\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.26071378 Acc: 0.87636933\n",
      "val Loss: 0.18032520 Acc: 0.94666667\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.24732745 Acc: 0.88732394\n",
      "val Loss: 0.16874352 Acc: 0.94666667\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.28584261 Acc: 0.87793427\n",
      "val Loss: 0.17039861 Acc: 0.93333333\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.24699201 Acc: 0.88732394\n",
      "val Loss: 0.17495525 Acc: 0.94666667\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.28003927 Acc: 0.87949922\n",
      "val Loss: 0.17870932 Acc: 0.94666667\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.26537053 Acc: 0.87480438\n",
      "val Loss: 0.15822506 Acc: 0.94666667\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.22947204 Acc: 0.90610329\n",
      "val Loss: 0.16906675 Acc: 0.93333333\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.23144942 Acc: 0.90140845\n",
      "val Loss: 0.15085569 Acc: 0.96000000\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.24840468 Acc: 0.88732394\n",
      "val Loss: 0.16233032 Acc: 0.94666667\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.27902900 Acc: 0.87323944\n",
      "val Loss: 0.16914952 Acc: 0.96000000\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.24684995 Acc: 0.89671362\n",
      "val Loss: 0.17209681 Acc: 0.94666667\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.25868184 Acc: 0.87949922\n",
      "val Loss: 0.17517321 Acc: 0.94666667\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.26575496 Acc: 0.88888889\n",
      "val Loss: 0.17258294 Acc: 0.94666667\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.24082100 Acc: 0.90297340\n",
      "val Loss: 0.15936941 Acc: 0.93333333\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.23978285 Acc: 0.89827856\n",
      "val Loss: 0.15647739 Acc: 0.96000000\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.24675713 Acc: 0.88575900\n",
      "val Loss: 0.16588686 Acc: 0.94666667\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.27920755 Acc: 0.87636933\n",
      "val Loss: 0.17520716 Acc: 0.94666667\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.24081803 Acc: 0.89201878\n",
      "val Loss: 0.16561751 Acc: 0.96000000\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.24384004 Acc: 0.89045383\n",
      "val Loss: 0.17332977 Acc: 0.96000000\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.23209794 Acc: 0.89671362\n",
      "val Loss: 0.17921338 Acc: 0.96000000\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.25009612 Acc: 0.88732394\n",
      "val Loss: 0.18018158 Acc: 0.92000000\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.24872671 Acc: 0.88888889\n",
      "val Loss: 0.16917104 Acc: 0.94666667\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.23905247 Acc: 0.88575900\n",
      "val Loss: 0.14971232 Acc: 0.94666667\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.24366645 Acc: 0.89514867\n",
      "val Loss: 0.16428948 Acc: 0.96000000\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.24584686 Acc: 0.89671362\n",
      "val Loss: 0.17895721 Acc: 0.96000000\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.21927879 Acc: 0.90923318\n",
      "val Loss: 0.16190293 Acc: 0.94666667\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.25671739 Acc: 0.88888889\n",
      "val Loss: 0.16106982 Acc: 0.93333333\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.24075785 Acc: 0.89358372\n",
      "val Loss: 0.16673560 Acc: 0.94666667\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.22843653 Acc: 0.90453834\n",
      "val Loss: 0.15701430 Acc: 0.96000000\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.26005817 Acc: 0.88106416\n",
      "val Loss: 0.14960739 Acc: 0.94666667\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.25924266 Acc: 0.87949922\n",
      "val Loss: 0.15472434 Acc: 0.93333333\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.22861368 Acc: 0.89827856\n",
      "val Loss: 0.15801855 Acc: 0.94666667\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.23508972 Acc: 0.89827856\n",
      "val Loss: 0.16792536 Acc: 0.94666667\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.23151002 Acc: 0.89358372\n",
      "val Loss: 0.17192399 Acc: 0.94666667\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.27897076 Acc: 0.88262911\n",
      "val Loss: 0.15723619 Acc: 0.94666667\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.26079081 Acc: 0.88419405\n",
      "val Loss: 0.15020003 Acc: 0.94666667\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.26715836 Acc: 0.89358372\n",
      "val Loss: 0.15336813 Acc: 0.94666667\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.24394678 Acc: 0.88575900\n",
      "val Loss: 0.16181088 Acc: 0.94666667\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.22834794 Acc: 0.89045383\n",
      "val Loss: 0.16490456 Acc: 0.93333333\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.25160469 Acc: 0.89671362\n",
      "val Loss: 0.16875233 Acc: 0.94666667\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.21620829 Acc: 0.91079812\n",
      "val Loss: 0.16554858 Acc: 0.93333333\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.25101321 Acc: 0.88888889\n",
      "val Loss: 0.15467497 Acc: 0.93333333\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.25340743 Acc: 0.88419405\n",
      "val Loss: 0.16612234 Acc: 0.94666667\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.22032457 Acc: 0.91236307\n",
      "val Loss: 0.15427143 Acc: 0.94666667\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.25570422 Acc: 0.87480438\n",
      "val Loss: 0.16209231 Acc: 0.94666667\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.28775814 Acc: 0.87480438\n",
      "val Loss: 0.16927405 Acc: 0.94666667\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.26963175 Acc: 0.87949922\n",
      "val Loss: 0.17121018 Acc: 0.94666667\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.23285284 Acc: 0.89984351\n",
      "val Loss: 0.17131176 Acc: 0.94666667\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.22803701 Acc: 0.90297340\n",
      "val Loss: 0.16013416 Acc: 0.94666667\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.26459065 Acc: 0.89358372\n",
      "val Loss: 0.16814508 Acc: 0.93333333\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.23773356 Acc: 0.89514867\n",
      "val Loss: 0.17786452 Acc: 0.94666667\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.25654332 Acc: 0.88888889\n",
      "val Loss: 0.18778878 Acc: 0.96000000\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.20274981 Acc: 0.92018779\n",
      "val Loss: 0.17374631 Acc: 0.94666667\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.25588125 Acc: 0.88262911\n",
      "val Loss: 0.16763810 Acc: 0.94666667\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.22165123 Acc: 0.90610329\n",
      "val Loss: 0.16337747 Acc: 0.96000000\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.25731263 Acc: 0.88732394\n",
      "val Loss: 0.16384072 Acc: 0.94666667\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.23604549 Acc: 0.89045383\n",
      "val Loss: 0.15305199 Acc: 0.94666667\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.25844937 Acc: 0.88419405\n",
      "val Loss: 0.17501113 Acc: 0.96000000\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.24592979 Acc: 0.88732394\n",
      "val Loss: 0.18245172 Acc: 0.94666667\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.24407985 Acc: 0.89358372\n",
      "val Loss: 0.16430693 Acc: 0.96000000\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.24857820 Acc: 0.87323944\n",
      "val Loss: 0.16971827 Acc: 0.93333333\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.25660366 Acc: 0.88888889\n",
      "val Loss: 0.16028211 Acc: 0.93333333\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.25057843 Acc: 0.89358372\n",
      "val Loss: 0.15941931 Acc: 0.93333333\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.24627051 Acc: 0.89827856\n",
      "val Loss: 0.15554885 Acc: 0.93333333\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.23789630 Acc: 0.90766823\n",
      "val Loss: 0.16433323 Acc: 0.96000000\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.28632328 Acc: 0.85602504\n",
      "val Loss: 0.17855852 Acc: 0.94666667\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.18598989 Acc: 0.93740219\n",
      "val Loss: 0.16451867 Acc: 0.94666667\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.25975437 Acc: 0.88106416\n",
      "val Loss: 0.15932839 Acc: 0.93333333\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.24075235 Acc: 0.90766823\n",
      "val Loss: 0.18106545 Acc: 0.94666667\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.26011952 Acc: 0.87949922\n",
      "val Loss: 0.17231962 Acc: 0.96000000\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.20663436 Acc: 0.91236307\n",
      "val Loss: 0.17556025 Acc: 0.94666667\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.19439417 Acc: 0.92175274\n",
      "val Loss: 0.16474275 Acc: 0.94666667\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.24770772 Acc: 0.88106416\n",
      "val Loss: 0.17388097 Acc: 0.94666667\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.24979821 Acc: 0.88575900\n",
      "val Loss: 0.15807717 Acc: 0.96000000\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.28109155 Acc: 0.88262911\n",
      "val Loss: 0.17394053 Acc: 0.94666667\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.23549694 Acc: 0.90453834\n",
      "val Loss: 0.16869734 Acc: 0.94666667\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.25031753 Acc: 0.88575900\n",
      "val Loss: 0.15802183 Acc: 0.93333333\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.24595975 Acc: 0.89827856\n",
      "val Loss: 0.15895218 Acc: 0.94666667\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.21842491 Acc: 0.90610329\n",
      "val Loss: 0.16633553 Acc: 0.94666667\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.26515896 Acc: 0.87480438\n",
      "val Loss: 0.17905907 Acc: 0.96000000\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.25218650 Acc: 0.89201878\n",
      "val Loss: 0.17197299 Acc: 0.93333333\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.25846236 Acc: 0.89045383\n",
      "val Loss: 0.18328547 Acc: 0.96000000\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.26713270 Acc: 0.87949922\n",
      "val Loss: 0.15826838 Acc: 0.94666667\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.23204807 Acc: 0.91079812\n",
      "val Loss: 0.16441328 Acc: 0.94666667\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.25291660 Acc: 0.88732394\n",
      "val Loss: 0.18263032 Acc: 0.96000000\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.30137647 Acc: 0.86697966\n",
      "val Loss: 0.17691263 Acc: 0.93333333\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.23573169 Acc: 0.89671362\n",
      "val Loss: 0.17910497 Acc: 0.93333333\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.24159933 Acc: 0.89514867\n",
      "val Loss: 0.15535535 Acc: 0.96000000\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.23675132 Acc: 0.90297340\n",
      "val Loss: 0.16848002 Acc: 0.94666667\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.23901734 Acc: 0.90297340\n",
      "val Loss: 0.16103380 Acc: 0.94666667\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.22741966 Acc: 0.90610329\n",
      "val Loss: 0.16648130 Acc: 0.94666667\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.24293634 Acc: 0.88732394\n",
      "val Loss: 0.17391159 Acc: 0.94666667\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.22521326 Acc: 0.91392801\n",
      "val Loss: 0.16894880 Acc: 0.96000000\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.26094410 Acc: 0.90140845\n",
      "val Loss: 0.17732166 Acc: 0.93333333\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.22821690 Acc: 0.91079812\n",
      "val Loss: 0.16722711 Acc: 0.94666667\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.24996147 Acc: 0.88575900\n",
      "val Loss: 0.17252127 Acc: 0.94666667\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.25967988 Acc: 0.88106416\n",
      "val Loss: 0.16247952 Acc: 0.94666667\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.24376218 Acc: 0.89045383\n",
      "val Loss: 0.17235537 Acc: 0.96000000\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.23596532 Acc: 0.89358372\n",
      "val Loss: 0.14594839 Acc: 0.94666667\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.24677099 Acc: 0.89045383\n",
      "val Loss: 0.16146867 Acc: 0.94666667\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.25444616 Acc: 0.88575900\n",
      "val Loss: 0.15909992 Acc: 0.94666667\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.24937004 Acc: 0.89045383\n",
      "val Loss: 0.15962678 Acc: 0.96000000\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.25093535 Acc: 0.88732394\n",
      "val Loss: 0.18096622 Acc: 0.94666667\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.23214788 Acc: 0.89984351\n",
      "val Loss: 0.17357165 Acc: 0.94666667\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.23965542 Acc: 0.89358372\n",
      "val Loss: 0.17264934 Acc: 0.93333333\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.23874040 Acc: 0.89201878\n",
      "val Loss: 0.17707204 Acc: 0.94666667\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.22843533 Acc: 0.89827856\n",
      "val Loss: 0.17571374 Acc: 0.94666667\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.25019151 Acc: 0.88106416\n",
      "val Loss: 0.17655043 Acc: 0.94666667\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.22783894 Acc: 0.90140845\n",
      "val Loss: 0.17292699 Acc: 0.96000000\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.25305489 Acc: 0.87949922\n",
      "val Loss: 0.15882451 Acc: 0.93333333\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.24087655 Acc: 0.89045383\n",
      "val Loss: 0.15353105 Acc: 0.94666667\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.25186013 Acc: 0.89827856\n",
      "val Loss: 0.16722651 Acc: 0.96000000\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.26816076 Acc: 0.87636933\n",
      "val Loss: 0.18160347 Acc: 0.93333333\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.25317524 Acc: 0.88732394\n",
      "val Loss: 0.16815366 Acc: 0.94666667\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.22110083 Acc: 0.90766823\n",
      "val Loss: 0.16395682 Acc: 0.94666667\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.26555059 Acc: 0.87949922\n",
      "val Loss: 0.16472681 Acc: 0.96000000\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.26269738 Acc: 0.88888889\n",
      "val Loss: 0.18929269 Acc: 0.94666667\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.27173211 Acc: 0.88575900\n",
      "val Loss: 0.17408251 Acc: 0.96000000\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.22196790 Acc: 0.90766823\n",
      "val Loss: 0.17291519 Acc: 0.96000000\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.29556310 Acc: 0.86541471\n",
      "val Loss: 0.18197950 Acc: 0.94666667\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.23770773 Acc: 0.90140845\n",
      "val Loss: 0.19379417 Acc: 0.93333333\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.26768367 Acc: 0.87167449\n",
      "val Loss: 0.16949013 Acc: 0.96000000\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.25555354 Acc: 0.89827856\n",
      "val Loss: 0.15060833 Acc: 0.94666667\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.22825806 Acc: 0.89671362\n",
      "val Loss: 0.15743350 Acc: 0.94666667\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.22040902 Acc: 0.90453834\n",
      "val Loss: 0.15972427 Acc: 0.94666667\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.27264476 Acc: 0.87949922\n",
      "val Loss: 0.16637380 Acc: 0.96000000\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.22923554 Acc: 0.89514867\n",
      "val Loss: 0.15771935 Acc: 0.96000000\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.23543591 Acc: 0.89045383\n",
      "val Loss: 0.15512515 Acc: 0.96000000\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.25696596 Acc: 0.89984351\n",
      "val Loss: 0.16750939 Acc: 0.96000000\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.23962576 Acc: 0.89201878\n",
      "val Loss: 0.16281788 Acc: 0.96000000\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.23287902 Acc: 0.89201878\n",
      "val Loss: 0.16758834 Acc: 0.96000000\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.24674780 Acc: 0.88262911\n",
      "val Loss: 0.17314227 Acc: 0.96000000\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.23966699 Acc: 0.88575900\n",
      "val Loss: 0.16502090 Acc: 0.96000000\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.22644578 Acc: 0.91392801\n",
      "val Loss: 0.17874621 Acc: 0.94666667\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.25402780 Acc: 0.89514867\n",
      "val Loss: 0.15448144 Acc: 0.94666667\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.25025493 Acc: 0.89201878\n",
      "val Loss: 0.17687224 Acc: 0.94666667\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.24727137 Acc: 0.90766823\n",
      "val Loss: 0.16675554 Acc: 0.94666667\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.24514467 Acc: 0.89671362\n",
      "val Loss: 0.18386822 Acc: 0.92000000\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.21711159 Acc: 0.90297340\n",
      "val Loss: 0.17710043 Acc: 0.96000000\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.25377341 Acc: 0.88732394\n",
      "val Loss: 0.17001899 Acc: 0.96000000\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.24224098 Acc: 0.89201878\n",
      "val Loss: 0.17160229 Acc: 0.94666667\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.21493864 Acc: 0.90923318\n",
      "val Loss: 0.17784201 Acc: 0.92000000\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.26854945 Acc: 0.87480438\n",
      "val Loss: 0.17925087 Acc: 0.94666667\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.23935848 Acc: 0.89358372\n",
      "val Loss: 0.16791915 Acc: 0.94666667\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.24153408 Acc: 0.89514867\n",
      "val Loss: 0.16787421 Acc: 0.94666667\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.22902621 Acc: 0.90766823\n",
      "val Loss: 0.17689952 Acc: 0.96000000\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.23260410 Acc: 0.90453834\n",
      "val Loss: 0.14957771 Acc: 0.94666667\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.25048632 Acc: 0.89827856\n",
      "val Loss: 0.16071861 Acc: 0.96000000\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.25374438 Acc: 0.88732394\n",
      "val Loss: 0.16673393 Acc: 0.93333333\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.24500673 Acc: 0.89827856\n",
      "val Loss: 0.17070024 Acc: 0.93333333\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.23855980 Acc: 0.88106416\n",
      "val Loss: 0.16386497 Acc: 0.94666667\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.25564607 Acc: 0.88888889\n",
      "val Loss: 0.17902645 Acc: 0.94666667\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.25339764 Acc: 0.89827856\n",
      "val Loss: 0.15752737 Acc: 0.94666667\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.23081034 Acc: 0.89827856\n",
      "val Loss: 0.15573215 Acc: 0.96000000\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.26405491 Acc: 0.88262911\n",
      "val Loss: 0.16469376 Acc: 0.96000000\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.24050635 Acc: 0.90453834\n",
      "val Loss: 0.16297453 Acc: 0.93333333\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.26972638 Acc: 0.87480438\n",
      "val Loss: 0.18256633 Acc: 0.93333333\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.23427793 Acc: 0.90766823\n",
      "val Loss: 0.16957568 Acc: 0.94666667\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.21807033 Acc: 0.90923318\n",
      "val Loss: 0.16298189 Acc: 0.94666667\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.26441863 Acc: 0.88106416\n",
      "val Loss: 0.17362858 Acc: 0.94666667\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.24043173 Acc: 0.89671362\n",
      "val Loss: 0.17821126 Acc: 0.94666667\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.22493543 Acc: 0.90453834\n",
      "val Loss: 0.18378608 Acc: 0.93333333\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.27298371 Acc: 0.87636933\n",
      "val Loss: 0.16578494 Acc: 0.94666667\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.21894625 Acc: 0.90610329\n",
      "val Loss: 0.16289781 Acc: 0.93333333\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.23400675 Acc: 0.89984351\n",
      "val Loss: 0.17041653 Acc: 0.94666667\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.24270456 Acc: 0.88888889\n",
      "val Loss: 0.16818500 Acc: 0.93333333\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.24290665 Acc: 0.89358372\n",
      "val Loss: 0.16758999 Acc: 0.94666667\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.22608743 Acc: 0.90766823\n",
      "val Loss: 0.16257833 Acc: 0.96000000\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.25909871 Acc: 0.89201878\n",
      "val Loss: 0.18662794 Acc: 0.94666667\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.25071478 Acc: 0.88732394\n",
      "val Loss: 0.20026634 Acc: 0.93333333\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.25733176 Acc: 0.87949922\n",
      "val Loss: 0.16930416 Acc: 0.94666667\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.23958258 Acc: 0.89045383\n",
      "val Loss: 0.17235249 Acc: 0.93333333\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.25418390 Acc: 0.88575900\n",
      "val Loss: 0.16590644 Acc: 0.94666667\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.23741119 Acc: 0.89201878\n",
      "val Loss: 0.18113413 Acc: 0.94666667\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.23856792 Acc: 0.90297340\n",
      "val Loss: 0.18319990 Acc: 0.94666667\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.26090011 Acc: 0.87167449\n",
      "val Loss: 0.16336787 Acc: 0.94666667\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.26915671 Acc: 0.87636933\n",
      "val Loss: 0.16891953 Acc: 0.96000000\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.25283553 Acc: 0.88262911\n",
      "val Loss: 0.16494953 Acc: 0.93333333\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.30527256 Acc: 0.86697966\n",
      "val Loss: 0.16365228 Acc: 0.94666667\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.26404484 Acc: 0.87793427\n",
      "val Loss: 0.17596993 Acc: 0.94666667\n",
      "\n",
      "Training complete in 34m 10s\n",
      "Best val Acc: 0.96000000 Best val loss: 0.16318206\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/MixNet_Extra_Large_Dataset_Yang.pth'\n",
    "#CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/MixNet_Extra_Large_Dataset_Soares.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 200,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
