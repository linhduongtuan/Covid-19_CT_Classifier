{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 17905, 'test': 13511}\n",
      "cuda:1\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 110\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([#('fc1', nn.Linear(1280, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#show our model architechture and send to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.10018904718032959, Val accuracy: 0.9635112130856339\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.19605275 Acc: 0.92549567\n",
      "test Loss: 0.09878928 Acc: 0.96351121\n",
      "New best model found!\n",
      "New record loss: 0.09878928356226108, previous record loss: 0.10018904718032959\n",
      "New record loss is SAVED: 0.09878928356226108\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.19359828 Acc: 0.92839989\n",
      "test Loss: 0.09477303 Acc: 0.96543557\n",
      "New best model found!\n",
      "New record loss: 0.09477303152799553, previous record loss: 0.09878928356226108\n",
      "New record loss is SAVED: 0.09477303152799553\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.18681816 Acc: 0.92957275\n",
      "test Loss: 0.10338013 Acc: 0.96380727\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.18628708 Acc: 0.93052220\n",
      "test Loss: 0.10472098 Acc: 0.96291910\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.18376893 Acc: 0.93046635\n",
      "test Loss: 0.08645943 Acc: 0.96891422\n",
      "New best model found!\n",
      "New record loss: 0.08645943048620425, previous record loss: 0.09477303152799553\n",
      "New record loss is SAVED: 0.08645943048620425\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.18381967 Acc: 0.93013125\n",
      "test Loss: 0.08675238 Acc: 0.96810007\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.18479565 Acc: 0.93035465\n",
      "test Loss: 0.08929119 Acc: 0.96765598\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.18139292 Acc: 0.93180676\n",
      "test Loss: 0.08338104 Acc: 0.97024647\n",
      "New best model found!\n",
      "New record loss: 0.08338104473732577, previous record loss: 0.08645943048620425\n",
      "New record loss is SAVED: 0.08338104473732577\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.17851712 Acc: 0.93364982\n",
      "test Loss: 0.08859588 Acc: 0.96639775\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.18025272 Acc: 0.93052220\n",
      "test Loss: 0.08232228 Acc: 0.97120864\n",
      "New best model found!\n",
      "New record loss: 0.08232228186892224, previous record loss: 0.08338104473732577\n",
      "New record loss is SAVED: 0.08232228186892224\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.17614655 Acc: 0.93314717\n",
      "test Loss: 0.09431956 Acc: 0.96499149\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.17674090 Acc: 0.93499023\n",
      "test Loss: 0.08042450 Acc: 0.97239287\n",
      "New best model found!\n",
      "New record loss: 0.08042450358188309, previous record loss: 0.08232228186892224\n",
      "New record loss is SAVED: 0.08042450358188309\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.17717536 Acc: 0.93465512\n",
      "test Loss: 0.09224395 Acc: 0.96684183\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.16923246 Acc: 0.93850880\n",
      "test Loss: 0.07804102 Acc: 0.97231885\n",
      "New best model found!\n",
      "New record loss: 0.07804101601713953, previous record loss: 0.08042450358188309\n",
      "New record loss is SAVED: 0.07804101601713953\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.17244950 Acc: 0.93420832\n",
      "test Loss: 0.07526115 Acc: 0.97276293\n",
      "New best model found!\n",
      "New record loss: 0.07526114836156514, previous record loss: 0.07804101601713953\n",
      "New record loss is SAVED: 0.07526114836156514\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.17197350 Acc: 0.93448757\n",
      "test Loss: 0.07606407 Acc: 0.97291096\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.16287890 Acc: 0.93945825\n",
      "test Loss: 0.07330591 Acc: 0.97402117\n",
      "New best model found!\n",
      "New record loss: 0.07330591289596619, previous record loss: 0.07526114836156514\n",
      "New record loss is SAVED: 0.07330591289596619\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.16074263 Acc: 0.93945825\n",
      "test Loss: 0.07077470 Acc: 0.97453926\n",
      "New best model found!\n",
      "New record loss: 0.07077469627075347, previous record loss: 0.07330591289596619\n",
      "New record loss is SAVED: 0.07077469627075347\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.15471829 Acc: 0.94051941\n",
      "test Loss: 0.07221250 Acc: 0.97342906\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.15926900 Acc: 0.93973750\n",
      "test Loss: 0.07249052 Acc: 0.97350307\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.15119269 Acc: 0.94375873\n",
      "test Loss: 0.07128479 Acc: 0.97416920\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.16033099 Acc: 0.93901145\n",
      "test Loss: 0.06805621 Acc: 0.97542743\n",
      "New best model found!\n",
      "New record loss: 0.06805620676917201, previous record loss: 0.07077469627075347\n",
      "New record loss is SAVED: 0.06805620676917201\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.15489638 Acc: 0.94253002\n",
      "test Loss: 0.06727311 Acc: 0.97587151\n",
      "New best model found!\n",
      "New record loss: 0.06727310917688664, previous record loss: 0.06805620676917201\n",
      "New record loss is SAVED: 0.06727310917688664\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.15357041 Acc: 0.94325607\n",
      "test Loss: 0.06596529 Acc: 0.97624158\n",
      "New best model found!\n",
      "New record loss: 0.06596528677802375, previous record loss: 0.06727310917688664\n",
      "New record loss is SAVED: 0.06596528677802375\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.15708904 Acc: 0.94035186\n",
      "test Loss: 0.06799903 Acc: 0.97527940\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.15371697 Acc: 0.94225077\n",
      "test Loss: 0.06685086 Acc: 0.97579750\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.15074169 Acc: 0.94392628\n",
      "test Loss: 0.06825392 Acc: 0.97587151\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.15398024 Acc: 0.94124546\n",
      "test Loss: 0.06463620 Acc: 0.97720376\n",
      "New best model found!\n",
      "New record loss: 0.06463619724581098, previous record loss: 0.06596528677802375\n",
      "New record loss is SAVED: 0.06463619724581098\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.15208435 Acc: 0.94280927\n",
      "test Loss: 0.06377004 Acc: 0.97668566\n",
      "New best model found!\n",
      "New record loss: 0.06377003636936583, previous record loss: 0.06463619724581098\n",
      "New record loss is SAVED: 0.06377003636936583\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.15054517 Acc: 0.94230662\n",
      "test Loss: 0.06418254 Acc: 0.97683369\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.15168356 Acc: 0.94325607\n",
      "test Loss: 0.06537591 Acc: 0.97631559\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.14916699 Acc: 0.94431723\n",
      "test Loss: 0.06303613 Acc: 0.97698172\n",
      "New best model found!\n",
      "New record loss: 0.06303613410657283, previous record loss: 0.06377003636936583\n",
      "New record loss is SAVED: 0.06303613410657283\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.15189544 Acc: 0.94269757\n",
      "test Loss: 0.06248452 Acc: 0.97749981\n",
      "New best model found!\n",
      "New record loss: 0.06248452374080917, previous record loss: 0.06303613410657283\n",
      "New record loss is SAVED: 0.06248452374080917\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.15001884 Acc: 0.94392628\n",
      "test Loss: 0.06352944 Acc: 0.97742580\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.14569470 Acc: 0.94638369\n",
      "test Loss: 0.06158696 Acc: 0.97757383\n",
      "New best model found!\n",
      "New record loss: 0.061586961626618315, previous record loss: 0.06248452374080917\n",
      "New record loss is SAVED: 0.061586961626618315\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.15335853 Acc: 0.94280927\n",
      "test Loss: 0.06202909 Acc: 0.97735179\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.14930463 Acc: 0.94336777\n",
      "test Loss: 0.06143381 Acc: 0.97786988\n",
      "New best model found!\n",
      "New record loss: 0.061433813424870434, previous record loss: 0.061586961626618315\n",
      "New record loss is SAVED: 0.061433813424870434\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.14763264 Acc: 0.94470818\n",
      "test Loss: 0.06041753 Acc: 0.97816594\n",
      "New best model found!\n",
      "New record loss: 0.060417533409693865, previous record loss: 0.061433813424870434\n",
      "New record loss is SAVED: 0.060417533409693865\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.15058830 Acc: 0.94493158\n",
      "test Loss: 0.06307266 Acc: 0.97698172\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.14787179 Acc: 0.94347948\n",
      "test Loss: 0.06156952 Acc: 0.97720376\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.14945173 Acc: 0.94308852\n",
      "test Loss: 0.05972993 Acc: 0.97883206\n",
      "New best model found!\n",
      "New record loss: 0.059729932315013994, previous record loss: 0.060417533409693865\n",
      "New record loss is SAVED: 0.059729932315013994\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.15550195 Acc: 0.94091036\n",
      "test Loss: 0.06247521 Acc: 0.97638961\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.14407454 Acc: 0.94409383\n",
      "test Loss: 0.06008723 Acc: 0.97749981\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.14761132 Acc: 0.94498743\n",
      "test Loss: 0.05886176 Acc: 0.97861002\n",
      "New best model found!\n",
      "New record loss: 0.05886176322958302, previous record loss: 0.059729932315013994\n",
      "New record loss is SAVED: 0.05886176322958302\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.14756922 Acc: 0.94381458\n",
      "test Loss: 0.06022661 Acc: 0.97698172\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.15039930 Acc: 0.94532254\n",
      "test Loss: 0.06023256 Acc: 0.97705573\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.14393693 Acc: 0.94627199\n",
      "test Loss: 0.05966789 Acc: 0.97742580\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.14587033 Acc: 0.94297682\n",
      "test Loss: 0.05898020 Acc: 0.97727777\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.14699212 Acc: 0.94442893\n",
      "test Loss: 0.05809716 Acc: 0.97868404\n",
      "New best model found!\n",
      "New record loss: 0.05809715975047836, previous record loss: 0.05886176322958302\n",
      "New record loss is SAVED: 0.05809715975047836\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.14417153 Acc: 0.94481988\n",
      "test Loss: 0.05734514 Acc: 0.97831397\n",
      "New best model found!\n",
      "New record loss: 0.057345137285181265, previous record loss: 0.05809715975047836\n",
      "New record loss is SAVED: 0.057345137285181265\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.14473010 Acc: 0.94582519\n",
      "test Loss: 0.05956681 Acc: 0.97735179\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.14283164 Acc: 0.94733315\n",
      "test Loss: 0.05636985 Acc: 0.97949819\n",
      "New best model found!\n",
      "New record loss: 0.056369845624774616, previous record loss: 0.057345137285181265\n",
      "New record loss is SAVED: 0.056369845624774616\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.14134301 Acc: 0.94621614\n",
      "test Loss: 0.05636395 Acc: 0.97972023\n",
      "New best model found!\n",
      "New record loss: 0.05636395283314144, previous record loss: 0.056369845624774616\n",
      "New record loss is SAVED: 0.05636395283314144\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.14541081 Acc: 0.94549009\n",
      "test Loss: 0.05616172 Acc: 0.97935016\n",
      "New best model found!\n",
      "New record loss: 0.05616171676402229, previous record loss: 0.05636395283314144\n",
      "New record loss is SAVED: 0.05616171676402229\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.14652944 Acc: 0.94487573\n",
      "test Loss: 0.05593945 Acc: 0.97883206\n",
      "New best model found!\n",
      "New record loss: 0.05593944811168789, previous record loss: 0.05616171676402229\n",
      "New record loss is SAVED: 0.05593944811168789\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.14424734 Acc: 0.94493158\n",
      "test Loss: 0.05839443 Acc: 0.97779587\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.14121453 Acc: 0.94705390\n",
      "test Loss: 0.05631833 Acc: 0.97853601\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.14050235 Acc: 0.94699805\n",
      "test Loss: 0.05448595 Acc: 0.97979424\n",
      "New best model found!\n",
      "New record loss: 0.05448594504883099, previous record loss: 0.05593944811168789\n",
      "New record loss is SAVED: 0.05448594504883099\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.14526954 Acc: 0.94649539\n",
      "test Loss: 0.05633599 Acc: 0.97868404\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.14051491 Acc: 0.94694219\n",
      "test Loss: 0.05560802 Acc: 0.97905410\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.13872913 Acc: 0.94939961\n",
      "test Loss: 0.05680032 Acc: 0.97823995\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.14544200 Acc: 0.94554594\n",
      "test Loss: 0.05545023 Acc: 0.97883206\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.14375690 Acc: 0.94532254\n",
      "test Loss: 0.05596264 Acc: 0.97861002\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.14360930 Acc: 0.94727730\n",
      "test Loss: 0.05520712 Acc: 0.97942417\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.14043359 Acc: 0.94884111\n",
      "test Loss: 0.05593156 Acc: 0.97831397\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.14193626 Acc: 0.94744485\n",
      "test Loss: 0.05667188 Acc: 0.97816594\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.14102187 Acc: 0.94560179\n",
      "test Loss: 0.05492057 Acc: 0.97935016\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.14457034 Acc: 0.94638369\n",
      "test Loss: 0.05650317 Acc: 0.97831397\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.13984135 Acc: 0.94671879\n",
      "test Loss: 0.05502560 Acc: 0.97942417\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.14377452 Acc: 0.94710975\n",
      "test Loss: 0.05553768 Acc: 0.97875805\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.13705315 Acc: 0.94956716\n",
      "test Loss: 0.05534998 Acc: 0.97890608\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.14506649 Acc: 0.94537839\n",
      "test Loss: 0.05474685 Acc: 0.97927615\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.13993783 Acc: 0.94878526\n",
      "test Loss: 0.05465760 Acc: 0.97949819\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.14052550 Acc: 0.94750070\n",
      "test Loss: 0.05374776 Acc: 0.98031234\n",
      "New best model found!\n",
      "New record loss: 0.05374776272333557, previous record loss: 0.05448594504883099\n",
      "New record loss is SAVED: 0.05374776272333557\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.14133657 Acc: 0.94744485\n",
      "test Loss: 0.05604430 Acc: 0.97868404\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.14572368 Acc: 0.94610444\n",
      "test Loss: 0.05540666 Acc: 0.97912812\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.13787622 Acc: 0.94660709\n",
      "test Loss: 0.05570575 Acc: 0.97898009\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.13753078 Acc: 0.94822675\n",
      "test Loss: 0.05387809 Acc: 0.98009030\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.14122690 Acc: 0.94738900\n",
      "test Loss: 0.05434580 Acc: 0.97972023\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.14546899 Acc: 0.94454063\n",
      "test Loss: 0.05731027 Acc: 0.97801791\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.14065665 Acc: 0.94934376\n",
      "test Loss: 0.05405671 Acc: 0.97979424\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.14238159 Acc: 0.94677464\n",
      "test Loss: 0.05355084 Acc: 0.98016431\n",
      "New best model found!\n",
      "New record loss: 0.05355084289920519, previous record loss: 0.05374776272333557\n",
      "New record loss is SAVED: 0.05355084289920519\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.14422468 Acc: 0.94582519\n",
      "test Loss: 0.05394419 Acc: 0.98023832\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.13894564 Acc: 0.94789165\n",
      "test Loss: 0.05584872 Acc: 0.97838798\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.14083260 Acc: 0.94750070\n",
      "test Loss: 0.05628384 Acc: 0.97868404\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.14629522 Acc: 0.94465233\n",
      "test Loss: 0.05653805 Acc: 0.97838798\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.14350735 Acc: 0.94699805\n",
      "test Loss: 0.05376788 Acc: 0.98016431\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.14061829 Acc: 0.94794750\n",
      "test Loss: 0.05449980 Acc: 0.97927615\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.13809837 Acc: 0.94811505\n",
      "test Loss: 0.05462862 Acc: 0.97905410\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.14033027 Acc: 0.94660709\n",
      "test Loss: 0.05580751 Acc: 0.97883206\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.14713395 Acc: 0.94476403\n",
      "test Loss: 0.05582757 Acc: 0.97898009\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.13974020 Acc: 0.94599274\n",
      "test Loss: 0.05430288 Acc: 0.97949819\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.13953339 Acc: 0.94621614\n",
      "test Loss: 0.05534057 Acc: 0.97942417\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.13871213 Acc: 0.94833845\n",
      "test Loss: 0.05602593 Acc: 0.97868404\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights/EfficientNet_B0_NS_Covid-19_V2.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = None #torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
