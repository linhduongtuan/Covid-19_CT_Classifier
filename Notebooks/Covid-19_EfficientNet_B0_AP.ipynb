{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 17905, 'test': 13511}\n",
      "cuda:0\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 100\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('tf_efficientnet_b0_ap', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([#('fc1', nn.Linear(1280, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#show our model architechture and send to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.75081961 Acc: 0.75476124\n",
      "test Loss: 0.26793210 Acc: 0.90555843\n",
      "New best model found!\n",
      "New record loss: 0.2679321019771794, previous record loss: inf\n",
      "New record loss is SAVED: 0.2679321019771794\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.41068204 Acc: 0.84088244\n",
      "test Loss: 0.25773142 Acc: 0.90622456\n",
      "New best model found!\n",
      "New record loss: 0.2577314151157485, previous record loss: 0.2679321019771794\n",
      "New record loss is SAVED: 0.2577314151157485\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.36431015 Acc: 0.86115610\n",
      "test Loss: 0.20300549 Acc: 0.92835467\n",
      "New best model found!\n",
      "New record loss: 0.2030054853838807, previous record loss: 0.2577314151157485\n",
      "New record loss is SAVED: 0.2030054853838807\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.34167937 Acc: 0.86942195\n",
      "test Loss: 0.21242329 Acc: 0.92613426\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.32001269 Acc: 0.87886065\n",
      "test Loss: 0.25379386 Acc: 0.91421804\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.30347220 Acc: 0.88695895\n",
      "test Loss: 0.18016374 Acc: 0.93486789\n",
      "New best model found!\n",
      "New record loss: 0.18016373983653758, previous record loss: 0.2030054853838807\n",
      "New record loss is SAVED: 0.18016373983653758\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.29001667 Acc: 0.89243228\n",
      "test Loss: 0.18297110 Acc: 0.93879062\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.28943031 Acc: 0.89025412\n",
      "test Loss: 0.16138876 Acc: 0.94123307\n",
      "New best model found!\n",
      "New record loss: 0.16138876258475504, previous record loss: 0.18016373983653758\n",
      "New record loss is SAVED: 0.16138876258475504\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.27285937 Acc: 0.89650935\n",
      "test Loss: 0.22239948 Acc: 0.91991710\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.27709055 Acc: 0.89500140\n",
      "test Loss: 0.15286666 Acc: 0.94426763\n",
      "New best model found!\n",
      "New record loss: 0.1528666575616633, previous record loss: 0.16138876258475504\n",
      "New record loss is SAVED: 0.1528666575616633\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.26459944 Acc: 0.90069813\n",
      "test Loss: 0.14344096 Acc: 0.94804234\n",
      "New best model found!\n",
      "New record loss: 0.14344095727841688, previous record loss: 0.1528666575616633\n",
      "New record loss is SAVED: 0.14344095727841688\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.25339665 Acc: 0.90460765\n",
      "test Loss: 0.15136322 Acc: 0.94522981\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.25792962 Acc: 0.90304384\n",
      "test Loss: 0.16113788 Acc: 0.94263933\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.26600961 Acc: 0.89885507\n",
      "test Loss: 0.14949267 Acc: 0.94819036\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.25231304 Acc: 0.90404915\n",
      "test Loss: 0.13757910 Acc: 0.95070683\n",
      "New best model found!\n",
      "New record loss: 0.137579102466456, previous record loss: 0.14344095727841688\n",
      "New record loss is SAVED: 0.137579102466456\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.24453399 Acc: 0.90924323\n",
      "test Loss: 0.12802570 Acc: 0.95588780\n",
      "New best model found!\n",
      "New record loss: 0.1280257039063821, previous record loss: 0.137579102466456\n",
      "New record loss is SAVED: 0.1280257039063821\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.23759608 Acc: 0.91063949\n",
      "test Loss: 0.13006702 Acc: 0.95351935\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.23544277 Acc: 0.91304105\n",
      "test Loss: 0.13688435 Acc: 0.95270520\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.23550296 Acc: 0.91209159\n",
      "test Loss: 0.12928180 Acc: 0.95285323\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.22985730 Acc: 0.91248255\n",
      "test Loss: 0.12695238 Acc: 0.95359337\n",
      "New best model found!\n",
      "New record loss: 0.1269523845175596, previous record loss: 0.1280257039063821\n",
      "New record loss is SAVED: 0.1269523845175596\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.23148578 Acc: 0.91415806\n",
      "test Loss: 0.13210161 Acc: 0.95078085\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.23678400 Acc: 0.91147724\n",
      "test Loss: 0.14867233 Acc: 0.94567390\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.22563609 Acc: 0.91538676\n",
      "test Loss: 0.13089218 Acc: 0.95366738\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.22099083 Acc: 0.91695057\n",
      "test Loss: 0.12456104 Acc: 0.95425949\n",
      "New best model found!\n",
      "New record loss: 0.12456103909522764, previous record loss: 0.1269523845175596\n",
      "New record loss is SAVED: 0.12456103909522764\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.21721902 Acc: 0.91678302\n",
      "test Loss: 0.12510728 Acc: 0.95670195\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.21774841 Acc: 0.91940799\n",
      "test Loss: 0.13107753 Acc: 0.95462956\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.21406212 Acc: 0.92125105\n",
      "test Loss: 0.10950055 Acc: 0.96180890\n",
      "New best model found!\n",
      "New record loss: 0.10950054582609571, previous record loss: 0.12456103909522764\n",
      "New record loss is SAVED: 0.10950054582609571\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.21036301 Acc: 0.92119520\n",
      "test Loss: 0.11594306 Acc: 0.95899637\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.20924112 Acc: 0.92376431\n",
      "test Loss: 0.12304703 Acc: 0.95603582\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.20815277 Acc: 0.92275901\n",
      "test Loss: 0.11731995 Acc: 0.95618385\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.19642348 Acc: 0.92611003\n",
      "test Loss: 0.09996743 Acc: 0.96558360\n",
      "New best model found!\n",
      "New record loss: 0.0999674272203276, previous record loss: 0.10950054582609571\n",
      "New record loss is SAVED: 0.0999674272203276\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.18894626 Acc: 0.93041050\n",
      "test Loss: 0.10102448 Acc: 0.96499149\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.19033743 Acc: 0.92823234\n",
      "test Loss: 0.09756901 Acc: 0.96698986\n",
      "New best model found!\n",
      "New record loss: 0.09756900503570821, previous record loss: 0.0999674272203276\n",
      "New record loss is SAVED: 0.09756900503570821\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.18632023 Acc: 0.93180676\n",
      "test Loss: 0.09528692 Acc: 0.96676782\n",
      "New best model found!\n",
      "New record loss: 0.09528692321344244, previous record loss: 0.09756900503570821\n",
      "New record loss is SAVED: 0.09528692321344244\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.18157943 Acc: 0.93370567\n",
      "test Loss: 0.09520996 Acc: 0.96713789\n",
      "New best model found!\n",
      "New record loss: 0.09520996381002665, previous record loss: 0.09528692321344244\n",
      "New record loss is SAVED: 0.09520996381002665\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.18324982 Acc: 0.93152751\n",
      "test Loss: 0.09577320 Acc: 0.96713789\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.18105330 Acc: 0.93348227\n",
      "test Loss: 0.09556664 Acc: 0.96787803\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.18824562 Acc: 0.93197431\n",
      "test Loss: 0.09372617 Acc: 0.96854415\n",
      "New best model found!\n",
      "New record loss: 0.09372617311452537, previous record loss: 0.09520996381002665\n",
      "New record loss is SAVED: 0.09372617311452537\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.18290001 Acc: 0.93303546\n",
      "test Loss: 0.09674722 Acc: 0.96632374\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.17980896 Acc: 0.93348227\n",
      "test Loss: 0.09278226 Acc: 0.96810007\n",
      "New best model found!\n",
      "New record loss: 0.09278226327546722, previous record loss: 0.09372617311452537\n",
      "New record loss is SAVED: 0.09278226327546722\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.18242249 Acc: 0.93007540\n",
      "test Loss: 0.09126097 Acc: 0.96854415\n",
      "New best model found!\n",
      "New record loss: 0.0912609688986467, previous record loss: 0.09278226327546722\n",
      "New record loss is SAVED: 0.0912609688986467\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.17888655 Acc: 0.93331472\n",
      "test Loss: 0.09205109 Acc: 0.96913626\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.17271422 Acc: 0.93532533\n",
      "test Loss: 0.09284352 Acc: 0.96861816\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.17882798 Acc: 0.93409662\n",
      "test Loss: 0.09088953 Acc: 0.96884020\n",
      "New best model found!\n",
      "New record loss: 0.09088953240496023, previous record loss: 0.0912609688986467\n",
      "New record loss is SAVED: 0.09088953240496023\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.17931878 Acc: 0.93381737\n",
      "test Loss: 0.09342772 Acc: 0.96743394\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.18126392 Acc: 0.93376152\n",
      "test Loss: 0.09101320 Acc: 0.96832211\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.17394572 Acc: 0.93633063\n",
      "test Loss: 0.08865376 Acc: 0.96906225\n",
      "New best model found!\n",
      "New record loss: 0.08865375563598263, previous record loss: 0.09088953240496023\n",
      "New record loss is SAVED: 0.08865375563598263\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.17622842 Acc: 0.93331472\n",
      "test Loss: 0.08934097 Acc: 0.96943231\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.17613212 Acc: 0.93532533\n",
      "test Loss: 0.09447232 Acc: 0.96602768\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights/EfficientNet_B0_AP_Covid-19_V3.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
