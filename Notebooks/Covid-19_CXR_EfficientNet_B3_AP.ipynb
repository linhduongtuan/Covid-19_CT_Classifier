{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 12884, 'val': 2758}\n",
      "cuda:1\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19/CXR_20200630/'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 50\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('tf_efficientnet_b3_ap', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14864075\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "                                 #('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.20218865201213407, Val accuracy: 0.9498194945848375\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.16230715 Acc: 0.93938218\n",
      "val Loss: 0.19432332 Acc: 0.94452502\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.16083687 Acc: 0.93945980\n",
      "val Loss: 0.19758417 Acc: 0.94525018\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.15598166 Acc: 0.94178826\n",
      "val Loss: 0.19188744 Acc: 0.94379985\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.15935498 Acc: 0.94031357\n",
      "val Loss: 0.21363279 Acc: 0.94089920\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.15388998 Acc: 0.94248681\n",
      "val Loss: 0.22881422 Acc: 0.94198695\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.16487841 Acc: 0.93751940\n",
      "val Loss: 0.23528329 Acc: 0.93509790\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.15668748 Acc: 0.93790748\n",
      "val Loss: 0.28534376 Acc: 0.91116751\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.14850331 Acc: 0.94388389\n",
      "val Loss: 0.23273051 Acc: 0.94307469\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.15670202 Acc: 0.94132257\n",
      "val Loss: 0.21940187 Acc: 0.94379985\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.15788119 Acc: 0.94178826\n",
      "val Loss: 0.19423300 Acc: 0.94452502\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.15113512 Acc: 0.94295250\n",
      "val Loss: 0.20067555 Acc: 0.94307469\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.15056404 Acc: 0.94434958\n",
      "val Loss: 0.20856763 Acc: 0.94488760\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.15447191 Acc: 0.94015834\n",
      "val Loss: 0.21558586 Acc: 0.93908629\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.14435880 Acc: 0.94543620\n",
      "val Loss: 0.22528940 Acc: 0.93836113\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.14788173 Acc: 0.94279727\n",
      "val Loss: 0.22544963 Acc: 0.94017404\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.14475959 Acc: 0.94535858\n",
      "val Loss: 0.23632996 Acc: 0.93328499\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.13996827 Acc: 0.94760944\n",
      "val Loss: 0.25532769 Acc: 0.94271211\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.14388731 Acc: 0.94636759\n",
      "val Loss: 0.25197659 Acc: 0.94053662\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.14102515 Acc: 0.94823036\n",
      "val Loss: 0.27730354 Acc: 0.93509790\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.13873489 Acc: 0.94737659\n",
      "val Loss: 0.28072267 Acc: 0.93836113\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.14574914 Acc: 0.94605713\n",
      "val Loss: 0.21870952 Acc: 0.94271211\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.13789649 Acc: 0.94923937\n",
      "val Loss: 0.25021106 Acc: 0.93183466\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.13990792 Acc: 0.94628997\n",
      "val Loss: 0.30784945 Acc: 0.93799855\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.13950140 Acc: 0.94597951\n",
      "val Loss: 0.33241015 Acc: 0.92893401\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.12502366 Acc: 0.95296492\n",
      "val Loss: 0.26080844 Acc: 0.93799855\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.11570861 Acc: 0.95676808\n",
      "val Loss: 0.25491255 Acc: 0.93944888\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.11717896 Acc: 0.95506054\n",
      "val Loss: 0.25831976 Acc: 0.93944888\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.11336844 Acc: 0.95692332\n",
      "val Loss: 0.26231542 Acc: 0.94017404\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.10620957 Acc: 0.96002794\n",
      "val Loss: 0.26455457 Acc: 0.94053662\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.10572459 Acc: 0.96134741\n",
      "val Loss: 0.25720114 Acc: 0.94089920\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.10351264 Acc: 0.96064887\n",
      "val Loss: 0.25635941 Acc: 0.94271211\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.10733352 Acc: 0.96150264\n",
      "val Loss: 0.26008859 Acc: 0.94234953\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.10303825 Acc: 0.96227880\n",
      "val Loss: 0.28058452 Acc: 0.94126178\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.09975425 Acc: 0.96251164\n",
      "val Loss: 0.26698098 Acc: 0.94126178\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.09969886 Acc: 0.96150264\n",
      "val Loss: 0.27002552 Acc: 0.94307469\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.09558127 Acc: 0.96289972\n",
      "val Loss: 0.27087694 Acc: 0.94343727\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.10319448 Acc: 0.96289972\n",
      "val Loss: 0.27283885 Acc: 0.94162437\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.09771001 Acc: 0.96204595\n",
      "val Loss: 0.28901267 Acc: 0.94198695\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.09814230 Acc: 0.96266687\n",
      "val Loss: 0.27290366 Acc: 0.94198695\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.09254395 Acc: 0.96577150\n",
      "val Loss: 0.27488922 Acc: 0.94271211\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.09848172 Acc: 0.96204595\n",
      "val Loss: 0.27326978 Acc: 0.94198695\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.09387098 Acc: 0.96654766\n",
      "val Loss: 0.28032861 Acc: 0.93981146\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.09960018 Acc: 0.96080410\n",
      "val Loss: 0.27853438 Acc: 0.94017404\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.09655174 Acc: 0.96631481\n",
      "val Loss: 0.28377227 Acc: 0.94198695\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.09311317 Acc: 0.96414157\n",
      "val Loss: 0.27392242 Acc: 0.94126178\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.09037863 Acc: 0.96724620\n",
      "val Loss: 0.28080988 Acc: 0.94017404\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.09552761 Acc: 0.96344303\n",
      "val Loss: 0.29305213 Acc: 0.94017404\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.08613552 Acc: 0.96786712\n",
      "val Loss: 0.28441152 Acc: 0.94017404\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.09447379 Acc: 0.96460726\n",
      "val Loss: 0.28785799 Acc: 0.93908629\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.09360490 Acc: 0.96390872\n",
      "val Loss: 0.28796283 Acc: 0.94053662\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.09095062 Acc: 0.96615958\n",
      "val Loss: 0.29304858 Acc: 0.94053662\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.08659943 Acc: 0.96716858\n",
      "val Loss: 0.29099830 Acc: 0.94053662\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.08788423 Acc: 0.96786712\n",
      "val Loss: 0.29336161 Acc: 0.93944888\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.09417640 Acc: 0.96491773\n",
      "val Loss: 0.29643489 Acc: 0.93836113\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.09024991 Acc: 0.96491773\n",
      "val Loss: 0.28917167 Acc: 0.94271211\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.09407079 Acc: 0.96375349\n",
      "val Loss: 0.29288918 Acc: 0.94452502\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.08877703 Acc: 0.96639242\n",
      "val Loss: 0.28766455 Acc: 0.94198695\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.09087545 Acc: 0.96538342\n",
      "val Loss: 0.29238717 Acc: 0.94126178\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.09233926 Acc: 0.96398634\n",
      "val Loss: 0.28578083 Acc: 0.94089920\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.09148208 Acc: 0.96569388\n",
      "val Loss: 0.29801641 Acc: 0.94234953\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.08594447 Acc: 0.96763428\n",
      "val Loss: 0.29640143 Acc: 0.94416244\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.08848398 Acc: 0.96515057\n",
      "val Loss: 0.31824437 Acc: 0.94089920\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.08653962 Acc: 0.96716858\n",
      "val Loss: 0.29226097 Acc: 0.94307469\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.08838302 Acc: 0.96608196\n",
      "val Loss: 0.28768100 Acc: 0.94198695\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.08662297 Acc: 0.96693573\n",
      "val Loss: 0.29797428 Acc: 0.94162437\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.08360482 Acc: 0.96825520\n",
      "val Loss: 0.30359215 Acc: 0.93944888\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.08323524 Acc: 0.96740143\n",
      "val Loss: 0.30559655 Acc: 0.94162437\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.09041278 Acc: 0.96584912\n",
      "val Loss: 0.30395556 Acc: 0.94017404\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.07840828 Acc: 0.97104936\n",
      "val Loss: 0.31884230 Acc: 0.94089920\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.08055735 Acc: 0.96763428\n",
      "val Loss: 0.31779168 Acc: 0.94234953\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.08401112 Acc: 0.96740143\n",
      "val Loss: 0.31746583 Acc: 0.94089920\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.08221216 Acc: 0.96856566\n",
      "val Loss: 0.31240766 Acc: 0.94271211\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.08157318 Acc: 0.96910897\n",
      "val Loss: 0.32469946 Acc: 0.94234953\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.08470601 Acc: 0.96887613\n",
      "val Loss: 0.31516803 Acc: 0.94271211\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.08505822 Acc: 0.96833282\n",
      "val Loss: 0.30391542 Acc: 0.94379985\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.08310604 Acc: 0.96833282\n",
      "val Loss: 0.29365949 Acc: 0.94416244\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.08799290 Acc: 0.96716858\n",
      "val Loss: 0.31350167 Acc: 0.94452502\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.08293977 Acc: 0.96949705\n",
      "val Loss: 0.31124315 Acc: 0.94343727\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.08570184 Acc: 0.96841043\n",
      "val Loss: 0.31203059 Acc: 0.94488760\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.08264776 Acc: 0.96809997\n",
      "val Loss: 0.31141621 Acc: 0.94343727\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.08012418 Acc: 0.96934182\n",
      "val Loss: 0.32625637 Acc: 0.94271211\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.08339000 Acc: 0.96755666\n",
      "val Loss: 0.31899475 Acc: 0.94307469\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.08344555 Acc: 0.96841043\n",
      "val Loss: 0.30421290 Acc: 0.94162437\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.08168863 Acc: 0.96957467\n",
      "val Loss: 0.31239930 Acc: 0.94126178\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.07923962 Acc: 0.97190314\n",
      "val Loss: 0.30042921 Acc: 0.94307469\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.08439197 Acc: 0.96802235\n",
      "val Loss: 0.31113772 Acc: 0.94126178\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.07672091 Acc: 0.96996274\n",
      "val Loss: 0.32924785 Acc: 0.94162437\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.08454174 Acc: 0.96864328\n",
      "val Loss: 0.31604527 Acc: 0.94198695\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.07669465 Acc: 0.97221360\n",
      "val Loss: 0.31560822 Acc: 0.94234953\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.07525468 Acc: 0.97004036\n",
      "val Loss: 0.31703059 Acc: 0.94053662\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.08240543 Acc: 0.97066129\n",
      "val Loss: 0.31492783 Acc: 0.94234953\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.08515910 Acc: 0.96802235\n",
      "val Loss: 0.31246700 Acc: 0.94053662\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.08166417 Acc: 0.97151506\n",
      "val Loss: 0.33842179 Acc: 0.94379985\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.08012329 Acc: 0.96825520\n",
      "val Loss: 0.32658479 Acc: 0.94162437\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.07563425 Acc: 0.97198075\n",
      "val Loss: 0.32761619 Acc: 0.94198695\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.08197928 Acc: 0.96972990\n",
      "val Loss: 0.31624001 Acc: 0.94271211\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.07575037 Acc: 0.97135983\n",
      "val Loss: 0.32587683 Acc: 0.94162437\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.07693903 Acc: 0.97081652\n",
      "val Loss: 0.31575034 Acc: 0.94234953\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.07772135 Acc: 0.97042844\n",
      "val Loss: 0.31525595 Acc: 0.94379985\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.07722368 Acc: 0.97081652\n",
      "val Loss: 0.32042067 Acc: 0.94162437\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.07325874 Acc: 0.97167029\n",
      "val Loss: 0.31504701 Acc: 0.94379985\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.07803265 Acc: 0.97073890\n",
      "val Loss: 0.31871460 Acc: 0.94307469\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.07629020 Acc: 0.97244645\n",
      "val Loss: 0.32912849 Acc: 0.94271211\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.07430534 Acc: 0.97322260\n",
      "val Loss: 0.31769115 Acc: 0.94452502\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.07214580 Acc: 0.97182552\n",
      "val Loss: 0.32238351 Acc: 0.94198695\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.07848742 Acc: 0.97104936\n",
      "val Loss: 0.32075954 Acc: 0.94307469\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.07385231 Acc: 0.97368830\n",
      "val Loss: 0.31698327 Acc: 0.94234953\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.07251682 Acc: 0.97322260\n",
      "val Loss: 0.32102724 Acc: 0.94307469\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.07979672 Acc: 0.96988513\n",
      "val Loss: 0.32200301 Acc: 0.94343727\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.07557662 Acc: 0.97097175\n",
      "val Loss: 0.31892461 Acc: 0.94416244\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.07630219 Acc: 0.97073890\n",
      "val Loss: 0.32011830 Acc: 0.94379985\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.07394238 Acc: 0.97066129\n",
      "val Loss: 0.33209715 Acc: 0.94234953\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.07440152 Acc: 0.97151506\n",
      "val Loss: 0.33190841 Acc: 0.94343727\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.07777913 Acc: 0.97097175\n",
      "val Loss: 0.33023207 Acc: 0.94271211\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.07303601 Acc: 0.97011798\n",
      "val Loss: 0.32356428 Acc: 0.94234953\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.07634036 Acc: 0.97066129\n",
      "val Loss: 0.32226610 Acc: 0.94379985\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.07735742 Acc: 0.97112698\n",
      "val Loss: 0.30908927 Acc: 0.94234953\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.08050923 Acc: 0.96980751\n",
      "val Loss: 0.31639184 Acc: 0.94271211\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.07421899 Acc: 0.97213598\n",
      "val Loss: 0.31802180 Acc: 0.94162437\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.07476919 Acc: 0.97104936\n",
      "val Loss: 0.31720133 Acc: 0.94343727\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.07502186 Acc: 0.97089413\n",
      "val Loss: 0.31743827 Acc: 0.94126178\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.07787983 Acc: 0.96980751\n",
      "val Loss: 0.32331999 Acc: 0.94198695\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.07341555 Acc: 0.97291214\n",
      "val Loss: 0.32641986 Acc: 0.94234953\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.07499771 Acc: 0.97244645\n",
      "val Loss: 0.31945472 Acc: 0.94234953\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.08073345 Acc: 0.96724620\n",
      "val Loss: 0.32167003 Acc: 0.94343727\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.07708030 Acc: 0.96926420\n",
      "val Loss: 0.32978338 Acc: 0.94162437\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.07750377 Acc: 0.97167029\n",
      "val Loss: 0.32789204 Acc: 0.94126178\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.07771709 Acc: 0.96988513\n",
      "val Loss: 0.31381975 Acc: 0.94307469\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.07533334 Acc: 0.97089413\n",
      "val Loss: 0.32969694 Acc: 0.94089920\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.08129747 Acc: 0.96887613\n",
      "val Loss: 0.33397355 Acc: 0.94126178\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.07504139 Acc: 0.97182552\n",
      "val Loss: 0.32951886 Acc: 0.94271211\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.07494362 Acc: 0.97135983\n",
      "val Loss: 0.32574762 Acc: 0.94343727\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.07077737 Acc: 0.97345545\n",
      "val Loss: 0.32521265 Acc: 0.94234953\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.07112274 Acc: 0.97345545\n",
      "val Loss: 0.32827377 Acc: 0.94162437\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.07468138 Acc: 0.97213598\n",
      "val Loss: 0.33296803 Acc: 0.94089920\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.07562241 Acc: 0.97089413\n",
      "val Loss: 0.32599377 Acc: 0.94053662\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.07227211 Acc: 0.97260168\n",
      "val Loss: 0.32372656 Acc: 0.94234953\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.07151008 Acc: 0.97368830\n",
      "val Loss: 0.32431202 Acc: 0.94343727\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.07628790 Acc: 0.97027321\n",
      "val Loss: 0.33838214 Acc: 0.94271211\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.07698295 Acc: 0.97283452\n",
      "val Loss: 0.32174631 Acc: 0.94126178\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.07712602 Acc: 0.97058367\n",
      "val Loss: 0.32848457 Acc: 0.94162437\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.07612424 Acc: 0.97120459\n",
      "val Loss: 0.31332249 Acc: 0.94343727\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.07715330 Acc: 0.97205837\n",
      "val Loss: 0.32252154 Acc: 0.94126178\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.07401318 Acc: 0.97050605\n",
      "val Loss: 0.33284647 Acc: 0.94307469\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.07306998 Acc: 0.97244645\n",
      "val Loss: 0.33287024 Acc: 0.94089920\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.07552494 Acc: 0.97042844\n",
      "val Loss: 0.31759874 Acc: 0.94271211\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.07862690 Acc: 0.97143744\n",
      "val Loss: 0.33296116 Acc: 0.94198695\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.07530575 Acc: 0.97368830\n",
      "val Loss: 0.33064530 Acc: 0.94126178\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.07071358 Acc: 0.97384353\n",
      "val Loss: 0.33027361 Acc: 0.94198695\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.07278315 Acc: 0.97213598\n",
      "val Loss: 0.33477491 Acc: 0.94307469\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.07348907 Acc: 0.97198075\n",
      "val Loss: 0.34131790 Acc: 0.94053662\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.07387166 Acc: 0.97011798\n",
      "val Loss: 0.33259964 Acc: 0.94126178\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.07637531 Acc: 0.97073890\n",
      "val Loss: 0.32081103 Acc: 0.94198695\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.07082293 Acc: 0.97430922\n",
      "val Loss: 0.32408954 Acc: 0.94234953\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.07598703 Acc: 0.97167029\n",
      "val Loss: 0.33616884 Acc: 0.94126178\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.07501659 Acc: 0.97143744\n",
      "val Loss: 0.33339198 Acc: 0.94198695\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.07637389 Acc: 0.97035082\n",
      "val Loss: 0.33939758 Acc: 0.94162437\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.07091918 Acc: 0.97283452\n",
      "val Loss: 0.32900073 Acc: 0.94126178\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.06969633 Acc: 0.97423161\n",
      "val Loss: 0.32732470 Acc: 0.94162437\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.07362907 Acc: 0.97330022\n",
      "val Loss: 0.33340247 Acc: 0.94234953\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.07700324 Acc: 0.97167029\n",
      "val Loss: 0.34251485 Acc: 0.94126178\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.07246546 Acc: 0.97267929\n",
      "val Loss: 0.32435135 Acc: 0.94017404\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.07765640 Acc: 0.96980751\n",
      "val Loss: 0.33859982 Acc: 0.93981146\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.07829380 Acc: 0.97120459\n",
      "val Loss: 0.31588169 Acc: 0.94162437\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.07495877 Acc: 0.97275691\n",
      "val Loss: 0.32183251 Acc: 0.94162437\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.07196867 Acc: 0.97221360\n",
      "val Loss: 0.32603890 Acc: 0.94017404\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.07347179 Acc: 0.97066129\n",
      "val Loss: 0.32393798 Acc: 0.94307469\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.07775521 Acc: 0.97027321\n",
      "val Loss: 0.33503684 Acc: 0.94053662\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.07891154 Acc: 0.97089413\n",
      "val Loss: 0.32716649 Acc: 0.94126178\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.07486572 Acc: 0.97151506\n",
      "val Loss: 0.32630241 Acc: 0.94234953\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.07245047 Acc: 0.97229121\n",
      "val Loss: 0.32406342 Acc: 0.94198695\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.07562235 Acc: 0.97159267\n",
      "val Loss: 0.33234731 Acc: 0.94271211\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.07613686 Acc: 0.97174790\n",
      "val Loss: 0.32529615 Acc: 0.94053662\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.07684597 Acc: 0.97042844\n",
      "val Loss: 0.33150856 Acc: 0.94162437\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.07993314 Acc: 0.96926420\n",
      "val Loss: 0.32740893 Acc: 0.94162437\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.07168343 Acc: 0.97236883\n",
      "val Loss: 0.32502204 Acc: 0.94126178\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.07629564 Acc: 0.97229121\n",
      "val Loss: 0.32223776 Acc: 0.94089920\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.07545999 Acc: 0.97190314\n",
      "val Loss: 0.32175593 Acc: 0.94198695\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.07148948 Acc: 0.97353306\n",
      "val Loss: 0.33252329 Acc: 0.94271211\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.07339154 Acc: 0.97306737\n",
      "val Loss: 0.33097149 Acc: 0.94379985\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.07632009 Acc: 0.97143744\n",
      "val Loss: 0.32997723 Acc: 0.94126178\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.07520329 Acc: 0.97190314\n",
      "val Loss: 0.31576309 Acc: 0.94089920\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.07364125 Acc: 0.97198075\n",
      "val Loss: 0.32254789 Acc: 0.94198695\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.07694142 Acc: 0.97236883\n",
      "val Loss: 0.33380904 Acc: 0.94234953\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.07433243 Acc: 0.97252406\n",
      "val Loss: 0.34407480 Acc: 0.94053662\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.07730633 Acc: 0.97097175\n",
      "val Loss: 0.32607377 Acc: 0.94198695\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.07467824 Acc: 0.97112698\n",
      "val Loss: 0.32643363 Acc: 0.94307469\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.07338917 Acc: 0.97252406\n",
      "val Loss: 0.31869333 Acc: 0.94162437\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.07061089 Acc: 0.97407637\n",
      "val Loss: 0.32252170 Acc: 0.94234953\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.08058062 Acc: 0.97112698\n",
      "val Loss: 0.32563190 Acc: 0.94089920\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.07817205 Acc: 0.97089413\n",
      "val Loss: 0.33071518 Acc: 0.94198695\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.07181171 Acc: 0.97198075\n",
      "val Loss: 0.33046474 Acc: 0.93908629\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.08038611 Acc: 0.96957467\n",
      "val Loss: 0.32844379 Acc: 0.94234953\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.07101642 Acc: 0.97298975\n",
      "val Loss: 0.33723124 Acc: 0.94198695\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.07554796 Acc: 0.97252406\n",
      "val Loss: 0.34187644 Acc: 0.93981146\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.07502678 Acc: 0.97167029\n",
      "val Loss: 0.32708549 Acc: 0.94126178\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.08115164 Acc: 0.96918659\n",
      "val Loss: 0.31881480 Acc: 0.94126178\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.07716714 Acc: 0.97143744\n",
      "val Loss: 0.32348691 Acc: 0.94162437\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.07338217 Acc: 0.97291214\n",
      "val Loss: 0.31953529 Acc: 0.94307469\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.07283977 Acc: 0.97291214\n",
      "val Loss: 0.32726701 Acc: 0.94089920\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.07423968 Acc: 0.97198075\n",
      "val Loss: 0.31867363 Acc: 0.94126178\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.08107062 Acc: 0.96949705\n",
      "val Loss: 0.33898398 Acc: 0.94271211\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.07142508 Acc: 0.97337783\n",
      "val Loss: 0.31775840 Acc: 0.94126178\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.07545446 Acc: 0.97104936\n",
      "val Loss: 0.31515361 Acc: 0.94271211\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.07552763 Acc: 0.97298975\n",
      "val Loss: 0.32980889 Acc: 0.94053662\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.07812654 Acc: 0.97035082\n",
      "val Loss: 0.33205862 Acc: 0.94162437\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.07468309 Acc: 0.97174790\n",
      "val Loss: 0.33394012 Acc: 0.94126178\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.07381638 Acc: 0.97182552\n",
      "val Loss: 0.32095257 Acc: 0.94234953\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.07614880 Acc: 0.97097175\n",
      "val Loss: 0.33187981 Acc: 0.94089920\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.07055975 Acc: 0.97345545\n",
      "val Loss: 0.32524231 Acc: 0.93981146\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.07143889 Acc: 0.97267929\n",
      "val Loss: 0.32920699 Acc: 0.94234953\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.07318004 Acc: 0.97275691\n",
      "val Loss: 0.33564841 Acc: 0.94126178\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.07097874 Acc: 0.97407637\n",
      "val Loss: 0.31413790 Acc: 0.94126178\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.07464636 Acc: 0.97182552\n",
      "val Loss: 0.32504001 Acc: 0.94162437\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.07771050 Acc: 0.97190314\n",
      "val Loss: 0.32483269 Acc: 0.94162437\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.07317435 Acc: 0.97314499\n",
      "val Loss: 0.33414327 Acc: 0.94162437\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.07375640 Acc: 0.97174790\n",
      "val Loss: 0.31451858 Acc: 0.94234953\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.07382683 Acc: 0.97198075\n",
      "val Loss: 0.32773765 Acc: 0.94089920\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.07111459 Acc: 0.97345545\n",
      "val Loss: 0.32537280 Acc: 0.94198695\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.07424203 Acc: 0.97306737\n",
      "val Loss: 0.31683491 Acc: 0.94307469\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.07258906 Acc: 0.97291214\n",
      "val Loss: 0.33822859 Acc: 0.94089920\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.07294221 Acc: 0.97345545\n",
      "val Loss: 0.31829836 Acc: 0.94126178\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.07197884 Acc: 0.97244645\n",
      "val Loss: 0.33267881 Acc: 0.94198695\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.07532884 Acc: 0.97104936\n",
      "val Loss: 0.31321478 Acc: 0.94089920\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.07100644 Acc: 0.97399876\n",
      "val Loss: 0.32343281 Acc: 0.94234953\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.07064443 Acc: 0.97361068\n",
      "val Loss: 0.32150086 Acc: 0.94089920\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.07650574 Acc: 0.97120459\n",
      "val Loss: 0.31640242 Acc: 0.94271211\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.07296049 Acc: 0.97376591\n",
      "val Loss: 0.32884118 Acc: 0.94126178\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.07073722 Acc: 0.97399876\n",
      "val Loss: 0.32391602 Acc: 0.94198695\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.06988666 Acc: 0.97376591\n",
      "val Loss: 0.33073885 Acc: 0.94198695\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.07098333 Acc: 0.97337783\n",
      "val Loss: 0.32265686 Acc: 0.94162437\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.07115641 Acc: 0.97298975\n",
      "val Loss: 0.31791746 Acc: 0.94307469\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.07585975 Acc: 0.97135983\n",
      "val Loss: 0.32852332 Acc: 0.94126178\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.07897708 Acc: 0.96825520\n",
      "val Loss: 0.31839328 Acc: 0.94234953\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.07763391 Acc: 0.97097175\n",
      "val Loss: 0.32429784 Acc: 0.94089920\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.07330130 Acc: 0.97252406\n",
      "val Loss: 0.32981940 Acc: 0.94198695\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.07229467 Acc: 0.97050605\n",
      "val Loss: 0.32428702 Acc: 0.94162437\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.07243933 Acc: 0.97485253\n",
      "val Loss: 0.34321972 Acc: 0.94053662\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.07625366 Acc: 0.97035082\n",
      "val Loss: 0.32760318 Acc: 0.94234953\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.07566975 Acc: 0.97260168\n",
      "val Loss: 0.32020442 Acc: 0.94126178\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.07228802 Acc: 0.97322260\n",
      "val Loss: 0.32106222 Acc: 0.94234953\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.07498739 Acc: 0.97198075\n",
      "val Loss: 0.31340261 Acc: 0.94234953\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.07059777 Acc: 0.97384353\n",
      "val Loss: 0.32877862 Acc: 0.94198695\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.07608298 Acc: 0.97104936\n",
      "val Loss: 0.31924304 Acc: 0.94053662\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.07407785 Acc: 0.97213598\n",
      "val Loss: 0.32823208 Acc: 0.94416244\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.07669016 Acc: 0.97205837\n",
      "val Loss: 0.34189896 Acc: 0.94234953\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.07398625 Acc: 0.97205837\n",
      "val Loss: 0.31770932 Acc: 0.94089920\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.07766646 Acc: 0.97011798\n",
      "val Loss: 0.33509164 Acc: 0.94162437\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.07418858 Acc: 0.97252406\n",
      "val Loss: 0.32782536 Acc: 0.94089920\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.07809588 Acc: 0.97291214\n",
      "val Loss: 0.32911403 Acc: 0.94162437\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.07982891 Acc: 0.96988513\n",
      "val Loss: 0.32061518 Acc: 0.94234953\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.07005785 Acc: 0.97423161\n",
      "val Loss: 0.32927247 Acc: 0.94089920\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.07450402 Acc: 0.97135983\n",
      "val Loss: 0.34432375 Acc: 0.94017404\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.06880094 Acc: 0.97330022\n",
      "val Loss: 0.32234514 Acc: 0.94126178\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.07497637 Acc: 0.97151506\n",
      "val Loss: 0.33302931 Acc: 0.94089920\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.07809327 Acc: 0.97089413\n",
      "val Loss: 0.32140843 Acc: 0.94053662\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.07105386 Acc: 0.97298975\n",
      "val Loss: 0.31803094 Acc: 0.94271211\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.07893100 Acc: 0.97058367\n",
      "val Loss: 0.33873105 Acc: 0.94017404\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.07311849 Acc: 0.97213598\n",
      "val Loss: 0.32469577 Acc: 0.94162437\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.07282814 Acc: 0.97244645\n",
      "val Loss: 0.32899313 Acc: 0.94089920\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.07637502 Acc: 0.97159267\n",
      "val Loss: 0.31749698 Acc: 0.94162437\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.07077038 Acc: 0.97392114\n",
      "val Loss: 0.34328964 Acc: 0.94017404\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.07787388 Acc: 0.97058367\n",
      "val Loss: 0.33738280 Acc: 0.94126178\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.07525377 Acc: 0.97058367\n",
      "val Loss: 0.33485680 Acc: 0.94162437\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.07541042 Acc: 0.97198075\n",
      "val Loss: 0.33013058 Acc: 0.94234953\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.07413199 Acc: 0.97213598\n",
      "val Loss: 0.33059284 Acc: 0.93981146\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.07377563 Acc: 0.97275691\n",
      "val Loss: 0.32766352 Acc: 0.94053662\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.07303727 Acc: 0.97229121\n",
      "val Loss: 0.33200243 Acc: 0.94126178\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.07431957 Acc: 0.97104936\n",
      "val Loss: 0.32827455 Acc: 0.94126178\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.07508214 Acc: 0.97190314\n",
      "val Loss: 0.32936882 Acc: 0.94089920\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.07088111 Acc: 0.97322260\n",
      "val Loss: 0.31723760 Acc: 0.94198695\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.07741028 Acc: 0.97066129\n",
      "val Loss: 0.33188496 Acc: 0.93944888\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.07453441 Acc: 0.97198075\n",
      "val Loss: 0.33403860 Acc: 0.94126178\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.07537321 Acc: 0.97330022\n",
      "val Loss: 0.34078433 Acc: 0.94017404\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.07038419 Acc: 0.97430922\n",
      "val Loss: 0.32940313 Acc: 0.94162437\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.07796185 Acc: 0.97135983\n",
      "val Loss: 0.34057811 Acc: 0.94053662\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.07878782 Acc: 0.97027321\n",
      "val Loss: 0.33094972 Acc: 0.94053662\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.07491744 Acc: 0.97267929\n",
      "val Loss: 0.32233551 Acc: 0.94198695\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.07700405 Acc: 0.97213598\n",
      "val Loss: 0.32217040 Acc: 0.94162437\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.07824092 Acc: 0.97011798\n",
      "val Loss: 0.31974043 Acc: 0.94162437\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.07461034 Acc: 0.97174790\n",
      "val Loss: 0.33551438 Acc: 0.94162437\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.07418400 Acc: 0.97306737\n",
      "val Loss: 0.32741199 Acc: 0.94126178\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.07548751 Acc: 0.97221360\n",
      "val Loss: 0.32778216 Acc: 0.94162437\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.07815437 Acc: 0.97159267\n",
      "val Loss: 0.32072768 Acc: 0.94126178\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.07483553 Acc: 0.97128221\n",
      "val Loss: 0.31452861 Acc: 0.94089920\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.06918542 Acc: 0.97392114\n",
      "val Loss: 0.33267966 Acc: 0.94198695\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.07853337 Acc: 0.96949705\n",
      "val Loss: 0.32374185 Acc: 0.94162437\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.07782294 Acc: 0.97081652\n",
      "val Loss: 0.32338615 Acc: 0.94126178\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.06833536 Acc: 0.97330022\n",
      "val Loss: 0.33658885 Acc: 0.94017404\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.07900520 Acc: 0.97097175\n",
      "val Loss: 0.32561701 Acc: 0.94126178\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.07068732 Acc: 0.97291214\n",
      "val Loss: 0.32358100 Acc: 0.94162437\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.07341837 Acc: 0.97283452\n",
      "val Loss: 0.33397888 Acc: 0.94053662\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.07421642 Acc: 0.97291214\n",
      "val Loss: 0.32242394 Acc: 0.94126178\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.07694564 Acc: 0.97298975\n",
      "val Loss: 0.32156453 Acc: 0.94089920\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.07322482 Acc: 0.97221360\n",
      "val Loss: 0.31879987 Acc: 0.94234953\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.07464376 Acc: 0.97120459\n",
      "val Loss: 0.33408927 Acc: 0.94162437\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.07333229 Acc: 0.97221360\n",
      "val Loss: 0.32004357 Acc: 0.94089920\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.07704064 Acc: 0.97120459\n",
      "val Loss: 0.32679357 Acc: 0.94271211\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.07690281 Acc: 0.97151506\n",
      "val Loss: 0.33195793 Acc: 0.94198695\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.07951692 Acc: 0.97190314\n",
      "val Loss: 0.33810635 Acc: 0.94053662\n",
      "\n",
      "Training complete in 1041m 9s\n",
      "Best val Acc: 0.94981949 Best val loss: 0.20218865\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CXR/EfficientNet_B3_AP_Dataset20200630.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 300,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
