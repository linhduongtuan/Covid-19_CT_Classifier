{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'non-COVID']\n",
      "{'train': 1985, 'test': 497}\n",
      "cuda:0\n",
      "{0: 'COVID', 1: 'non-COVID'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19_CT'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 70\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc): Linear(in_features=2656, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('tresnet_xl', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.head #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81066958\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = classifier\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01, momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "# model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.013277859624261618, Val accuracy: 0.9939637826961771\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.07117805 Acc: 0.97027708\n",
      "test Loss: 0.01670736 Acc: 0.99396378\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.07083692 Acc: 0.96926952\n",
      "test Loss: 0.01614681 Acc: 0.99396378\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.07441651 Acc: 0.96876574\n",
      "test Loss: 0.01672438 Acc: 0.99396378\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.07837084 Acc: 0.96423174\n",
      "test Loss: 0.01653521 Acc: 0.99396378\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.06387518 Acc: 0.97430730\n",
      "test Loss: 0.01755602 Acc: 0.99396378\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.06917402 Acc: 0.97279597\n",
      "test Loss: 0.01421881 Acc: 0.99396378\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.06675795 Acc: 0.97430730\n",
      "test Loss: 0.01474000 Acc: 0.99396378\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.06498371 Acc: 0.97430730\n",
      "test Loss: 0.01647133 Acc: 0.99396378\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.07424118 Acc: 0.97078086\n",
      "test Loss: 0.01488360 Acc: 0.99396378\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.06621444 Acc: 0.97279597\n",
      "test Loss: 0.01457719 Acc: 0.99396378\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.06711519 Acc: 0.97329975\n",
      "test Loss: 0.01366668 Acc: 0.99396378\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.06303599 Acc: 0.97279597\n",
      "test Loss: 0.01438481 Acc: 0.99597586\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.07348484 Acc: 0.96876574\n",
      "test Loss: 0.01501956 Acc: 0.99396378\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.08717447 Acc: 0.96523929\n",
      "test Loss: 0.01372162 Acc: 0.99396378\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.07157252 Acc: 0.96926952\n",
      "test Loss: 0.01466499 Acc: 0.99396378\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.07331780 Acc: 0.96926952\n",
      "test Loss: 0.01672366 Acc: 0.99396378\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.07311417 Acc: 0.96876574\n",
      "test Loss: 0.01482342 Acc: 0.99396378\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.06776572 Acc: 0.96775819\n",
      "test Loss: 0.01644470 Acc: 0.99396378\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.06636304 Acc: 0.97329975\n",
      "test Loss: 0.01712730 Acc: 0.99396378\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.07555914 Acc: 0.96574307\n",
      "test Loss: 0.01542451 Acc: 0.99396378\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.06971640 Acc: 0.96826196\n",
      "test Loss: 0.01394065 Acc: 0.99396378\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.08006388 Acc: 0.96826196\n",
      "test Loss: 0.01868310 Acc: 0.99396378\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.07588122 Acc: 0.96876574\n",
      "test Loss: 0.01518475 Acc: 0.99597586\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.07007020 Acc: 0.97279597\n",
      "test Loss: 0.01451958 Acc: 0.99396378\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.06830424 Acc: 0.97279597\n",
      "test Loss: 0.01469554 Acc: 0.99396378\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.08084773 Acc: 0.96523929\n",
      "test Loss: 0.01753672 Acc: 0.99597586\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.06113693 Acc: 0.97430730\n",
      "test Loss: 0.01714486 Acc: 0.99597586\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.06555607 Acc: 0.96926952\n",
      "test Loss: 0.01492207 Acc: 0.99396378\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.06575705 Acc: 0.97178841\n",
      "test Loss: 0.01850497 Acc: 0.99396378\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.06801541 Acc: 0.97178841\n",
      "test Loss: 0.01345885 Acc: 0.99396378\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.07925221 Acc: 0.96926952\n",
      "test Loss: 0.01527894 Acc: 0.99396378\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.07432653 Acc: 0.97178841\n",
      "test Loss: 0.01722513 Acc: 0.99396378\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.07336354 Acc: 0.96675063\n",
      "test Loss: 0.01550261 Acc: 0.99396378\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.08913774 Acc: 0.95919395\n",
      "test Loss: 0.01495547 Acc: 0.99396378\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.07363508 Acc: 0.96675063\n",
      "test Loss: 0.01532270 Acc: 0.99396378\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.05990407 Acc: 0.97783375\n",
      "test Loss: 0.01728363 Acc: 0.99195171\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.07368035 Acc: 0.97078086\n",
      "test Loss: 0.01550174 Acc: 0.99396378\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.05845623 Acc: 0.97531486\n",
      "test Loss: 0.01536547 Acc: 0.99396378\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.08058368 Acc: 0.96725441\n",
      "test Loss: 0.01492970 Acc: 0.99396378\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.06987284 Acc: 0.97128463\n",
      "test Loss: 0.01564558 Acc: 0.99396378\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.06804460 Acc: 0.97178841\n",
      "test Loss: 0.01540042 Acc: 0.99396378\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.06545771 Acc: 0.97229219\n",
      "test Loss: 0.01440184 Acc: 0.99396378\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.07683893 Acc: 0.96775819\n",
      "test Loss: 0.01408295 Acc: 0.99396378\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.06055000 Acc: 0.97430730\n",
      "test Loss: 0.01609834 Acc: 0.99396378\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.06948420 Acc: 0.97078086\n",
      "test Loss: 0.01444673 Acc: 0.99396378\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.07854079 Acc: 0.97178841\n",
      "test Loss: 0.01526086 Acc: 0.99396378\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.06530689 Acc: 0.97078086\n",
      "test Loss: 0.01468871 Acc: 0.99396378\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.06684573 Acc: 0.97229219\n",
      "test Loss: 0.01498644 Acc: 0.99396378\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.06840817 Acc: 0.97380353\n",
      "test Loss: 0.01506815 Acc: 0.99396378\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.07514886 Acc: 0.96775819\n",
      "test Loss: 0.01530731 Acc: 0.99396378\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.05915680 Acc: 0.97783375\n",
      "test Loss: 0.01601209 Acc: 0.99396378\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.07695636 Acc: 0.96725441\n",
      "test Loss: 0.01461427 Acc: 0.99396378\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.07763308 Acc: 0.97078086\n",
      "test Loss: 0.01565803 Acc: 0.99396378\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.05463361 Acc: 0.98035264\n",
      "test Loss: 0.01567302 Acc: 0.99396378\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.07624487 Acc: 0.96926952\n",
      "test Loss: 0.01544728 Acc: 0.99396378\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.06988803 Acc: 0.97128463\n",
      "test Loss: 0.01329723 Acc: 0.99396378\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.06248846 Acc: 0.97531486\n",
      "test Loss: 0.01432583 Acc: 0.99396378\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.06948941 Acc: 0.96977330\n",
      "test Loss: 0.01561637 Acc: 0.99396378\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.06336778 Acc: 0.97481108\n",
      "test Loss: 0.01632028 Acc: 0.99396378\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.06620609 Acc: 0.97682620\n",
      "test Loss: 0.01619580 Acc: 0.99597586\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.07776693 Acc: 0.97027708\n",
      "test Loss: 0.01467134 Acc: 0.99396378\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.09414572 Acc: 0.95818640\n",
      "test Loss: 0.01617880 Acc: 0.99396378\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.06037242 Acc: 0.97632242\n",
      "test Loss: 0.01455207 Acc: 0.99396378\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.08048496 Acc: 0.96624685\n",
      "test Loss: 0.01489360 Acc: 0.99396378\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.07191387 Acc: 0.97128463\n",
      "test Loss: 0.01491077 Acc: 0.99396378\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.07541874 Acc: 0.96725441\n",
      "test Loss: 0.01533022 Acc: 0.99396378\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.07925080 Acc: 0.96372796\n",
      "test Loss: 0.01546220 Acc: 0.99396378\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.06586620 Acc: 0.97481108\n",
      "test Loss: 0.01566951 Acc: 0.99396378\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.06451717 Acc: 0.97178841\n",
      "test Loss: 0.01557470 Acc: 0.99396378\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.06507372 Acc: 0.97531486\n",
      "test Loss: 0.01471773 Acc: 0.99396378\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.07319756 Acc: 0.96977330\n",
      "test Loss: 0.01311208 Acc: 0.99396378\n",
      "New best model found!\n",
      "New record loss: 0.013112075864855926, previous record loss: 0.013277859624261618\n",
      "New record loss is SAVED: 0.013112075864855926\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.07069273 Acc: 0.96977330\n",
      "test Loss: 0.01486531 Acc: 0.99195171\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.07079758 Acc: 0.97078086\n",
      "test Loss: 0.01380767 Acc: 0.99396378\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.06730820 Acc: 0.97229219\n",
      "test Loss: 0.01678322 Acc: 0.99396378\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.06640944 Acc: 0.97531486\n",
      "test Loss: 0.01607899 Acc: 0.99396378\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.07673174 Acc: 0.97027708\n",
      "test Loss: 0.01571735 Acc: 0.99396378\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.06688552 Acc: 0.97128463\n",
      "test Loss: 0.01416127 Acc: 0.99396378\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.06662042 Acc: 0.97128463\n",
      "test Loss: 0.01533484 Acc: 0.99396378\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.06444974 Acc: 0.97229219\n",
      "test Loss: 0.01555375 Acc: 0.99396378\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.06527965 Acc: 0.97430730\n",
      "test Loss: 0.01471411 Acc: 0.99396378\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.05983475 Acc: 0.97481108\n",
      "test Loss: 0.01511153 Acc: 0.99396378\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.07903558 Acc: 0.96624685\n",
      "test Loss: 0.01532947 Acc: 0.99396378\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.06180956 Acc: 0.97430730\n",
      "test Loss: 0.01518154 Acc: 0.99396378\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.06965818 Acc: 0.97128463\n",
      "test Loss: 0.01584868 Acc: 0.99396378\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.08066291 Acc: 0.96322418\n",
      "test Loss: 0.01351893 Acc: 0.99396378\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.08300121 Acc: 0.96826196\n",
      "test Loss: 0.01443241 Acc: 0.99396378\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.07531260 Acc: 0.96523929\n",
      "test Loss: 0.01805819 Acc: 0.99195171\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.06460816 Acc: 0.97279597\n",
      "test Loss: 0.01456345 Acc: 0.99597586\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.07021836 Acc: 0.97128463\n",
      "test Loss: 0.01536893 Acc: 0.99396378\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.05959302 Acc: 0.97632242\n",
      "test Loss: 0.01419559 Acc: 0.99396378\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.06814383 Acc: 0.97329975\n",
      "test Loss: 0.01549101 Acc: 0.99396378\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.07317266 Acc: 0.97078086\n",
      "test Loss: 0.01581354 Acc: 0.99396378\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.07022497 Acc: 0.96775819\n",
      "test Loss: 0.01417631 Acc: 0.99396378\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.07278715 Acc: 0.97128463\n",
      "test Loss: 0.01672317 Acc: 0.99396378\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.08289083 Acc: 0.96322418\n",
      "test Loss: 0.01617824 Acc: 0.99396378\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.06027358 Acc: 0.97481108\n",
      "test Loss: 0.01612317 Acc: 0.99396378\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.05754628 Acc: 0.97984887\n",
      "test Loss: 0.01614530 Acc: 0.99396378\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.07092769 Acc: 0.97329975\n",
      "test Loss: 0.01523801 Acc: 0.99396378\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.07022217 Acc: 0.97279597\n",
      "test Loss: 0.01596597 Acc: 0.99396378\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.07396060 Acc: 0.97329975\n",
      "test Loss: 0.01647458 Acc: 0.99396378\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.06451363 Acc: 0.97531486\n",
      "test Loss: 0.01481305 Acc: 0.99396378\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.07034483 Acc: 0.97078086\n",
      "test Loss: 0.01595118 Acc: 0.99396378\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.07977472 Acc: 0.96574307\n",
      "test Loss: 0.01627899 Acc: 0.99396378\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.06863258 Acc: 0.97027708\n",
      "test Loss: 0.01551647 Acc: 0.99396378\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.06811391 Acc: 0.97128463\n",
      "test Loss: 0.01455449 Acc: 0.99396378\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.06837597 Acc: 0.97178841\n",
      "test Loss: 0.01504915 Acc: 0.99396378\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.06171443 Acc: 0.97581864\n",
      "test Loss: 0.01533105 Acc: 0.99396378\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.06874091 Acc: 0.96926952\n",
      "test Loss: 0.01489609 Acc: 0.99396378\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.06594989 Acc: 0.97178841\n",
      "test Loss: 0.01450712 Acc: 0.99396378\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.07044513 Acc: 0.97178841\n",
      "test Loss: 0.01489056 Acc: 0.99396378\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.06994925 Acc: 0.97430730\n",
      "test Loss: 0.01684821 Acc: 0.99396378\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.08012753 Acc: 0.96624685\n",
      "test Loss: 0.01667769 Acc: 0.99597586\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.06469281 Acc: 0.97128463\n",
      "test Loss: 0.01405916 Acc: 0.99396378\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.06876762 Acc: 0.97027708\n",
      "test Loss: 0.01597476 Acc: 0.99597586\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.06896744 Acc: 0.97380353\n",
      "test Loss: 0.01530226 Acc: 0.99396378\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.07227895 Acc: 0.97027708\n",
      "test Loss: 0.01526161 Acc: 0.99597586\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.07532635 Acc: 0.97128463\n",
      "test Loss: 0.01542137 Acc: 0.99396378\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.06753383 Acc: 0.97229219\n",
      "test Loss: 0.01322059 Acc: 0.99396378\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.06345907 Acc: 0.97531486\n",
      "test Loss: 0.01326916 Acc: 0.99396378\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.07553448 Acc: 0.96624685\n",
      "test Loss: 0.01674036 Acc: 0.99396378\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.06937660 Acc: 0.97229219\n",
      "test Loss: 0.01611909 Acc: 0.99396378\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.06561120 Acc: 0.97380353\n",
      "test Loss: 0.01475967 Acc: 0.99195171\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.06101076 Acc: 0.97380353\n",
      "test Loss: 0.01486587 Acc: 0.99396378\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.07628306 Acc: 0.96775819\n",
      "test Loss: 0.01594233 Acc: 0.99396378\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.07122700 Acc: 0.97128463\n",
      "test Loss: 0.01522416 Acc: 0.99396378\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.06959735 Acc: 0.97380353\n",
      "test Loss: 0.01595091 Acc: 0.99597586\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.07486351 Acc: 0.96876574\n",
      "test Loss: 0.01526890 Acc: 0.99396378\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.07186086 Acc: 0.96826196\n",
      "test Loss: 0.01558746 Acc: 0.99396378\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.07132952 Acc: 0.96675063\n",
      "test Loss: 0.01615388 Acc: 0.99396378\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.07381022 Acc: 0.96624685\n",
      "test Loss: 0.01615727 Acc: 0.99396378\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.06737740 Acc: 0.97632242\n",
      "test Loss: 0.01729447 Acc: 0.99396378\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.06772699 Acc: 0.97178841\n",
      "test Loss: 0.01543620 Acc: 0.99396378\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.06723659 Acc: 0.97380353\n",
      "test Loss: 0.01512844 Acc: 0.99396378\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.06792200 Acc: 0.97430730\n",
      "test Loss: 0.01649911 Acc: 0.99396378\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.06993897 Acc: 0.97329975\n",
      "test Loss: 0.01465171 Acc: 0.99396378\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.06867887 Acc: 0.97178841\n",
      "test Loss: 0.01378668 Acc: 0.99396378\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.06606419 Acc: 0.97329975\n",
      "test Loss: 0.01533828 Acc: 0.99396378\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.06886938 Acc: 0.97229219\n",
      "test Loss: 0.01493862 Acc: 0.99396378\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.06496282 Acc: 0.97279597\n",
      "test Loss: 0.01441591 Acc: 0.99396378\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.06535433 Acc: 0.97329975\n",
      "test Loss: 0.01543080 Acc: 0.99396378\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.05734240 Acc: 0.97531486\n",
      "test Loss: 0.01519253 Acc: 0.99597586\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.07526455 Acc: 0.97128463\n",
      "test Loss: 0.01477309 Acc: 0.99396378\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.07005396 Acc: 0.96675063\n",
      "test Loss: 0.01472341 Acc: 0.99396378\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.07477846 Acc: 0.97078086\n",
      "test Loss: 0.01679496 Acc: 0.99396378\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.06442039 Acc: 0.97329975\n",
      "test Loss: 0.01345085 Acc: 0.99396378\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.07312667 Acc: 0.97178841\n",
      "test Loss: 0.01395602 Acc: 0.99396378\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.06898016 Acc: 0.97178841\n",
      "test Loss: 0.01483734 Acc: 0.99396378\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.06902985 Acc: 0.97229219\n",
      "test Loss: 0.01456232 Acc: 0.99396378\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.07060263 Acc: 0.96876574\n",
      "test Loss: 0.01507666 Acc: 0.99396378\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.06984315 Acc: 0.97178841\n",
      "test Loss: 0.01565208 Acc: 0.99396378\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.08006149 Acc: 0.96423174\n",
      "test Loss: 0.01573339 Acc: 0.99396378\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.06908017 Acc: 0.97430730\n",
      "test Loss: 0.01563750 Acc: 0.99396378\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.07024357 Acc: 0.97682620\n",
      "test Loss: 0.01433533 Acc: 0.99396378\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.08523962 Acc: 0.96120907\n",
      "test Loss: 0.01386376 Acc: 0.99396378\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.06122516 Acc: 0.97581864\n",
      "test Loss: 0.01606372 Acc: 0.99396378\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.07323374 Acc: 0.96826196\n",
      "test Loss: 0.01536629 Acc: 0.99396378\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.05859190 Acc: 0.97531486\n",
      "test Loss: 0.01588034 Acc: 0.99396378\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.08616089 Acc: 0.96826196\n",
      "test Loss: 0.01676537 Acc: 0.99396378\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.07247399 Acc: 0.96977330\n",
      "test Loss: 0.01556366 Acc: 0.99396378\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.06950801 Acc: 0.97279597\n",
      "test Loss: 0.01545436 Acc: 0.99396378\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.06324534 Acc: 0.97531486\n",
      "test Loss: 0.01589425 Acc: 0.99396378\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.06825876 Acc: 0.97128463\n",
      "test Loss: 0.01585146 Acc: 0.99396378\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.07439933 Acc: 0.96876574\n",
      "test Loss: 0.01548757 Acc: 0.99396378\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.06727251 Acc: 0.97329975\n",
      "test Loss: 0.01504725 Acc: 0.99396378\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.07979045 Acc: 0.96675063\n",
      "test Loss: 0.01712739 Acc: 0.99396378\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.06425500 Acc: 0.97078086\n",
      "test Loss: 0.01636064 Acc: 0.99396378\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.08482700 Acc: 0.96574307\n",
      "test Loss: 0.01583802 Acc: 0.99396378\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.08153111 Acc: 0.96322418\n",
      "test Loss: 0.01395725 Acc: 0.99396378\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.07731555 Acc: 0.96977330\n",
      "test Loss: 0.01639860 Acc: 0.99396378\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.06634187 Acc: 0.97078086\n",
      "test Loss: 0.01612466 Acc: 0.99195171\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.06315340 Acc: 0.97531486\n",
      "test Loss: 0.01776777 Acc: 0.99396378\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.06510640 Acc: 0.97481108\n",
      "test Loss: 0.01520564 Acc: 0.99396378\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.06918835 Acc: 0.97229219\n",
      "test Loss: 0.01578623 Acc: 0.99396378\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.07738017 Acc: 0.96926952\n",
      "test Loss: 0.01483067 Acc: 0.99396378\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.07588196 Acc: 0.96826196\n",
      "test Loss: 0.01408991 Acc: 0.99396378\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.07254954 Acc: 0.96977330\n",
      "test Loss: 0.01642442 Acc: 0.99396378\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.06114157 Acc: 0.97581864\n",
      "test Loss: 0.01443455 Acc: 0.99396378\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.06733907 Acc: 0.97027708\n",
      "test Loss: 0.01569128 Acc: 0.99396378\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.06948348 Acc: 0.97229219\n",
      "test Loss: 0.01607318 Acc: 0.99396378\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.07765342 Acc: 0.96926952\n",
      "test Loss: 0.01454794 Acc: 0.99396378\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.06953052 Acc: 0.97128463\n",
      "test Loss: 0.01623337 Acc: 0.99396378\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.05849008 Acc: 0.97632242\n",
      "test Loss: 0.01641779 Acc: 0.99396378\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.06384293 Acc: 0.97279597\n",
      "test Loss: 0.01781994 Acc: 0.99396378\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.06600285 Acc: 0.97329975\n",
      "test Loss: 0.01512111 Acc: 0.99396378\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.07338788 Acc: 0.96574307\n",
      "test Loss: 0.01511394 Acc: 0.99396378\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.06456190 Acc: 0.97632242\n",
      "test Loss: 0.01402645 Acc: 0.99396378\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.07637638 Acc: 0.96523929\n",
      "test Loss: 0.01544887 Acc: 0.99396378\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.08538909 Acc: 0.96372796\n",
      "test Loss: 0.01588242 Acc: 0.99396378\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.06859073 Acc: 0.97078086\n",
      "test Loss: 0.01643893 Acc: 0.99396378\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.07298746 Acc: 0.96926952\n",
      "test Loss: 0.01686550 Acc: 0.99396378\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.06701000 Acc: 0.97229219\n",
      "test Loss: 0.01537757 Acc: 0.99396378\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.08255431 Acc: 0.96876574\n",
      "test Loss: 0.01622997 Acc: 0.99396378\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.07230520 Acc: 0.97279597\n",
      "test Loss: 0.01704299 Acc: 0.99396378\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.05972956 Acc: 0.97329975\n",
      "test Loss: 0.01433970 Acc: 0.99396378\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.07539600 Acc: 0.96775819\n",
      "test Loss: 0.01779468 Acc: 0.99396378\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.07502676 Acc: 0.96725441\n",
      "test Loss: 0.01757118 Acc: 0.99396378\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.07054774 Acc: 0.97128463\n",
      "test Loss: 0.01388462 Acc: 0.99396378\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.06756777 Acc: 0.97229219\n",
      "test Loss: 0.01640977 Acc: 0.99396378\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.05625304 Acc: 0.97682620\n",
      "test Loss: 0.01511053 Acc: 0.99396378\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.08036129 Acc: 0.96826196\n",
      "test Loss: 0.01543978 Acc: 0.99396378\n",
      "\n",
      "Training complete in 115m 4s\n",
      "Best val Acc: 0.99396378 Best val loss: 0.01311208\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19_CT/weights/TResNet_Extra_Large_ImgSize224_Covid-19.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 200,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
