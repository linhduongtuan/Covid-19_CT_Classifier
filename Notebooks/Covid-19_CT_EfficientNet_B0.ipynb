{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'NonCOVID-19']\n",
      "{'train': 639, 'val': 75}\n",
      "cuda:0\n",
      "{0: 'COVID-19', 1: 'NonCOVID-19'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19/CT/Yang'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 100\n",
    "num_epochs = 300\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 7919262\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([#('fc1', nn.Linear(1280, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "#lr = lambda x: (((1 + math.cos(x * math.pi / num_epochs)) / 2) ** 1) * 0.9\n",
    "#scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.23267162839571634, Val accuracy: 0.9466666666666668\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.34828281 Acc: 0.87323944\n",
      "val Loss: 0.25803000 Acc: 0.94666667\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.31961510 Acc: 0.87480438\n",
      "val Loss: 0.26198557 Acc: 0.94666667\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.32639174 Acc: 0.86071987\n",
      "val Loss: 0.26270181 Acc: 0.94666667\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.29536231 Acc: 0.88262911\n",
      "val Loss: 0.25691247 Acc: 0.94666667\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.28374023 Acc: 0.87949922\n",
      "val Loss: 0.25925541 Acc: 0.93333333\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.30398221 Acc: 0.87167449\n",
      "val Loss: 0.25761497 Acc: 0.94666667\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.27157078 Acc: 0.88888889\n",
      "val Loss: 0.25885218 Acc: 0.94666667\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.25774874 Acc: 0.89358372\n",
      "val Loss: 0.25885081 Acc: 0.93333333\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.28825041 Acc: 0.89045383\n",
      "val Loss: 0.26268846 Acc: 0.92000000\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.29534186 Acc: 0.89358372\n",
      "val Loss: 0.26543540 Acc: 0.90666667\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.30088264 Acc: 0.86697966\n",
      "val Loss: 0.26290450 Acc: 0.90666667\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.23085997 Acc: 0.90140845\n",
      "val Loss: 0.26614276 Acc: 0.90666667\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.28805411 Acc: 0.87636933\n",
      "val Loss: 0.26544842 Acc: 0.90666667\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.32957606 Acc: 0.86697966\n",
      "val Loss: 0.27349466 Acc: 0.90666667\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.28968827 Acc: 0.89514867\n",
      "val Loss: 0.27365011 Acc: 0.92000000\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.31007475 Acc: 0.87167449\n",
      "val Loss: 0.27241918 Acc: 0.92000000\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.27598338 Acc: 0.87480438\n",
      "val Loss: 0.27585885 Acc: 0.90666667\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.31739199 Acc: 0.87480438\n",
      "val Loss: 0.27214721 Acc: 0.90666667\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.30268869 Acc: 0.88106416\n",
      "val Loss: 0.27376950 Acc: 0.90666667\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.26610611 Acc: 0.89514867\n",
      "val Loss: 0.26661509 Acc: 0.92000000\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.28298753 Acc: 0.88262911\n",
      "val Loss: 0.26176393 Acc: 0.93333333\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.30259610 Acc: 0.87010955\n",
      "val Loss: 0.26512730 Acc: 0.90666667\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.27015879 Acc: 0.89827856\n",
      "val Loss: 0.26582947 Acc: 0.90666667\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.26769507 Acc: 0.88732394\n",
      "val Loss: 0.27023658 Acc: 0.90666667\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.29902321 Acc: 0.88419405\n",
      "val Loss: 0.27498671 Acc: 0.92000000\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.33859867 Acc: 0.86071987\n",
      "val Loss: 0.27188534 Acc: 0.90666667\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.30316848 Acc: 0.88888889\n",
      "val Loss: 0.26864266 Acc: 0.90666667\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.28899920 Acc: 0.87480438\n",
      "val Loss: 0.27607188 Acc: 0.90666667\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.31013917 Acc: 0.85915493\n",
      "val Loss: 0.27123201 Acc: 0.90666667\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.27700057 Acc: 0.87793427\n",
      "val Loss: 0.27305162 Acc: 0.90666667\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.32919782 Acc: 0.86384977\n",
      "val Loss: 0.26975417 Acc: 0.90666667\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.26216804 Acc: 0.88262911\n",
      "val Loss: 0.27464223 Acc: 0.90666667\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.29158355 Acc: 0.89358372\n",
      "val Loss: 0.26445293 Acc: 0.90666667\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.29256751 Acc: 0.88575900\n",
      "val Loss: 0.26350924 Acc: 0.90666667\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.27801921 Acc: 0.89201878\n",
      "val Loss: 0.26856214 Acc: 0.90666667\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.28442584 Acc: 0.88888889\n",
      "val Loss: 0.26891446 Acc: 0.90666667\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.33386836 Acc: 0.85758998\n",
      "val Loss: 0.27244300 Acc: 0.90666667\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.27340235 Acc: 0.89358372\n",
      "val Loss: 0.26804274 Acc: 0.90666667\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.30969674 Acc: 0.87010955\n",
      "val Loss: 0.26742464 Acc: 0.90666667\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.32479599 Acc: 0.86071987\n",
      "val Loss: 0.27011141 Acc: 0.90666667\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.27304791 Acc: 0.89358372\n",
      "val Loss: 0.26876962 Acc: 0.90666667\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.28416115 Acc: 0.88575900\n",
      "val Loss: 0.27327341 Acc: 0.90666667\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.28926855 Acc: 0.86854460\n",
      "val Loss: 0.27527660 Acc: 0.90666667\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.27295546 Acc: 0.89358372\n",
      "val Loss: 0.27554557 Acc: 0.90666667\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.24028118 Acc: 0.90610329\n",
      "val Loss: 0.27716723 Acc: 0.90666667\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.25313142 Acc: 0.90297340\n",
      "val Loss: 0.26580924 Acc: 0.90666667\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.29242719 Acc: 0.89827856\n",
      "val Loss: 0.26562145 Acc: 0.90666667\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.26969034 Acc: 0.87636933\n",
      "val Loss: 0.27202711 Acc: 0.90666667\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.28764984 Acc: 0.87793427\n",
      "val Loss: 0.26039937 Acc: 0.90666667\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.27684011 Acc: 0.88106416\n",
      "val Loss: 0.26093984 Acc: 0.92000000\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.28663504 Acc: 0.87949922\n",
      "val Loss: 0.26648641 Acc: 0.90666667\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.26460482 Acc: 0.88732394\n",
      "val Loss: 0.26760441 Acc: 0.90666667\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.28462106 Acc: 0.87636933\n",
      "val Loss: 0.27080011 Acc: 0.92000000\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.29430960 Acc: 0.86384977\n",
      "val Loss: 0.27745208 Acc: 0.92000000\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.33111101 Acc: 0.87010955\n",
      "val Loss: 0.27600873 Acc: 0.90666667\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.27759998 Acc: 0.88732394\n",
      "val Loss: 0.27777806 Acc: 0.92000000\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.30384833 Acc: 0.87636933\n",
      "val Loss: 0.27399993 Acc: 0.90666667\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.28002117 Acc: 0.88575900\n",
      "val Loss: 0.29352725 Acc: 0.92000000\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.31542913 Acc: 0.85446009\n",
      "val Loss: 0.28091830 Acc: 0.92000000\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.32447543 Acc: 0.86541471\n",
      "val Loss: 0.28580913 Acc: 0.92000000\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.28744477 Acc: 0.87949922\n",
      "val Loss: 0.27904150 Acc: 0.90666667\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.27009517 Acc: 0.90297340\n",
      "val Loss: 0.27267024 Acc: 0.90666667\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.31849784 Acc: 0.86541471\n",
      "val Loss: 0.27179903 Acc: 0.90666667\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.25176225 Acc: 0.90297340\n",
      "val Loss: 0.27673998 Acc: 0.92000000\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.29921723 Acc: 0.87480438\n",
      "val Loss: 0.27279359 Acc: 0.92000000\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.25712833 Acc: 0.90297340\n",
      "val Loss: 0.27612957 Acc: 0.90666667\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.31550432 Acc: 0.86228482\n",
      "val Loss: 0.27964386 Acc: 0.90666667\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.28710618 Acc: 0.88419405\n",
      "val Loss: 0.27194309 Acc: 0.90666667\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.28058517 Acc: 0.88575900\n",
      "val Loss: 0.28074658 Acc: 0.90666667\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.27319036 Acc: 0.89984351\n",
      "val Loss: 0.27395788 Acc: 0.92000000\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.28650380 Acc: 0.89045383\n",
      "val Loss: 0.27111086 Acc: 0.90666667\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.26052671 Acc: 0.89045383\n",
      "val Loss: 0.27029845 Acc: 0.92000000\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.27410388 Acc: 0.89045383\n",
      "val Loss: 0.27199423 Acc: 0.92000000\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.27428064 Acc: 0.89514867\n",
      "val Loss: 0.27279636 Acc: 0.92000000\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.28222328 Acc: 0.89201878\n",
      "val Loss: 0.27880418 Acc: 0.92000000\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.28357672 Acc: 0.88888889\n",
      "val Loss: 0.28300533 Acc: 0.92000000\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.29032054 Acc: 0.87480438\n",
      "val Loss: 0.28118414 Acc: 0.92000000\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.27483856 Acc: 0.88419405\n",
      "val Loss: 0.27639621 Acc: 0.92000000\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.26820891 Acc: 0.88106416\n",
      "val Loss: 0.27504930 Acc: 0.92000000\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.26258448 Acc: 0.88419405\n",
      "val Loss: 0.27127886 Acc: 0.92000000\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.30130362 Acc: 0.87323944\n",
      "val Loss: 0.27942762 Acc: 0.90666667\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.30956109 Acc: 0.87323944\n",
      "val Loss: 0.27651456 Acc: 0.92000000\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.28448073 Acc: 0.89045383\n",
      "val Loss: 0.27786255 Acc: 0.92000000\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.31674225 Acc: 0.87323944\n",
      "val Loss: 0.28736982 Acc: 0.92000000\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.25538865 Acc: 0.88888889\n",
      "val Loss: 0.27915204 Acc: 0.92000000\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.28446306 Acc: 0.87323944\n",
      "val Loss: 0.27150568 Acc: 0.90666667\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.29165176 Acc: 0.88419405\n",
      "val Loss: 0.27679005 Acc: 0.92000000\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.28793037 Acc: 0.88419405\n",
      "val Loss: 0.27390957 Acc: 0.90666667\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.27889330 Acc: 0.88575900\n",
      "val Loss: 0.28117207 Acc: 0.92000000\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.29126416 Acc: 0.87636933\n",
      "val Loss: 0.27675447 Acc: 0.92000000\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.25003946 Acc: 0.90610329\n",
      "val Loss: 0.27337706 Acc: 0.92000000\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.28005036 Acc: 0.89201878\n",
      "val Loss: 0.26703179 Acc: 0.92000000\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.26887639 Acc: 0.88888889\n",
      "val Loss: 0.27021000 Acc: 0.92000000\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.30499903 Acc: 0.87636933\n",
      "val Loss: 0.27967787 Acc: 0.92000000\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.29370505 Acc: 0.87949922\n",
      "val Loss: 0.28930110 Acc: 0.92000000\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.25705738 Acc: 0.89514867\n",
      "val Loss: 0.28502628 Acc: 0.92000000\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.27997856 Acc: 0.89514867\n",
      "val Loss: 0.28145906 Acc: 0.92000000\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.32032991 Acc: 0.87793427\n",
      "val Loss: 0.28077522 Acc: 0.92000000\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.29046165 Acc: 0.87636933\n",
      "val Loss: 0.27307028 Acc: 0.90666667\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.28710565 Acc: 0.89984351\n",
      "val Loss: 0.27478921 Acc: 0.92000000\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.28467082 Acc: 0.87480438\n",
      "val Loss: 0.28165886 Acc: 0.92000000\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.29249028 Acc: 0.87636933\n",
      "val Loss: 0.28302652 Acc: 0.92000000\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.26398917 Acc: 0.89671362\n",
      "val Loss: 0.28195843 Acc: 0.92000000\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.27964067 Acc: 0.89358372\n",
      "val Loss: 0.27308756 Acc: 0.90666667\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.28138501 Acc: 0.88106416\n",
      "val Loss: 0.27478734 Acc: 0.90666667\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.29837245 Acc: 0.88419405\n",
      "val Loss: 0.27389503 Acc: 0.90666667\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.27446120 Acc: 0.88888889\n",
      "val Loss: 0.28310549 Acc: 0.92000000\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.29276237 Acc: 0.88106416\n",
      "val Loss: 0.29372734 Acc: 0.92000000\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.28090858 Acc: 0.87480438\n",
      "val Loss: 0.28605014 Acc: 0.92000000\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.28239984 Acc: 0.89358372\n",
      "val Loss: 0.28111845 Acc: 0.92000000\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.27649617 Acc: 0.88262911\n",
      "val Loss: 0.28367305 Acc: 0.92000000\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.26660098 Acc: 0.89514867\n",
      "val Loss: 0.27888003 Acc: 0.92000000\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.27393167 Acc: 0.89201878\n",
      "val Loss: 0.28123012 Acc: 0.92000000\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.29999824 Acc: 0.88732394\n",
      "val Loss: 0.27959037 Acc: 0.92000000\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.31338070 Acc: 0.86854460\n",
      "val Loss: 0.27833688 Acc: 0.92000000\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.27776561 Acc: 0.89045383\n",
      "val Loss: 0.27087352 Acc: 0.92000000\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.31084835 Acc: 0.88106416\n",
      "val Loss: 0.27372363 Acc: 0.92000000\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.31311928 Acc: 0.86854460\n",
      "val Loss: 0.27180311 Acc: 0.90666667\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.32317577 Acc: 0.87010955\n",
      "val Loss: 0.26760134 Acc: 0.92000000\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.29425079 Acc: 0.87636933\n",
      "val Loss: 0.27706680 Acc: 0.92000000\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.30537172 Acc: 0.88419405\n",
      "val Loss: 0.27998981 Acc: 0.92000000\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.25979549 Acc: 0.89201878\n",
      "val Loss: 0.27783909 Acc: 0.92000000\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.28279154 Acc: 0.87323944\n",
      "val Loss: 0.28643394 Acc: 0.92000000\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.26388684 Acc: 0.89827856\n",
      "val Loss: 0.28398076 Acc: 0.92000000\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.27339673 Acc: 0.90453834\n",
      "val Loss: 0.26891699 Acc: 0.92000000\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.30091291 Acc: 0.87636933\n",
      "val Loss: 0.27963755 Acc: 0.92000000\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.29586212 Acc: 0.88106416\n",
      "val Loss: 0.28483906 Acc: 0.92000000\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.28431987 Acc: 0.88419405\n",
      "val Loss: 0.28419635 Acc: 0.92000000\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.29398756 Acc: 0.89201878\n",
      "val Loss: 0.28935152 Acc: 0.92000000\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.30184199 Acc: 0.87480438\n",
      "val Loss: 0.28211191 Acc: 0.92000000\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.27650380 Acc: 0.88888889\n",
      "val Loss: 0.28336689 Acc: 0.92000000\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.26703716 Acc: 0.88106416\n",
      "val Loss: 0.28222695 Acc: 0.92000000\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.27978930 Acc: 0.89514867\n",
      "val Loss: 0.27859852 Acc: 0.92000000\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.28261961 Acc: 0.88732394\n",
      "val Loss: 0.28848454 Acc: 0.92000000\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.29873325 Acc: 0.90140845\n",
      "val Loss: 0.28600031 Acc: 0.92000000\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.27256487 Acc: 0.89827856\n",
      "val Loss: 0.27687696 Acc: 0.92000000\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.27058082 Acc: 0.89358372\n",
      "val Loss: 0.28377467 Acc: 0.92000000\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.27333480 Acc: 0.88732394\n",
      "val Loss: 0.28984272 Acc: 0.92000000\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.29043367 Acc: 0.88575900\n",
      "val Loss: 0.27303559 Acc: 0.92000000\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.29149806 Acc: 0.89358372\n",
      "val Loss: 0.27336815 Acc: 0.92000000\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.33106232 Acc: 0.86697966\n",
      "val Loss: 0.26685724 Acc: 0.90666667\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.29576420 Acc: 0.88106416\n",
      "val Loss: 0.26649562 Acc: 0.92000000\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.31083243 Acc: 0.87010955\n",
      "val Loss: 0.28091928 Acc: 0.92000000\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.28061290 Acc: 0.88262911\n",
      "val Loss: 0.27461827 Acc: 0.92000000\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.30125794 Acc: 0.89201878\n",
      "val Loss: 0.27676567 Acc: 0.92000000\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.28154136 Acc: 0.89201878\n",
      "val Loss: 0.27969465 Acc: 0.92000000\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.26972935 Acc: 0.88888889\n",
      "val Loss: 0.27299821 Acc: 0.92000000\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.31585871 Acc: 0.86854460\n",
      "val Loss: 0.28431869 Acc: 0.92000000\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.26334428 Acc: 0.89514867\n",
      "val Loss: 0.27004573 Acc: 0.92000000\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.23591015 Acc: 0.90766823\n",
      "val Loss: 0.27473113 Acc: 0.92000000\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.28304694 Acc: 0.88888889\n",
      "val Loss: 0.27721834 Acc: 0.92000000\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.25833528 Acc: 0.89827856\n",
      "val Loss: 0.28174752 Acc: 0.90666667\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.27390125 Acc: 0.89201878\n",
      "val Loss: 0.27590480 Acc: 0.92000000\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.26037006 Acc: 0.90610329\n",
      "val Loss: 0.28526956 Acc: 0.92000000\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.26761769 Acc: 0.89827856\n",
      "val Loss: 0.28558844 Acc: 0.92000000\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.27562730 Acc: 0.88732394\n",
      "val Loss: 0.27882192 Acc: 0.92000000\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.25859526 Acc: 0.88419405\n",
      "val Loss: 0.27684838 Acc: 0.92000000\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.29872855 Acc: 0.88262911\n",
      "val Loss: 0.28552914 Acc: 0.92000000\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.30673128 Acc: 0.87167449\n",
      "val Loss: 0.28103137 Acc: 0.92000000\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.24925094 Acc: 0.88888889\n",
      "val Loss: 0.27206007 Acc: 0.92000000\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.26230889 Acc: 0.88732394\n",
      "val Loss: 0.27542338 Acc: 0.92000000\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.31308149 Acc: 0.86384977\n",
      "val Loss: 0.27751049 Acc: 0.92000000\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.29594095 Acc: 0.88575900\n",
      "val Loss: 0.27323973 Acc: 0.92000000\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.26985743 Acc: 0.89358372\n",
      "val Loss: 0.26862279 Acc: 0.90666667\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.27198058 Acc: 0.88419405\n",
      "val Loss: 0.26903182 Acc: 0.90666667\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.30439774 Acc: 0.86384977\n",
      "val Loss: 0.27574581 Acc: 0.92000000\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.21878030 Acc: 0.89827856\n",
      "val Loss: 0.27888611 Acc: 0.92000000\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.26306451 Acc: 0.88262911\n",
      "val Loss: 0.28530252 Acc: 0.92000000\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.28565403 Acc: 0.88888889\n",
      "val Loss: 0.28847310 Acc: 0.92000000\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.26960750 Acc: 0.88575900\n",
      "val Loss: 0.28939867 Acc: 0.90666667\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.30514660 Acc: 0.88262911\n",
      "val Loss: 0.28552034 Acc: 0.92000000\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.30834920 Acc: 0.87010955\n",
      "val Loss: 0.28592286 Acc: 0.92000000\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.27375732 Acc: 0.88732394\n",
      "val Loss: 0.28741422 Acc: 0.92000000\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.27069576 Acc: 0.87167449\n",
      "val Loss: 0.27738029 Acc: 0.92000000\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.28685336 Acc: 0.88575900\n",
      "val Loss: 0.28175822 Acc: 0.92000000\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.29685651 Acc: 0.89201878\n",
      "val Loss: 0.28773385 Acc: 0.92000000\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.26414930 Acc: 0.88732394\n",
      "val Loss: 0.29009357 Acc: 0.90666667\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.30932480 Acc: 0.87167449\n",
      "val Loss: 0.29019681 Acc: 0.92000000\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.28019029 Acc: 0.88419405\n",
      "val Loss: 0.28380674 Acc: 0.92000000\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.30228357 Acc: 0.86854460\n",
      "val Loss: 0.28330562 Acc: 0.92000000\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.31904974 Acc: 0.86541471\n",
      "val Loss: 0.29003596 Acc: 0.92000000\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.27728253 Acc: 0.89514867\n",
      "val Loss: 0.29967529 Acc: 0.90666667\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.24700722 Acc: 0.88575900\n",
      "val Loss: 0.28799981 Acc: 0.90666667\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.29952054 Acc: 0.87949922\n",
      "val Loss: 0.29316050 Acc: 0.92000000\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.29728355 Acc: 0.87480438\n",
      "val Loss: 0.27869573 Acc: 0.92000000\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.30618413 Acc: 0.87010955\n",
      "val Loss: 0.27745444 Acc: 0.92000000\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.31842247 Acc: 0.86854460\n",
      "val Loss: 0.28068787 Acc: 0.92000000\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.28426686 Acc: 0.90140845\n",
      "val Loss: 0.28446293 Acc: 0.92000000\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.30208467 Acc: 0.87793427\n",
      "val Loss: 0.28228602 Acc: 0.92000000\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.28556310 Acc: 0.88888889\n",
      "val Loss: 0.28364405 Acc: 0.92000000\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.25933634 Acc: 0.88888889\n",
      "val Loss: 0.27865121 Acc: 0.92000000\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.24701412 Acc: 0.90453834\n",
      "val Loss: 0.28550729 Acc: 0.92000000\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.28488450 Acc: 0.87323944\n",
      "val Loss: 0.27570176 Acc: 0.90666667\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.29813559 Acc: 0.87636933\n",
      "val Loss: 0.27822953 Acc: 0.92000000\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.30250279 Acc: 0.85446009\n",
      "val Loss: 0.27320090 Acc: 0.92000000\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.24782582 Acc: 0.90923318\n",
      "val Loss: 0.27745315 Acc: 0.92000000\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.28619464 Acc: 0.88575900\n",
      "val Loss: 0.27476355 Acc: 0.90666667\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.25449403 Acc: 0.89514867\n",
      "val Loss: 0.26945972 Acc: 0.92000000\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.28998624 Acc: 0.87636933\n",
      "val Loss: 0.26830155 Acc: 0.92000000\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.28166725 Acc: 0.88262911\n",
      "val Loss: 0.27481869 Acc: 0.92000000\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.26210575 Acc: 0.89671362\n",
      "val Loss: 0.27926776 Acc: 0.92000000\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.30629143 Acc: 0.87480438\n",
      "val Loss: 0.27936155 Acc: 0.92000000\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.29521082 Acc: 0.88732394\n",
      "val Loss: 0.28611308 Acc: 0.92000000\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.25889743 Acc: 0.89514867\n",
      "val Loss: 0.28758040 Acc: 0.90666667\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.24857206 Acc: 0.89358372\n",
      "val Loss: 0.28209683 Acc: 0.92000000\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.31439797 Acc: 0.87793427\n",
      "val Loss: 0.27690801 Acc: 0.92000000\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.25227435 Acc: 0.90140845\n",
      "val Loss: 0.28114992 Acc: 0.92000000\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.23981728 Acc: 0.91079812\n",
      "val Loss: 0.28379631 Acc: 0.92000000\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.30039927 Acc: 0.87793427\n",
      "val Loss: 0.27904257 Acc: 0.92000000\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.26806305 Acc: 0.89045383\n",
      "val Loss: 0.28073272 Acc: 0.92000000\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.31095644 Acc: 0.86697966\n",
      "val Loss: 0.28429648 Acc: 0.92000000\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.28686327 Acc: 0.88732394\n",
      "val Loss: 0.27504745 Acc: 0.92000000\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.25974289 Acc: 0.89358372\n",
      "val Loss: 0.27848312 Acc: 0.92000000\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.25994783 Acc: 0.89514867\n",
      "val Loss: 0.27587375 Acc: 0.92000000\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.23322044 Acc: 0.90453834\n",
      "val Loss: 0.28599682 Acc: 0.92000000\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.23836795 Acc: 0.90297340\n",
      "val Loss: 0.28132054 Acc: 0.92000000\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.27700922 Acc: 0.89358372\n",
      "val Loss: 0.28700322 Acc: 0.92000000\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.27102630 Acc: 0.87323944\n",
      "val Loss: 0.28793946 Acc: 0.92000000\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.27183291 Acc: 0.88888889\n",
      "val Loss: 0.28609872 Acc: 0.92000000\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.23646434 Acc: 0.90610329\n",
      "val Loss: 0.28680134 Acc: 0.92000000\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.31939359 Acc: 0.87949922\n",
      "val Loss: 0.27388012 Acc: 0.92000000\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.28914270 Acc: 0.87793427\n",
      "val Loss: 0.27635646 Acc: 0.92000000\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.27742039 Acc: 0.87480438\n",
      "val Loss: 0.28061980 Acc: 0.92000000\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.30857476 Acc: 0.88419405\n",
      "val Loss: 0.28852749 Acc: 0.92000000\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.26847806 Acc: 0.87949922\n",
      "val Loss: 0.28450766 Acc: 0.92000000\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.31031374 Acc: 0.89045383\n",
      "val Loss: 0.27854443 Acc: 0.92000000\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.29163435 Acc: 0.87793427\n",
      "val Loss: 0.28380522 Acc: 0.92000000\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.32501201 Acc: 0.86228482\n",
      "val Loss: 0.28113282 Acc: 0.92000000\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.31768367 Acc: 0.86228482\n",
      "val Loss: 0.27579501 Acc: 0.92000000\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.28913714 Acc: 0.88262911\n",
      "val Loss: 0.27934325 Acc: 0.92000000\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.31391516 Acc: 0.87793427\n",
      "val Loss: 0.27642691 Acc: 0.92000000\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.26345801 Acc: 0.88888889\n",
      "val Loss: 0.27120954 Acc: 0.92000000\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.27306441 Acc: 0.89201878\n",
      "val Loss: 0.27569738 Acc: 0.92000000\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.30073044 Acc: 0.88106416\n",
      "val Loss: 0.28768173 Acc: 0.92000000\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.32748613 Acc: 0.86384977\n",
      "val Loss: 0.28584373 Acc: 0.92000000\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.25564974 Acc: 0.89671362\n",
      "val Loss: 0.28647590 Acc: 0.92000000\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.30025237 Acc: 0.88575900\n",
      "val Loss: 0.29127157 Acc: 0.92000000\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.29678835 Acc: 0.87323944\n",
      "val Loss: 0.28419644 Acc: 0.92000000\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.26867608 Acc: 0.88732394\n",
      "val Loss: 0.28028238 Acc: 0.92000000\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.26325496 Acc: 0.89671362\n",
      "val Loss: 0.27671611 Acc: 0.92000000\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.26372500 Acc: 0.88888889\n",
      "val Loss: 0.26854619 Acc: 0.92000000\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.26478698 Acc: 0.88419405\n",
      "val Loss: 0.28041270 Acc: 0.92000000\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.30801170 Acc: 0.88575900\n",
      "val Loss: 0.28516340 Acc: 0.92000000\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.27504028 Acc: 0.89671362\n",
      "val Loss: 0.28039700 Acc: 0.90666667\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.31200225 Acc: 0.86228482\n",
      "val Loss: 0.28147042 Acc: 0.92000000\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.29527227 Acc: 0.87167449\n",
      "val Loss: 0.28314406 Acc: 0.92000000\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.33135944 Acc: 0.84976526\n",
      "val Loss: 0.29033515 Acc: 0.92000000\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.31247084 Acc: 0.88419405\n",
      "val Loss: 0.27437398 Acc: 0.92000000\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.29538170 Acc: 0.87636933\n",
      "val Loss: 0.27319017 Acc: 0.92000000\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.28216827 Acc: 0.88575900\n",
      "val Loss: 0.28752956 Acc: 0.92000000\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.29423790 Acc: 0.87949922\n",
      "val Loss: 0.27593693 Acc: 0.92000000\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.31766108 Acc: 0.86854460\n",
      "val Loss: 0.28913644 Acc: 0.92000000\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.27812562 Acc: 0.88888889\n",
      "val Loss: 0.27532580 Acc: 0.92000000\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.26232012 Acc: 0.87949922\n",
      "val Loss: 0.28637400 Acc: 0.92000000\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.30099746 Acc: 0.88106416\n",
      "val Loss: 0.28976381 Acc: 0.92000000\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.30042116 Acc: 0.88575900\n",
      "val Loss: 0.28084475 Acc: 0.92000000\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.27983131 Acc: 0.87636933\n",
      "val Loss: 0.28187183 Acc: 0.92000000\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.27496342 Acc: 0.89201878\n",
      "val Loss: 0.28163743 Acc: 0.92000000\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.26465016 Acc: 0.88419405\n",
      "val Loss: 0.27155051 Acc: 0.92000000\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.30594894 Acc: 0.86854460\n",
      "val Loss: 0.27976701 Acc: 0.92000000\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.28043136 Acc: 0.89201878\n",
      "val Loss: 0.27786919 Acc: 0.92000000\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.24899536 Acc: 0.91392801\n",
      "val Loss: 0.29102853 Acc: 0.92000000\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.27604797 Acc: 0.87636933\n",
      "val Loss: 0.28427741 Acc: 0.92000000\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.27890316 Acc: 0.89358372\n",
      "val Loss: 0.27986911 Acc: 0.92000000\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.27233718 Acc: 0.89201878\n",
      "val Loss: 0.27922952 Acc: 0.92000000\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.31388283 Acc: 0.87793427\n",
      "val Loss: 0.26781127 Acc: 0.92000000\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.30387513 Acc: 0.88575900\n",
      "val Loss: 0.27683917 Acc: 0.92000000\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.28078528 Acc: 0.88575900\n",
      "val Loss: 0.28075704 Acc: 0.92000000\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.32051981 Acc: 0.85758998\n",
      "val Loss: 0.27857858 Acc: 0.92000000\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.22614016 Acc: 0.90923318\n",
      "val Loss: 0.27894583 Acc: 0.92000000\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.25329744 Acc: 0.90610329\n",
      "val Loss: 0.27923191 Acc: 0.92000000\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.27220933 Acc: 0.88888889\n",
      "val Loss: 0.28465876 Acc: 0.92000000\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.29288037 Acc: 0.88106416\n",
      "val Loss: 0.26938590 Acc: 0.90666667\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.30115928 Acc: 0.87167449\n",
      "val Loss: 0.27174813 Acc: 0.92000000\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.29935049 Acc: 0.89358372\n",
      "val Loss: 0.26971880 Acc: 0.90666667\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.28382940 Acc: 0.88888889\n",
      "val Loss: 0.27654719 Acc: 0.92000000\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.29637640 Acc: 0.87636933\n",
      "val Loss: 0.27617121 Acc: 0.92000000\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.29909796 Acc: 0.86384977\n",
      "val Loss: 0.27794975 Acc: 0.90666667\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.27231951 Acc: 0.90297340\n",
      "val Loss: 0.27795857 Acc: 0.92000000\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.26610691 Acc: 0.90610329\n",
      "val Loss: 0.27290273 Acc: 0.90666667\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.26497567 Acc: 0.89358372\n",
      "val Loss: 0.27769378 Acc: 0.92000000\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.28255092 Acc: 0.88888889\n",
      "val Loss: 0.27039516 Acc: 0.92000000\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.27724465 Acc: 0.87480438\n",
      "val Loss: 0.27718288 Acc: 0.92000000\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.28503540 Acc: 0.89201878\n",
      "val Loss: 0.27678582 Acc: 0.90666667\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.26137380 Acc: 0.88106416\n",
      "val Loss: 0.28111443 Acc: 0.92000000\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.28344401 Acc: 0.88575900\n",
      "val Loss: 0.27962178 Acc: 0.90666667\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.24805479 Acc: 0.91392801\n",
      "val Loss: 0.26826832 Acc: 0.92000000\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.31717112 Acc: 0.87480438\n",
      "val Loss: 0.28035307 Acc: 0.92000000\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.26724146 Acc: 0.88732394\n",
      "val Loss: 0.27956548 Acc: 0.92000000\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.26899944 Acc: 0.89514867\n",
      "val Loss: 0.27992925 Acc: 0.90666667\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.26679260 Acc: 0.89045383\n",
      "val Loss: 0.28046831 Acc: 0.92000000\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.26764527 Acc: 0.88106416\n",
      "val Loss: 0.27568817 Acc: 0.90666667\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.33950750 Acc: 0.87010955\n",
      "val Loss: 0.27746037 Acc: 0.90666667\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.24839799 Acc: 0.90453834\n",
      "val Loss: 0.27783242 Acc: 0.92000000\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.29459361 Acc: 0.88575900\n",
      "val Loss: 0.28673324 Acc: 0.92000000\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.29541049 Acc: 0.86697966\n",
      "val Loss: 0.27468747 Acc: 0.92000000\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.26920652 Acc: 0.87793427\n",
      "val Loss: 0.27703151 Acc: 0.92000000\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.28455599 Acc: 0.88732394\n",
      "val Loss: 0.28172895 Acc: 0.92000000\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.27995851 Acc: 0.88262911\n",
      "val Loss: 0.27571058 Acc: 0.92000000\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.30396476 Acc: 0.88419405\n",
      "val Loss: 0.28769359 Acc: 0.92000000\n",
      "\n",
      "Training complete in 31m 38s\n",
      "Best val Acc: 0.94666667 Best val loss: 0.23267163\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/EfficientNet_B0_dataset_Yang.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 300,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
