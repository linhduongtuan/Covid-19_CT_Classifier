{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'non-COVID']\n",
      "{'train': 1985, 'test': 497}\n",
      "cuda:0\n",
      "{0: 'COVID', 1: 'non-COVID'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19_CT'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 100\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc): Linear(in_features=2432, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = models.resnet50(pretrained=True)\n",
    "#model = timm.create_model('resnet50', pretrained=True)\n",
    "model = timm.create_model('tresnet_l', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.head #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58619970\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = classifier\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.049485715919397245, Val accuracy: 0.9798792756539236\n",
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.13722910 Acc: 0.94005038\n",
      "test Loss: 0.05331970 Acc: 0.96981891\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.14090408 Acc: 0.94055416\n",
      "test Loss: 0.05186374 Acc: 0.97384306\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.14183411 Acc: 0.94105793\n",
      "test Loss: 0.05218005 Acc: 0.97384306\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.14104913 Acc: 0.93702771\n",
      "test Loss: 0.05118430 Acc: 0.97585513\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.13237273 Acc: 0.94458438\n",
      "test Loss: 0.05477336 Acc: 0.96981891\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.13806123 Acc: 0.94508816\n",
      "test Loss: 0.05238691 Acc: 0.97384306\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.12874452 Acc: 0.94710327\n",
      "test Loss: 0.05237838 Acc: 0.96981891\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.13042319 Acc: 0.94659950\n",
      "test Loss: 0.05388725 Acc: 0.96981891\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.13290568 Acc: 0.94206549\n",
      "test Loss: 0.05330174 Acc: 0.96981891\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.13099452 Acc: 0.94911839\n",
      "test Loss: 0.05150120 Acc: 0.97183099\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.14243273 Acc: 0.94609572\n",
      "test Loss: 0.05166920 Acc: 0.97585513\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.12870421 Acc: 0.94760705\n",
      "test Loss: 0.05169204 Acc: 0.97585513\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.12303710 Acc: 0.94962217\n",
      "test Loss: 0.05183660 Acc: 0.97183099\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.12774433 Acc: 0.94962217\n",
      "test Loss: 0.05135057 Acc: 0.97786720\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.13297537 Acc: 0.94760705\n",
      "test Loss: 0.05099080 Acc: 0.97585513\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.13414993 Acc: 0.94256927\n",
      "test Loss: 0.05284089 Acc: 0.97384306\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.12743796 Acc: 0.94710327\n",
      "test Loss: 0.05262347 Acc: 0.97384306\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.14039224 Acc: 0.94357683\n",
      "test Loss: 0.05227639 Acc: 0.97585513\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.12750935 Acc: 0.94760705\n",
      "test Loss: 0.05206419 Acc: 0.97585513\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.12767620 Acc: 0.94559194\n",
      "test Loss: 0.05183183 Acc: 0.97786720\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.13477572 Acc: 0.93954660\n",
      "test Loss: 0.05330533 Acc: 0.97183099\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.13257774 Acc: 0.94508816\n",
      "test Loss: 0.05311301 Acc: 0.96981891\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.14008156 Acc: 0.94206549\n",
      "test Loss: 0.05205307 Acc: 0.97786720\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.12815701 Acc: 0.94256927\n",
      "test Loss: 0.05101862 Acc: 0.97585513\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.13007371 Acc: 0.94458438\n",
      "test Loss: 0.05158739 Acc: 0.97384306\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.12799100 Acc: 0.94760705\n",
      "test Loss: 0.05023692 Acc: 0.97786720\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.13682585 Acc: 0.94156171\n",
      "test Loss: 0.05066272 Acc: 0.97786720\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.13087711 Acc: 0.94408060\n",
      "test Loss: 0.05410631 Acc: 0.96981891\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.13017129 Acc: 0.94105793\n",
      "test Loss: 0.05600948 Acc: 0.96981891\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.13500182 Acc: 0.94256927\n",
      "test Loss: 0.05106493 Acc: 0.97585513\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.12535577 Acc: 0.94307305\n",
      "test Loss: 0.05199468 Acc: 0.97384306\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.13269201 Acc: 0.94105793\n",
      "test Loss: 0.05317828 Acc: 0.97183099\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.13742273 Acc: 0.94206549\n",
      "test Loss: 0.05294010 Acc: 0.96981891\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.12589671 Acc: 0.94659950\n",
      "test Loss: 0.05248378 Acc: 0.96981891\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.12231152 Acc: 0.95113350\n",
      "test Loss: 0.05209615 Acc: 0.97585513\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.13836671 Acc: 0.94055416\n",
      "test Loss: 0.05286202 Acc: 0.97384306\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.13555316 Acc: 0.94156171\n",
      "test Loss: 0.05323836 Acc: 0.96981891\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.13564265 Acc: 0.94105793\n",
      "test Loss: 0.05285666 Acc: 0.97183099\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.13911396 Acc: 0.93853904\n",
      "test Loss: 0.05159071 Acc: 0.97786720\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.14073717 Acc: 0.94458438\n",
      "test Loss: 0.05400213 Acc: 0.96981891\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.12875025 Acc: 0.94609572\n",
      "test Loss: 0.05381485 Acc: 0.96981891\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.13651079 Acc: 0.94508816\n",
      "test Loss: 0.05139953 Acc: 0.97585513\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.12780988 Acc: 0.94609572\n",
      "test Loss: 0.05186457 Acc: 0.97585513\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.13291450 Acc: 0.94559194\n",
      "test Loss: 0.05311127 Acc: 0.96981891\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.13170804 Acc: 0.94307305\n",
      "test Loss: 0.05296276 Acc: 0.96981891\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.13425188 Acc: 0.94458438\n",
      "test Loss: 0.05191551 Acc: 0.97786720\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.14123889 Acc: 0.93803526\n",
      "test Loss: 0.05170516 Acc: 0.97384306\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.13128004 Acc: 0.93904282\n",
      "test Loss: 0.05207644 Acc: 0.97384306\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.13172424 Acc: 0.94861461\n",
      "test Loss: 0.05249841 Acc: 0.97384306\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.12073394 Acc: 0.94861461\n",
      "test Loss: 0.05352742 Acc: 0.96981891\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.13522545 Acc: 0.94206549\n",
      "test Loss: 0.05155868 Acc: 0.97987928\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.13150332 Acc: 0.94307305\n",
      "test Loss: 0.05069352 Acc: 0.97786720\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.12562693 Acc: 0.94659950\n",
      "test Loss: 0.05365837 Acc: 0.96981891\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.12250438 Acc: 0.95062972\n",
      "test Loss: 0.05253219 Acc: 0.97183099\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.12616827 Acc: 0.94609572\n",
      "test Loss: 0.05198693 Acc: 0.97384306\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.14674977 Acc: 0.93198992\n",
      "test Loss: 0.05140072 Acc: 0.97585513\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.12031921 Acc: 0.95113350\n",
      "test Loss: 0.05300410 Acc: 0.97585513\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.14372735 Acc: 0.93753149\n",
      "test Loss: 0.05320045 Acc: 0.96981891\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.14241890 Acc: 0.93602015\n",
      "test Loss: 0.05256351 Acc: 0.97384306\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.14546279 Acc: 0.93702771\n",
      "test Loss: 0.05169153 Acc: 0.97585513\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.13047634 Acc: 0.94811083\n",
      "test Loss: 0.05222585 Acc: 0.97183099\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.12710852 Acc: 0.94760705\n",
      "test Loss: 0.05414960 Acc: 0.96981891\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.11279481 Acc: 0.94962217\n",
      "test Loss: 0.05249968 Acc: 0.97183099\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.12744063 Acc: 0.94659950\n",
      "test Loss: 0.05247933 Acc: 0.97183099\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.12707954 Acc: 0.94408060\n",
      "test Loss: 0.05262736 Acc: 0.97585513\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.14220086 Acc: 0.93753149\n",
      "test Loss: 0.05358771 Acc: 0.97384306\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.13765662 Acc: 0.93753149\n",
      "test Loss: 0.05161040 Acc: 0.97585513\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.13283640 Acc: 0.94458438\n",
      "test Loss: 0.05122929 Acc: 0.97786720\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.12682206 Acc: 0.94408060\n",
      "test Loss: 0.05282241 Acc: 0.97183099\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.12750623 Acc: 0.94760705\n",
      "test Loss: 0.05183099 Acc: 0.97786720\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.12616959 Acc: 0.94005038\n",
      "test Loss: 0.05045549 Acc: 0.97585513\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.13772055 Acc: 0.93702771\n",
      "test Loss: 0.05274446 Acc: 0.97183099\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.13280146 Acc: 0.94156171\n",
      "test Loss: 0.05296539 Acc: 0.96981891\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.12635532 Acc: 0.94458438\n",
      "test Loss: 0.05204670 Acc: 0.97183099\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.13253604 Acc: 0.94307305\n",
      "test Loss: 0.05249563 Acc: 0.97183099\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.13177657 Acc: 0.94861461\n",
      "test Loss: 0.05233534 Acc: 0.97183099\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.12906291 Acc: 0.94256927\n",
      "test Loss: 0.05090510 Acc: 0.97786720\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.14032920 Acc: 0.93551637\n",
      "test Loss: 0.05122104 Acc: 0.97585513\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.12312118 Acc: 0.94760705\n",
      "test Loss: 0.05188732 Acc: 0.97585513\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.14551118 Acc: 0.93803526\n",
      "test Loss: 0.05132617 Acc: 0.97585513\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.13625930 Acc: 0.93954660\n",
      "test Loss: 0.05113076 Acc: 0.97786720\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.14020789 Acc: 0.93954660\n",
      "test Loss: 0.05217110 Acc: 0.97585513\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.12366629 Acc: 0.94609572\n",
      "test Loss: 0.05223116 Acc: 0.97585513\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.12904516 Acc: 0.94105793\n",
      "test Loss: 0.05088804 Acc: 0.97585513\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.11892117 Acc: 0.95012594\n",
      "test Loss: 0.05165751 Acc: 0.97585513\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.12695401 Acc: 0.94408060\n",
      "test Loss: 0.05297605 Acc: 0.96981891\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.13318504 Acc: 0.94911839\n",
      "test Loss: 0.05122860 Acc: 0.97786720\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.12691984 Acc: 0.94659950\n",
      "test Loss: 0.05272278 Acc: 0.97183099\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.12041196 Acc: 0.94861461\n",
      "test Loss: 0.05156484 Acc: 0.97384306\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.13751576 Acc: 0.94105793\n",
      "test Loss: 0.05053383 Acc: 0.97786720\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.11825825 Acc: 0.94710327\n",
      "test Loss: 0.05248617 Acc: 0.97183099\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.12226278 Acc: 0.95415617\n",
      "test Loss: 0.05197839 Acc: 0.97384306\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.13154808 Acc: 0.94609572\n",
      "test Loss: 0.05250943 Acc: 0.97384306\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.13120316 Acc: 0.94710327\n",
      "test Loss: 0.05280743 Acc: 0.97183099\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.13768759 Acc: 0.93954660\n",
      "test Loss: 0.05166714 Acc: 0.97786720\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.13626874 Acc: 0.94256927\n",
      "test Loss: 0.05242869 Acc: 0.97384306\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.13055061 Acc: 0.94710327\n",
      "test Loss: 0.05231979 Acc: 0.97384306\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.12933809 Acc: 0.94911839\n",
      "test Loss: 0.05118916 Acc: 0.97585513\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.15048844 Acc: 0.93148615\n",
      "test Loss: 0.05365520 Acc: 0.97183099\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.13341015 Acc: 0.94357683\n",
      "test Loss: 0.05301774 Acc: 0.97183099\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.13420905 Acc: 0.94256927\n",
      "test Loss: 0.05185459 Acc: 0.97585513\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.12161443 Acc: 0.94760705\n",
      "test Loss: 0.05351857 Acc: 0.97183099\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.12670936 Acc: 0.94307305\n",
      "test Loss: 0.05133286 Acc: 0.97585513\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.13228843 Acc: 0.94710327\n",
      "test Loss: 0.05285816 Acc: 0.97183099\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.12781592 Acc: 0.94458438\n",
      "test Loss: 0.05306120 Acc: 0.96981891\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.12504368 Acc: 0.94508816\n",
      "test Loss: 0.05333381 Acc: 0.96981891\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.13436059 Acc: 0.94206549\n",
      "test Loss: 0.05316640 Acc: 0.96981891\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.13627562 Acc: 0.94508816\n",
      "test Loss: 0.05213743 Acc: 0.96981891\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.14267998 Acc: 0.93954660\n",
      "test Loss: 0.05295784 Acc: 0.96981891\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.15265933 Acc: 0.93098237\n",
      "test Loss: 0.05301912 Acc: 0.97183099\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.13450980 Acc: 0.94105793\n",
      "test Loss: 0.05435037 Acc: 0.96981891\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.14146595 Acc: 0.94156171\n",
      "test Loss: 0.05308970 Acc: 0.96981891\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.13160425 Acc: 0.94962217\n",
      "test Loss: 0.05256283 Acc: 0.96981891\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.12827413 Acc: 0.94659950\n",
      "test Loss: 0.05419826 Acc: 0.96981891\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.13503250 Acc: 0.94156171\n",
      "test Loss: 0.05275054 Acc: 0.96981891\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.12445671 Acc: 0.95012594\n",
      "test Loss: 0.05151151 Acc: 0.97384306\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.13362247 Acc: 0.94458438\n",
      "test Loss: 0.05459574 Acc: 0.96981891\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.12697930 Acc: 0.94659950\n",
      "test Loss: 0.05126933 Acc: 0.97585513\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.14089153 Acc: 0.94055416\n",
      "test Loss: 0.05196328 Acc: 0.97786720\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.13185091 Acc: 0.94710327\n",
      "test Loss: 0.05333807 Acc: 0.96981891\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.12192257 Acc: 0.94962217\n",
      "test Loss: 0.05275653 Acc: 0.96981891\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.13466425 Acc: 0.93853904\n",
      "test Loss: 0.05210298 Acc: 0.97183099\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.11509361 Acc: 0.95314861\n",
      "test Loss: 0.05270164 Acc: 0.97183099\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.13588915 Acc: 0.94710327\n",
      "test Loss: 0.05328286 Acc: 0.97183099\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.12824705 Acc: 0.94911839\n",
      "test Loss: 0.05302219 Acc: 0.96981891\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.13088311 Acc: 0.94710327\n",
      "test Loss: 0.05333828 Acc: 0.96981891\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.12778073 Acc: 0.94911839\n",
      "test Loss: 0.05301350 Acc: 0.96981891\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.13146083 Acc: 0.94609572\n",
      "test Loss: 0.05210828 Acc: 0.97384306\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.12350824 Acc: 0.94760705\n",
      "test Loss: 0.05123632 Acc: 0.97585513\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.13010750 Acc: 0.94962217\n",
      "test Loss: 0.05057805 Acc: 0.97585513\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.12754353 Acc: 0.94811083\n",
      "test Loss: 0.05235631 Acc: 0.96981891\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.14602416 Acc: 0.93702771\n",
      "test Loss: 0.05120831 Acc: 0.97786720\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.11827021 Acc: 0.95314861\n",
      "test Loss: 0.05064440 Acc: 0.97585513\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.12511099 Acc: 0.94861461\n",
      "test Loss: 0.05176864 Acc: 0.97585513\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.13704018 Acc: 0.94105793\n",
      "test Loss: 0.05233341 Acc: 0.97384306\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.13212895 Acc: 0.94307305\n",
      "test Loss: 0.05260066 Acc: 0.97183099\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.12555597 Acc: 0.94609572\n",
      "test Loss: 0.05208784 Acc: 0.97786720\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.12545022 Acc: 0.94408060\n",
      "test Loss: 0.05240950 Acc: 0.97384306\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.13313252 Acc: 0.94609572\n",
      "test Loss: 0.05170873 Acc: 0.97384306\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.13669391 Acc: 0.93853904\n",
      "test Loss: 0.05287632 Acc: 0.97183099\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.13664452 Acc: 0.94307305\n",
      "test Loss: 0.05200447 Acc: 0.97384306\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.13631883 Acc: 0.94458438\n",
      "test Loss: 0.05270077 Acc: 0.96981891\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.12875190 Acc: 0.95012594\n",
      "test Loss: 0.05227684 Acc: 0.97585513\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.12635451 Acc: 0.94408060\n",
      "test Loss: 0.05281039 Acc: 0.96981891\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.12483471 Acc: 0.94811083\n",
      "test Loss: 0.05115952 Acc: 0.97585513\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.12816079 Acc: 0.94559194\n",
      "test Loss: 0.05193166 Acc: 0.97786720\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.14875610 Acc: 0.93652393\n",
      "test Loss: 0.05245948 Acc: 0.97183099\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.12815435 Acc: 0.94256927\n",
      "test Loss: 0.05267435 Acc: 0.96981891\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.12312407 Acc: 0.95062972\n",
      "test Loss: 0.05330153 Acc: 0.96981891\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.12361373 Acc: 0.95163728\n",
      "test Loss: 0.05098570 Acc: 0.97585513\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.11965915 Acc: 0.95012594\n",
      "test Loss: 0.05267077 Acc: 0.97384306\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.14442644 Acc: 0.93652393\n",
      "test Loss: 0.05275041 Acc: 0.97384306\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.12543661 Acc: 0.94357683\n",
      "test Loss: 0.05135225 Acc: 0.97786720\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.13880697 Acc: 0.94156171\n",
      "test Loss: 0.05109311 Acc: 0.97786720\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.11845045 Acc: 0.95062972\n",
      "test Loss: 0.05157380 Acc: 0.97585513\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.13393273 Acc: 0.94307305\n",
      "test Loss: 0.05330902 Acc: 0.96981891\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.12597639 Acc: 0.94710327\n",
      "test Loss: 0.05141829 Acc: 0.97585513\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.12255755 Acc: 0.95214106\n",
      "test Loss: 0.05051344 Acc: 0.97585513\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.11621618 Acc: 0.95566751\n",
      "test Loss: 0.05266414 Acc: 0.97384306\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.12957816 Acc: 0.94659950\n",
      "test Loss: 0.05161561 Acc: 0.97585513\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.15132466 Acc: 0.93047859\n",
      "test Loss: 0.05249922 Acc: 0.97585513\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.12501577 Acc: 0.94307305\n",
      "test Loss: 0.05163661 Acc: 0.97585513\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.12546808 Acc: 0.94156171\n",
      "test Loss: 0.05080805 Acc: 0.97585513\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.13053232 Acc: 0.95012594\n",
      "test Loss: 0.05241230 Acc: 0.97384306\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.12316021 Acc: 0.94609572\n",
      "test Loss: 0.05172742 Acc: 0.97384306\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.12517970 Acc: 0.94659950\n",
      "test Loss: 0.05259531 Acc: 0.97183099\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.13329336 Acc: 0.94408060\n",
      "test Loss: 0.05232411 Acc: 0.97183099\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.12834614 Acc: 0.94559194\n",
      "test Loss: 0.05235611 Acc: 0.97183099\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.12298737 Acc: 0.94962217\n",
      "test Loss: 0.05223550 Acc: 0.97585513\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.13725263 Acc: 0.94357683\n",
      "test Loss: 0.05263694 Acc: 0.96981891\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.13126611 Acc: 0.94609572\n",
      "test Loss: 0.05368761 Acc: 0.97585513\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.12426844 Acc: 0.94962217\n",
      "test Loss: 0.05220956 Acc: 0.97786720\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.12378937 Acc: 0.95062972\n",
      "test Loss: 0.05335316 Acc: 0.96981891\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.13650755 Acc: 0.94357683\n",
      "test Loss: 0.05176298 Acc: 0.97384306\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.15065493 Acc: 0.94256927\n",
      "test Loss: 0.05267140 Acc: 0.97384306\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.14141735 Acc: 0.93551637\n",
      "test Loss: 0.05175349 Acc: 0.97183099\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.13891993 Acc: 0.93853904\n",
      "test Loss: 0.05215745 Acc: 0.97786720\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.14337227 Acc: 0.93904282\n",
      "test Loss: 0.05202818 Acc: 0.97585513\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.12710918 Acc: 0.95062972\n",
      "test Loss: 0.05327819 Acc: 0.96981891\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.13772361 Acc: 0.94458438\n",
      "test Loss: 0.05311541 Acc: 0.96981891\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.12494250 Acc: 0.94811083\n",
      "test Loss: 0.05195626 Acc: 0.97384306\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.13171638 Acc: 0.94005038\n",
      "test Loss: 0.05253488 Acc: 0.97585513\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.13213046 Acc: 0.94811083\n",
      "test Loss: 0.05170376 Acc: 0.97585513\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.13410598 Acc: 0.94659950\n",
      "test Loss: 0.05229094 Acc: 0.97987928\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.13474157 Acc: 0.94408060\n",
      "test Loss: 0.05128456 Acc: 0.97585513\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.14249486 Acc: 0.94105793\n",
      "test Loss: 0.05153212 Acc: 0.97585513\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.12624821 Acc: 0.94710327\n",
      "test Loss: 0.05239042 Acc: 0.97183099\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.12597129 Acc: 0.94962217\n",
      "test Loss: 0.05180772 Acc: 0.97585513\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.13970226 Acc: 0.94307305\n",
      "test Loss: 0.05212964 Acc: 0.97384306\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.12335739 Acc: 0.94760705\n",
      "test Loss: 0.05118371 Acc: 0.97786720\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.13786978 Acc: 0.94256927\n",
      "test Loss: 0.05323599 Acc: 0.97384306\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.13402245 Acc: 0.94156171\n",
      "test Loss: 0.05202885 Acc: 0.97585513\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.10002743 Acc: 0.96171285\n",
      "test Loss: 0.05374713 Acc: 0.96981891\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.14875227 Acc: 0.93652393\n",
      "test Loss: 0.05246110 Acc: 0.97585513\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.13943683 Acc: 0.94408060\n",
      "test Loss: 0.05234104 Acc: 0.97786720\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.14151179 Acc: 0.94508816\n",
      "test Loss: 0.05183993 Acc: 0.97786720\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.13411888 Acc: 0.94559194\n",
      "test Loss: 0.05328113 Acc: 0.97183099\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.13153471 Acc: 0.94559194\n",
      "test Loss: 0.05356052 Acc: 0.97183099\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.13356540 Acc: 0.93551637\n",
      "test Loss: 0.05097570 Acc: 0.97786720\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.14451951 Acc: 0.93400504\n",
      "test Loss: 0.05241014 Acc: 0.97183099\n",
      "\n",
      "Training complete in 92m 42s\n",
      "Best val Acc: 0.97987928 Best val loss: 0.04948572\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19_CT/weights/TResNet_Large_Covid-19.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 200,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
