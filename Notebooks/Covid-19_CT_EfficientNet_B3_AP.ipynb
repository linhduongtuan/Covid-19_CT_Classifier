{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'NonCOVID-19']\n",
      "{'train': 639, 'val': 75}\n",
      "cuda:1\n",
      "{0: 'COVID-19', 1: 'NonCOVID-19'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19/CT/Yang'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "batch_size = 50\n",
    "num_epochs = 300\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['val']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('tf_efficientnet_b3_ap', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14863946\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "                                 #('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 2)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "#lr = lambda x: (((1 + math.cos(x * math.pi / num_epochs)) / 2) ** 1) * 0.9\n",
    "#scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "   \n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            # deep copy the model\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record ACC: {epoch_acc}, previous record acc: {best_acc}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record acc is SAVED: {epoch_acc}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.14583175629377365, Val accuracy: 0.9600000000000001\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.23929566 Acc: 0.91236307\n",
      "val Loss: 0.13402257 Acc: 0.94666667\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.28195097 Acc: 0.90766823\n",
      "val Loss: 0.13282832 Acc: 0.96000000\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.28755622 Acc: 0.89358372\n",
      "val Loss: 0.14266274 Acc: 0.93333333\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.29565552 Acc: 0.89514867\n",
      "val Loss: 0.14174364 Acc: 0.94666667\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.26975619 Acc: 0.90610329\n",
      "val Loss: 0.13773098 Acc: 0.94666667\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.24634505 Acc: 0.91549296\n",
      "val Loss: 0.14831443 Acc: 0.93333333\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.26847312 Acc: 0.90297340\n",
      "val Loss: 0.15406476 Acc: 0.94666667\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.30474936 Acc: 0.89671362\n",
      "val Loss: 0.14640078 Acc: 0.96000000\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.27838287 Acc: 0.92175274\n",
      "val Loss: 0.14792290 Acc: 0.92000000\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.24050500 Acc: 0.89984351\n",
      "val Loss: 0.14184672 Acc: 0.93333333\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.28804902 Acc: 0.90140845\n",
      "val Loss: 0.13278999 Acc: 0.93333333\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.30493164 Acc: 0.88888889\n",
      "val Loss: 0.14779078 Acc: 0.92000000\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.30589523 Acc: 0.89827856\n",
      "val Loss: 0.14336155 Acc: 0.94666667\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.21304600 Acc: 0.92644757\n",
      "val Loss: 0.13686019 Acc: 0.93333333\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.24894316 Acc: 0.89827856\n",
      "val Loss: 0.13983998 Acc: 0.93333333\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.23461799 Acc: 0.90610329\n",
      "val Loss: 0.15662310 Acc: 0.90666667\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.24702751 Acc: 0.90923318\n",
      "val Loss: 0.15775732 Acc: 0.90666667\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.21963453 Acc: 0.93270736\n",
      "val Loss: 0.15133360 Acc: 0.93333333\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.24238408 Acc: 0.91236307\n",
      "val Loss: 0.14450206 Acc: 0.96000000\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.30899711 Acc: 0.89201878\n",
      "val Loss: 0.13627488 Acc: 0.96000000\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.31987750 Acc: 0.91236307\n",
      "val Loss: 0.13833397 Acc: 0.93333333\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.34902158 Acc: 0.89358372\n",
      "val Loss: 0.14290131 Acc: 0.93333333\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.24157958 Acc: 0.90453834\n",
      "val Loss: 0.14371666 Acc: 0.94666667\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.26718770 Acc: 0.89984351\n",
      "val Loss: 0.13651032 Acc: 0.96000000\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.28588482 Acc: 0.90923318\n",
      "val Loss: 0.13825593 Acc: 0.96000000\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.25687155 Acc: 0.90610329\n",
      "val Loss: 0.14373431 Acc: 0.93333333\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.26065927 Acc: 0.90766823\n",
      "val Loss: 0.14742720 Acc: 0.96000000\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.24267087 Acc: 0.91079812\n",
      "val Loss: 0.15243525 Acc: 0.94666667\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.20334618 Acc: 0.92488263\n",
      "val Loss: 0.14356669 Acc: 0.96000000\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.25692447 Acc: 0.90923318\n",
      "val Loss: 0.14755132 Acc: 0.93333333\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.27849990 Acc: 0.89827856\n",
      "val Loss: 0.15210980 Acc: 0.93333333\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.27289016 Acc: 0.90923318\n",
      "val Loss: 0.16177556 Acc: 0.90666667\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.24783979 Acc: 0.91236307\n",
      "val Loss: 0.14832100 Acc: 0.92000000\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.24940019 Acc: 0.91079812\n",
      "val Loss: 0.14978776 Acc: 0.93333333\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.23291373 Acc: 0.91236307\n",
      "val Loss: 0.12598923 Acc: 0.96000000\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.26166623 Acc: 0.90766823\n",
      "val Loss: 0.13460153 Acc: 0.94666667\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.26123462 Acc: 0.91392801\n",
      "val Loss: 0.14616357 Acc: 0.94666667\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.24675016 Acc: 0.91549296\n",
      "val Loss: 0.14485759 Acc: 0.92000000\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.26801520 Acc: 0.90766823\n",
      "val Loss: 0.14594078 Acc: 0.93333333\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.31635752 Acc: 0.88888889\n",
      "val Loss: 0.13617488 Acc: 0.96000000\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.23492905 Acc: 0.91079812\n",
      "val Loss: 0.13356061 Acc: 0.96000000\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.29507987 Acc: 0.90297340\n",
      "val Loss: 0.14314763 Acc: 0.94666667\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.27708768 Acc: 0.90297340\n",
      "val Loss: 0.13894023 Acc: 0.93333333\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.29208167 Acc: 0.89827856\n",
      "val Loss: 0.14802894 Acc: 0.94666667\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.25276345 Acc: 0.91549296\n",
      "val Loss: 0.14264583 Acc: 0.94666667\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.23594000 Acc: 0.92957746\n",
      "val Loss: 0.14680643 Acc: 0.93333333\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.24401527 Acc: 0.90766823\n",
      "val Loss: 0.15805686 Acc: 0.90666667\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.21112431 Acc: 0.91705790\n",
      "val Loss: 0.14120788 Acc: 0.94666667\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.24515840 Acc: 0.90297340\n",
      "val Loss: 0.14643169 Acc: 0.93333333\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.30386225 Acc: 0.89827856\n",
      "val Loss: 0.14758145 Acc: 0.93333333\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.24498410 Acc: 0.91705790\n",
      "val Loss: 0.14717622 Acc: 0.94666667\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.27726956 Acc: 0.89827856\n",
      "val Loss: 0.15182617 Acc: 0.93333333\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.25014475 Acc: 0.90610329\n",
      "val Loss: 0.14264872 Acc: 0.93333333\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.22461262 Acc: 0.92644757\n",
      "val Loss: 0.14401876 Acc: 0.93333333\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.19609171 Acc: 0.92644757\n",
      "val Loss: 0.14803743 Acc: 0.93333333\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.30843474 Acc: 0.88575900\n",
      "val Loss: 0.14296452 Acc: 0.96000000\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.21264711 Acc: 0.92801252\n",
      "val Loss: 0.13905750 Acc: 0.93333333\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.23777370 Acc: 0.91705790\n",
      "val Loss: 0.15000898 Acc: 0.90666667\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.23597567 Acc: 0.91705790\n",
      "val Loss: 0.14132523 Acc: 0.96000000\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.25902512 Acc: 0.90923318\n",
      "val Loss: 0.13517205 Acc: 0.93333333\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.27273833 Acc: 0.89514867\n",
      "val Loss: 0.14824717 Acc: 0.93333333\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.25735830 Acc: 0.90766823\n",
      "val Loss: 0.13906503 Acc: 0.94666667\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.23432836 Acc: 0.91079812\n",
      "val Loss: 0.13208539 Acc: 0.96000000\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.24094846 Acc: 0.91236307\n",
      "val Loss: 0.14744573 Acc: 0.96000000\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.24272181 Acc: 0.92018779\n",
      "val Loss: 0.14851604 Acc: 0.94666667\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.23346541 Acc: 0.90453834\n",
      "val Loss: 0.14802827 Acc: 0.93333333\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.30246928 Acc: 0.89201878\n",
      "val Loss: 0.14168099 Acc: 0.93333333\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.23572995 Acc: 0.92175274\n",
      "val Loss: 0.13973609 Acc: 0.93333333\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.25048378 Acc: 0.89671362\n",
      "val Loss: 0.13888525 Acc: 0.92000000\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.27112658 Acc: 0.91705790\n",
      "val Loss: 0.13910136 Acc: 0.96000000\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.31098859 Acc: 0.89671362\n",
      "val Loss: 0.14143694 Acc: 0.96000000\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.28039651 Acc: 0.90766823\n",
      "val Loss: 0.14349271 Acc: 0.94666667\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.27893111 Acc: 0.89358372\n",
      "val Loss: 0.14306376 Acc: 0.94666667\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.25199205 Acc: 0.90610329\n",
      "val Loss: 0.15250302 Acc: 0.93333333\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.28615814 Acc: 0.89671362\n",
      "val Loss: 0.14966890 Acc: 0.92000000\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.27919793 Acc: 0.90453834\n",
      "val Loss: 0.13385882 Acc: 0.94666667\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.25613879 Acc: 0.90766823\n",
      "val Loss: 0.13769608 Acc: 0.96000000\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.24608169 Acc: 0.90610329\n",
      "val Loss: 0.13764758 Acc: 0.96000000\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.25870495 Acc: 0.89671362\n",
      "val Loss: 0.13965718 Acc: 0.93333333\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.23390999 Acc: 0.93114241\n",
      "val Loss: 0.13550022 Acc: 0.96000000\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.25764231 Acc: 0.92175274\n",
      "val Loss: 0.14785277 Acc: 0.93333333\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.25753500 Acc: 0.91236307\n",
      "val Loss: 0.14253781 Acc: 0.93333333\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.28377223 Acc: 0.90923318\n",
      "val Loss: 0.13964678 Acc: 0.94666667\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.29072213 Acc: 0.91705790\n",
      "val Loss: 0.15150379 Acc: 0.93333333\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.29078965 Acc: 0.89827856\n",
      "val Loss: 0.15382763 Acc: 0.90666667\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.27267122 Acc: 0.90297340\n",
      "val Loss: 0.13825409 Acc: 0.94666667\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.22699398 Acc: 0.91862285\n",
      "val Loss: 0.13963041 Acc: 0.94666667\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.24805589 Acc: 0.90923318\n",
      "val Loss: 0.14275093 Acc: 0.94666667\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.27175098 Acc: 0.90766823\n",
      "val Loss: 0.14163952 Acc: 0.93333333\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.28119650 Acc: 0.89358372\n",
      "val Loss: 0.15104876 Acc: 0.92000000\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.24057911 Acc: 0.91079812\n",
      "val Loss: 0.15440857 Acc: 0.90666667\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.23843848 Acc: 0.92331768\n",
      "val Loss: 0.14832760 Acc: 0.93333333\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.25498342 Acc: 0.91705790\n",
      "val Loss: 0.13687354 Acc: 0.96000000\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.24030083 Acc: 0.92331768\n",
      "val Loss: 0.13519086 Acc: 0.96000000\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.18809365 Acc: 0.93896714\n",
      "val Loss: 0.13813818 Acc: 0.94666667\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.25387965 Acc: 0.90140845\n",
      "val Loss: 0.14574274 Acc: 0.93333333\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.24499282 Acc: 0.91549296\n",
      "val Loss: 0.13932946 Acc: 0.96000000\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.27082768 Acc: 0.92175274\n",
      "val Loss: 0.13674811 Acc: 0.96000000\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.26627675 Acc: 0.89358372\n",
      "val Loss: 0.14426721 Acc: 0.93333333\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.29318924 Acc: 0.89984351\n",
      "val Loss: 0.13231804 Acc: 0.97333333\n",
      "New best model found!\n",
      "New record ACC: 0.9733333333333334, previous record acc: 0.9600000000000001\n",
      "New record acc is SAVED: 0.9733333333333334\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.24918651 Acc: 0.92175274\n",
      "val Loss: 0.13619158 Acc: 0.96000000\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.25629439 Acc: 0.91705790\n",
      "val Loss: 0.13542457 Acc: 0.93333333\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.26550221 Acc: 0.91392801\n",
      "val Loss: 0.14356356 Acc: 0.92000000\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.24573827 Acc: 0.89984351\n",
      "val Loss: 0.13513173 Acc: 0.93333333\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.24722560 Acc: 0.91236307\n",
      "val Loss: 0.14706500 Acc: 0.93333333\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.23019975 Acc: 0.92644757\n",
      "val Loss: 0.13644230 Acc: 0.96000000\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.26499725 Acc: 0.90453834\n",
      "val Loss: 0.14989205 Acc: 0.92000000\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.24996696 Acc: 0.90453834\n",
      "val Loss: 0.14294781 Acc: 0.93333333\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.23674975 Acc: 0.92175274\n",
      "val Loss: 0.13483187 Acc: 0.94666667\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.24138293 Acc: 0.91705790\n",
      "val Loss: 0.13402176 Acc: 0.96000000\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.27002060 Acc: 0.90923318\n",
      "val Loss: 0.14645862 Acc: 0.96000000\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.27037230 Acc: 0.90297340\n",
      "val Loss: 0.13729555 Acc: 0.96000000\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.29290851 Acc: 0.89827856\n",
      "val Loss: 0.15816006 Acc: 0.94666667\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.26679818 Acc: 0.90923318\n",
      "val Loss: 0.14840880 Acc: 0.92000000\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.23471912 Acc: 0.90610329\n",
      "val Loss: 0.14748591 Acc: 0.94666667\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.28922514 Acc: 0.90297340\n",
      "val Loss: 0.14176365 Acc: 0.94666667\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.24426784 Acc: 0.92331768\n",
      "val Loss: 0.14061542 Acc: 0.93333333\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.25237413 Acc: 0.90610329\n",
      "val Loss: 0.14213139 Acc: 0.93333333\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.20015013 Acc: 0.93270736\n",
      "val Loss: 0.14872287 Acc: 0.93333333\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.25594841 Acc: 0.91549296\n",
      "val Loss: 0.15054774 Acc: 0.93333333\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.28813601 Acc: 0.91862285\n",
      "val Loss: 0.14497497 Acc: 0.93333333\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.28088369 Acc: 0.90297340\n",
      "val Loss: 0.14988769 Acc: 0.93333333\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.25473929 Acc: 0.91236307\n",
      "val Loss: 0.13935969 Acc: 0.96000000\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.29178962 Acc: 0.89514867\n",
      "val Loss: 0.14216992 Acc: 0.93333333\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.24242746 Acc: 0.91079812\n",
      "val Loss: 0.14285522 Acc: 0.96000000\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.28042386 Acc: 0.90297340\n",
      "val Loss: 0.14123417 Acc: 0.94666667\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.28238047 Acc: 0.90766823\n",
      "val Loss: 0.14736948 Acc: 0.93333333\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.27213264 Acc: 0.90610329\n",
      "val Loss: 0.14614094 Acc: 0.93333333\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.27346176 Acc: 0.91549296\n",
      "val Loss: 0.14521876 Acc: 0.92000000\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.25562313 Acc: 0.91236307\n",
      "val Loss: 0.14807943 Acc: 0.92000000\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.24091587 Acc: 0.91392801\n",
      "val Loss: 0.14521522 Acc: 0.96000000\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.21806996 Acc: 0.92801252\n",
      "val Loss: 0.14206129 Acc: 0.96000000\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.24112873 Acc: 0.91549296\n",
      "val Loss: 0.14401345 Acc: 0.96000000\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.28922685 Acc: 0.90453834\n",
      "val Loss: 0.14961979 Acc: 0.92000000\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.23591295 Acc: 0.92018779\n",
      "val Loss: 0.14995728 Acc: 0.93333333\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.24897392 Acc: 0.92488263\n",
      "val Loss: 0.13496493 Acc: 0.96000000\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.22250998 Acc: 0.93270736\n",
      "val Loss: 0.13508771 Acc: 0.96000000\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.21135812 Acc: 0.92957746\n",
      "val Loss: 0.13558511 Acc: 0.96000000\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.29496176 Acc: 0.91236307\n",
      "val Loss: 0.13912929 Acc: 0.93333333\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.23945141 Acc: 0.90766823\n",
      "val Loss: 0.14572543 Acc: 0.92000000\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.23641423 Acc: 0.91862285\n",
      "val Loss: 0.14180148 Acc: 0.96000000\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.21620216 Acc: 0.92331768\n",
      "val Loss: 0.14320756 Acc: 0.94666667\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.29473828 Acc: 0.89358372\n",
      "val Loss: 0.15400998 Acc: 0.93333333\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.27549448 Acc: 0.90140845\n",
      "val Loss: 0.15224995 Acc: 0.92000000\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.26381576 Acc: 0.91079812\n",
      "val Loss: 0.14627421 Acc: 0.92000000\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.22110978 Acc: 0.91705790\n",
      "val Loss: 0.14350502 Acc: 0.94666667\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.24025476 Acc: 0.91862285\n",
      "val Loss: 0.13413330 Acc: 0.96000000\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.24262642 Acc: 0.92175274\n",
      "val Loss: 0.13733970 Acc: 0.94666667\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.25677149 Acc: 0.91862285\n",
      "val Loss: 0.14295823 Acc: 0.93333333\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.27107697 Acc: 0.92175274\n",
      "val Loss: 0.13891117 Acc: 0.96000000\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.23625881 Acc: 0.92801252\n",
      "val Loss: 0.14559698 Acc: 0.93333333\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.25162220 Acc: 0.90923318\n",
      "val Loss: 0.13672896 Acc: 0.96000000\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.25747740 Acc: 0.90297340\n",
      "val Loss: 0.14296632 Acc: 0.96000000\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.21966756 Acc: 0.93583725\n",
      "val Loss: 0.14164789 Acc: 0.96000000\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.22899183 Acc: 0.91549296\n",
      "val Loss: 0.13381922 Acc: 0.96000000\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.27777174 Acc: 0.91549296\n",
      "val Loss: 0.15003160 Acc: 0.90666667\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.23812585 Acc: 0.91705790\n",
      "val Loss: 0.14642887 Acc: 0.92000000\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.28233653 Acc: 0.90297340\n",
      "val Loss: 0.13909389 Acc: 0.94666667\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.25159776 Acc: 0.90610329\n",
      "val Loss: 0.13774915 Acc: 0.97333333\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.26833410 Acc: 0.91236307\n",
      "val Loss: 0.14588449 Acc: 0.96000000\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.26256023 Acc: 0.89514867\n",
      "val Loss: 0.14874356 Acc: 0.92000000\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.23101133 Acc: 0.91705790\n",
      "val Loss: 0.14232797 Acc: 0.94666667\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.25805316 Acc: 0.90140845\n",
      "val Loss: 0.13996544 Acc: 0.93333333\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.30477179 Acc: 0.89358372\n",
      "val Loss: 0.14549512 Acc: 0.96000000\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.25859259 Acc: 0.92331768\n",
      "val Loss: 0.14248486 Acc: 0.94666667\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.23814343 Acc: 0.92175274\n",
      "val Loss: 0.14034667 Acc: 0.94666667\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.30941950 Acc: 0.89514867\n",
      "val Loss: 0.13586623 Acc: 0.96000000\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.25236284 Acc: 0.89671362\n",
      "val Loss: 0.14004916 Acc: 0.93333333\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.22567395 Acc: 0.92018779\n",
      "val Loss: 0.14023034 Acc: 0.92000000\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.25752857 Acc: 0.92018779\n",
      "val Loss: 0.14142756 Acc: 0.92000000\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.27215243 Acc: 0.91079812\n",
      "val Loss: 0.14204248 Acc: 0.92000000\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.23505791 Acc: 0.92331768\n",
      "val Loss: 0.13698875 Acc: 0.94666667\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.24522991 Acc: 0.91549296\n",
      "val Loss: 0.14510674 Acc: 0.96000000\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.25558071 Acc: 0.89827856\n",
      "val Loss: 0.14335737 Acc: 0.94666667\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.28313034 Acc: 0.90297340\n",
      "val Loss: 0.14015607 Acc: 0.96000000\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.24264722 Acc: 0.91862285\n",
      "val Loss: 0.14548521 Acc: 0.93333333\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.24399609 Acc: 0.91862285\n",
      "val Loss: 0.13726358 Acc: 0.96000000\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.29379643 Acc: 0.89827856\n",
      "val Loss: 0.14876392 Acc: 0.90666667\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.28286310 Acc: 0.91549296\n",
      "val Loss: 0.14717115 Acc: 0.93333333\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.25750433 Acc: 0.91862285\n",
      "val Loss: 0.14559281 Acc: 0.93333333\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.27005689 Acc: 0.90766823\n",
      "val Loss: 0.14067761 Acc: 0.93333333\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.24334928 Acc: 0.90923318\n",
      "val Loss: 0.13994720 Acc: 0.96000000\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.25833456 Acc: 0.91549296\n",
      "val Loss: 0.15940646 Acc: 0.92000000\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.25741358 Acc: 0.90923318\n",
      "val Loss: 0.13744968 Acc: 0.96000000\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.23157389 Acc: 0.91236307\n",
      "val Loss: 0.13903450 Acc: 0.97333333\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.28091699 Acc: 0.90453834\n",
      "val Loss: 0.14731005 Acc: 0.93333333\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.20951709 Acc: 0.92331768\n",
      "val Loss: 0.15303813 Acc: 0.93333333\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.27324369 Acc: 0.90453834\n",
      "val Loss: 0.15831445 Acc: 0.94666667\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.26510586 Acc: 0.89358372\n",
      "val Loss: 0.14983990 Acc: 0.94666667\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.25119999 Acc: 0.91392801\n",
      "val Loss: 0.14212378 Acc: 0.94666667\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.25503330 Acc: 0.90610329\n",
      "val Loss: 0.13791827 Acc: 0.96000000\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.27336116 Acc: 0.91392801\n",
      "val Loss: 0.13600435 Acc: 0.94666667\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.25384218 Acc: 0.91392801\n",
      "val Loss: 0.14076425 Acc: 0.93333333\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.24836730 Acc: 0.91862285\n",
      "val Loss: 0.13893816 Acc: 0.93333333\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.25612424 Acc: 0.91392801\n",
      "val Loss: 0.14614092 Acc: 0.90666667\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.24166106 Acc: 0.90766823\n",
      "val Loss: 0.14291520 Acc: 0.94666667\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.22248609 Acc: 0.90766823\n",
      "val Loss: 0.14387007 Acc: 0.96000000\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.26832761 Acc: 0.90610329\n",
      "val Loss: 0.15340526 Acc: 0.92000000\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.23468959 Acc: 0.91079812\n",
      "val Loss: 0.15015937 Acc: 0.94666667\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.26476340 Acc: 0.90610329\n",
      "val Loss: 0.15047397 Acc: 0.94666667\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.26131746 Acc: 0.91079812\n",
      "val Loss: 0.14334415 Acc: 0.96000000\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.28983020 Acc: 0.91236307\n",
      "val Loss: 0.15017936 Acc: 0.92000000\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.24310889 Acc: 0.91392801\n",
      "val Loss: 0.13658229 Acc: 0.94666667\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.31950628 Acc: 0.89201878\n",
      "val Loss: 0.14000599 Acc: 0.93333333\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.29482799 Acc: 0.90297340\n",
      "val Loss: 0.14010939 Acc: 0.96000000\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.27847297 Acc: 0.89045383\n",
      "val Loss: 0.14151863 Acc: 0.96000000\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.25227893 Acc: 0.90923318\n",
      "val Loss: 0.14378423 Acc: 0.93333333\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.28276794 Acc: 0.89984351\n",
      "val Loss: 0.14846470 Acc: 0.93333333\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.24128751 Acc: 0.91549296\n",
      "val Loss: 0.15385175 Acc: 0.96000000\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.25210187 Acc: 0.93270736\n",
      "val Loss: 0.14542187 Acc: 0.96000000\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.21707294 Acc: 0.92957746\n",
      "val Loss: 0.14399220 Acc: 0.94666667\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.26939711 Acc: 0.90923318\n",
      "val Loss: 0.14876143 Acc: 0.94666667\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.26592764 Acc: 0.91705790\n",
      "val Loss: 0.13372668 Acc: 0.96000000\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.26014914 Acc: 0.91705790\n",
      "val Loss: 0.14559624 Acc: 0.93333333\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.27370698 Acc: 0.90610329\n",
      "val Loss: 0.14589551 Acc: 0.94666667\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.29545740 Acc: 0.89671362\n",
      "val Loss: 0.15162429 Acc: 0.92000000\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.28901796 Acc: 0.91549296\n",
      "val Loss: 0.13347321 Acc: 0.94666667\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.21998746 Acc: 0.91549296\n",
      "val Loss: 0.14185880 Acc: 0.96000000\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.28428247 Acc: 0.90453834\n",
      "val Loss: 0.14740297 Acc: 0.96000000\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.27009405 Acc: 0.90610329\n",
      "val Loss: 0.14409028 Acc: 0.96000000\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.23816165 Acc: 0.90140845\n",
      "val Loss: 0.14574632 Acc: 0.92000000\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.25308636 Acc: 0.91236307\n",
      "val Loss: 0.14095909 Acc: 0.94666667\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.25383805 Acc: 0.91549296\n",
      "val Loss: 0.13844971 Acc: 0.93333333\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.25242624 Acc: 0.90297340\n",
      "val Loss: 0.13563783 Acc: 0.94666667\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.27156902 Acc: 0.90297340\n",
      "val Loss: 0.14248731 Acc: 0.90666667\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.26331391 Acc: 0.90297340\n",
      "val Loss: 0.14666957 Acc: 0.93333333\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.25221148 Acc: 0.90453834\n",
      "val Loss: 0.14944035 Acc: 0.94666667\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.22604988 Acc: 0.91549296\n",
      "val Loss: 0.13944372 Acc: 0.94666667\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.25422049 Acc: 0.90766823\n",
      "val Loss: 0.13985371 Acc: 0.93333333\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.23657336 Acc: 0.92018779\n",
      "val Loss: 0.14323769 Acc: 0.93333333\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.26486321 Acc: 0.89045383\n",
      "val Loss: 0.13624375 Acc: 0.96000000\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.25229031 Acc: 0.92644757\n",
      "val Loss: 0.13837589 Acc: 0.96000000\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.20103681 Acc: 0.92331768\n",
      "val Loss: 0.13863546 Acc: 0.93333333\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.21781791 Acc: 0.92488263\n",
      "val Loss: 0.13812306 Acc: 0.96000000\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.27752372 Acc: 0.88732394\n",
      "val Loss: 0.14152039 Acc: 0.93333333\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.22443515 Acc: 0.92331768\n",
      "val Loss: 0.14728142 Acc: 0.93333333\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.28756757 Acc: 0.91236307\n",
      "val Loss: 0.15710034 Acc: 0.90666667\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.25685379 Acc: 0.92018779\n",
      "val Loss: 0.15367766 Acc: 0.93333333\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.29198716 Acc: 0.90766823\n",
      "val Loss: 0.13788057 Acc: 0.96000000\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.23442582 Acc: 0.91705790\n",
      "val Loss: 0.15094262 Acc: 0.92000000\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.28548037 Acc: 0.90297340\n",
      "val Loss: 0.14809720 Acc: 0.92000000\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.29427582 Acc: 0.91079812\n",
      "val Loss: 0.13989048 Acc: 0.93333333\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.25845112 Acc: 0.91549296\n",
      "val Loss: 0.15108445 Acc: 0.92000000\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.24868630 Acc: 0.91392801\n",
      "val Loss: 0.14806433 Acc: 0.96000000\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.26733536 Acc: 0.90766823\n",
      "val Loss: 0.14246840 Acc: 0.94666667\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.30701975 Acc: 0.91236307\n",
      "val Loss: 0.13890044 Acc: 0.96000000\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.25865839 Acc: 0.90297340\n",
      "val Loss: 0.13583500 Acc: 0.96000000\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.27311588 Acc: 0.91549296\n",
      "val Loss: 0.13952828 Acc: 0.93333333\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.23545500 Acc: 0.91392801\n",
      "val Loss: 0.14187019 Acc: 0.94666667\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.30946679 Acc: 0.90453834\n",
      "val Loss: 0.14132119 Acc: 0.94666667\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.25323596 Acc: 0.91236307\n",
      "val Loss: 0.14684470 Acc: 0.92000000\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.25790453 Acc: 0.90297340\n",
      "val Loss: 0.14738871 Acc: 0.92000000\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.26205055 Acc: 0.92018779\n",
      "val Loss: 0.13846149 Acc: 0.94666667\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.27742220 Acc: 0.90766823\n",
      "val Loss: 0.13679098 Acc: 0.96000000\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.26335152 Acc: 0.91862285\n",
      "val Loss: 0.13274393 Acc: 0.96000000\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.25980375 Acc: 0.91392801\n",
      "val Loss: 0.13285035 Acc: 0.93333333\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.21725167 Acc: 0.90766823\n",
      "val Loss: 0.14041257 Acc: 0.94666667\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.29790923 Acc: 0.90453834\n",
      "val Loss: 0.15141203 Acc: 0.93333333\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.24522860 Acc: 0.90453834\n",
      "val Loss: 0.13969627 Acc: 0.94666667\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.28562213 Acc: 0.90923318\n",
      "val Loss: 0.14253051 Acc: 0.94666667\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.29023323 Acc: 0.89045383\n",
      "val Loss: 0.14825031 Acc: 0.94666667\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.20819183 Acc: 0.92331768\n",
      "val Loss: 0.14608402 Acc: 0.92000000\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.24612047 Acc: 0.92331768\n",
      "val Loss: 0.13680341 Acc: 0.96000000\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.25049661 Acc: 0.91236307\n",
      "val Loss: 0.14747688 Acc: 0.94666667\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.24833788 Acc: 0.91236307\n",
      "val Loss: 0.14262205 Acc: 0.94666667\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.19674600 Acc: 0.93270736\n",
      "val Loss: 0.14503311 Acc: 0.93333333\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.27022172 Acc: 0.92018779\n",
      "val Loss: 0.14526965 Acc: 0.94666667\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.25119699 Acc: 0.90610329\n",
      "val Loss: 0.14547610 Acc: 0.96000000\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.22631567 Acc: 0.91236307\n",
      "val Loss: 0.14548623 Acc: 0.96000000\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.24939453 Acc: 0.89358372\n",
      "val Loss: 0.14436542 Acc: 0.93333333\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.28184000 Acc: 0.92018779\n",
      "val Loss: 0.14746174 Acc: 0.93333333\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.27777904 Acc: 0.90610329\n",
      "val Loss: 0.14653641 Acc: 0.93333333\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.24523277 Acc: 0.91549296\n",
      "val Loss: 0.13208522 Acc: 0.96000000\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.20346722 Acc: 0.92175274\n",
      "val Loss: 0.13281378 Acc: 0.96000000\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.26199598 Acc: 0.90610329\n",
      "val Loss: 0.14055978 Acc: 0.97333333\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.24204317 Acc: 0.91392801\n",
      "val Loss: 0.14457172 Acc: 0.94666667\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.25968506 Acc: 0.90453834\n",
      "val Loss: 0.14473485 Acc: 0.96000000\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.28847086 Acc: 0.89671362\n",
      "val Loss: 0.13964082 Acc: 0.93333333\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.27335092 Acc: 0.90453834\n",
      "val Loss: 0.14184375 Acc: 0.93333333\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.28119574 Acc: 0.89984351\n",
      "val Loss: 0.13696045 Acc: 0.94666667\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.27845598 Acc: 0.91392801\n",
      "val Loss: 0.13975435 Acc: 0.94666667\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.23119135 Acc: 0.91705790\n",
      "val Loss: 0.13842475 Acc: 0.93333333\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.24597404 Acc: 0.92801252\n",
      "val Loss: 0.15407884 Acc: 0.90666667\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.24006852 Acc: 0.90923318\n",
      "val Loss: 0.14495699 Acc: 0.93333333\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.26870698 Acc: 0.90610329\n",
      "val Loss: 0.13928868 Acc: 0.93333333\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.27717473 Acc: 0.89045383\n",
      "val Loss: 0.13501489 Acc: 0.94666667\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.28925458 Acc: 0.91392801\n",
      "val Loss: 0.13432420 Acc: 0.96000000\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.23947191 Acc: 0.90453834\n",
      "val Loss: 0.15070051 Acc: 0.94666667\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.25692581 Acc: 0.90297340\n",
      "val Loss: 0.15240249 Acc: 0.92000000\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.28511673 Acc: 0.90453834\n",
      "val Loss: 0.14163493 Acc: 0.94666667\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.28406231 Acc: 0.89514867\n",
      "val Loss: 0.13828872 Acc: 0.96000000\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.24081289 Acc: 0.91392801\n",
      "val Loss: 0.14468491 Acc: 0.96000000\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.25569182 Acc: 0.90610329\n",
      "val Loss: 0.14920678 Acc: 0.93333333\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.28996972 Acc: 0.90766823\n",
      "val Loss: 0.13467665 Acc: 0.96000000\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.26413308 Acc: 0.91079812\n",
      "val Loss: 0.13599734 Acc: 0.96000000\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.26676314 Acc: 0.89827856\n",
      "val Loss: 0.13720688 Acc: 0.96000000\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.24760394 Acc: 0.92018779\n",
      "val Loss: 0.13482856 Acc: 0.96000000\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.25298380 Acc: 0.90453834\n",
      "val Loss: 0.13484648 Acc: 0.96000000\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.27891169 Acc: 0.90297340\n",
      "val Loss: 0.14291487 Acc: 0.93333333\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.24747403 Acc: 0.90766823\n",
      "val Loss: 0.14603720 Acc: 0.93333333\n",
      "\n",
      "Training complete in 43m 29s\n",
      "Best val Acc: 0.97333333 Best val loss: 0.13231804\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights_CT/EfficientNet_B3_AP_dataset_Yang.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = num_epochs,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
