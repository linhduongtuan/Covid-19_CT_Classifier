{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'Normal', 'Pneumonia']\n",
      "{'train': 17905, 'test': 13511}\n",
      "cuda:1\n",
      "{0: 'Covid-19', 1: 'Normal', 2: 'Pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 54\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('tf_efficientnet_b3_ap', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the model is: 14864075\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([\n",
    "                                 #('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "                                 ('fc1', nn.Linear(2048, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint loaded\n",
      "Val loss: 0.12029734140230437, Val accuracy: 0.9587743320257568\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.23603043 Acc: 0.91298520\n",
      "test Loss: 0.12289173 Acc: 0.95462956\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.23068622 Acc: 0.91304105\n",
      "test Loss: 0.11276348 Acc: 0.95988454\n",
      "New best model found!\n",
      "New record loss: 0.11276348417880012, previous record loss: 0.12029734140230437\n",
      "New record loss is SAVED: 0.11276348417880012\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.22718659 Acc: 0.91616867\n",
      "test Loss: 0.12207807 Acc: 0.95670195\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.22023688 Acc: 0.91845853\n",
      "test Loss: 0.10934677 Acc: 0.96121679\n",
      "New best model found!\n",
      "New record loss: 0.10934676674032853, previous record loss: 0.11276348417880012\n",
      "New record loss is SAVED: 0.10934676674032853\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.21691981 Acc: 0.91985479\n",
      "test Loss: 0.10480965 Acc: 0.96158686\n",
      "New best model found!\n",
      "New record loss: 0.10480965299505762, previous record loss: 0.10934676674032853\n",
      "New record loss is SAVED: 0.10480965299505762\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.20961038 Acc: 0.92281486\n",
      "test Loss: 0.12555095 Acc: 0.95662793\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.20918102 Acc: 0.92253560\n",
      "test Loss: 0.10424975 Acc: 0.96314114\n",
      "New best model found!\n",
      "New record loss: 0.10424975078972336, previous record loss: 0.10480965299505762\n",
      "New record loss is SAVED: 0.10424975078972336\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.20131207 Acc: 0.92599832\n",
      "test Loss: 0.10023812 Acc: 0.96550958\n",
      "New best model found!\n",
      "New record loss: 0.10023811548268907, previous record loss: 0.10424975078972336\n",
      "New record loss is SAVED: 0.10023811548268907\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.19857698 Acc: 0.92672438\n",
      "test Loss: 0.09453345 Acc: 0.96684183\n",
      "New best model found!\n",
      "New record loss: 0.0945334475148056, previous record loss: 0.10023811548268907\n",
      "New record loss is SAVED: 0.0945334475148056\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.19790730 Acc: 0.92594247\n",
      "test Loss: 0.08751957 Acc: 0.96965436\n",
      "New best model found!\n",
      "New record loss: 0.08751957006402324, previous record loss: 0.0945334475148056\n",
      "New record loss is SAVED: 0.08751957006402324\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.19639942 Acc: 0.92772968\n",
      "test Loss: 0.09483019 Acc: 0.96610169\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.19686602 Acc: 0.92772968\n",
      "test Loss: 0.09004907 Acc: 0.96743394\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.19498976 Acc: 0.92745043\n",
      "test Loss: 0.09113282 Acc: 0.96876619\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.19020711 Acc: 0.92985200\n",
      "test Loss: 0.08641275 Acc: 0.96943231\n",
      "New best model found!\n",
      "New record loss: 0.08641275311243604, previous record loss: 0.08751957006402324\n",
      "New record loss is SAVED: 0.08641275311243604\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.18303891 Acc: 0.93258866\n",
      "test Loss: 0.08105499 Acc: 0.97180075\n",
      "New best model found!\n",
      "New record loss: 0.08105499042061165, previous record loss: 0.08641275311243604\n",
      "New record loss is SAVED: 0.08105499042061165\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.18289436 Acc: 0.93113655\n",
      "test Loss: 0.07816382 Acc: 0.97209681\n",
      "New best model found!\n",
      "New record loss: 0.07816381514045949, previous record loss: 0.08105499042061165\n",
      "New record loss is SAVED: 0.07816381514045949\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.17638643 Acc: 0.93387322\n",
      "test Loss: 0.07922830 Acc: 0.97106062\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.17608785 Acc: 0.93504608\n",
      "test Loss: 0.07530713 Acc: 0.97365110\n",
      "New best model found!\n",
      "New record loss: 0.07530713206793042, previous record loss: 0.07816381514045949\n",
      "New record loss is SAVED: 0.07530713206793042\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.17558840 Acc: 0.93711254\n",
      "test Loss: 0.08053222 Acc: 0.97128266\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.17128354 Acc: 0.93878805\n",
      "test Loss: 0.06984128 Acc: 0.97483532\n",
      "New best model found!\n",
      "New record loss: 0.06984127725710762, previous record loss: 0.07530713206793042\n",
      "New record loss is SAVED: 0.06984127725710762\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.16894750 Acc: 0.93722424\n",
      "test Loss: 0.07385689 Acc: 0.97224484\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.15822755 Acc: 0.94118961\n",
      "test Loss: 0.06101636 Acc: 0.97809193\n",
      "New best model found!\n",
      "New record loss: 0.061016357495869894, previous record loss: 0.06984127725710762\n",
      "New record loss is SAVED: 0.061016357495869894\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.15007191 Acc: 0.94470818\n",
      "test Loss: 0.05940677 Acc: 0.97861002\n",
      "New best model found!\n",
      "New record loss: 0.05940676647454255, previous record loss: 0.061016357495869894\n",
      "New record loss is SAVED: 0.05940676647454255\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.14806092 Acc: 0.94325607\n",
      "test Loss: 0.05852912 Acc: 0.97949819\n",
      "New best model found!\n",
      "New record loss: 0.05852912410891883, previous record loss: 0.05940676647454255\n",
      "New record loss is SAVED: 0.05852912410891883\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.14677460 Acc: 0.94694219\n",
      "test Loss: 0.05753462 Acc: 0.97994227\n",
      "New best model found!\n",
      "New record loss: 0.05753461575046944, previous record loss: 0.05852912410891883\n",
      "New record loss is SAVED: 0.05753461575046944\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.14212185 Acc: 0.94571349\n",
      "test Loss: 0.05596123 Acc: 0.97979424\n",
      "New best model found!\n",
      "New record loss: 0.055961229195858976, previous record loss: 0.05753461575046944\n",
      "New record loss is SAVED: 0.055961229195858976\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.14372796 Acc: 0.94710975\n",
      "test Loss: 0.05584161 Acc: 0.97994227\n",
      "New best model found!\n",
      "New record loss: 0.05584160961173781, previous record loss: 0.055961229195858976\n",
      "New record loss is SAVED: 0.05584160961173781\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.14858738 Acc: 0.94403798\n",
      "test Loss: 0.05598299 Acc: 0.97957220\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.14108071 Acc: 0.94839430\n",
      "test Loss: 0.05307118 Acc: 0.98171860\n",
      "New best model found!\n",
      "New record loss: 0.05307117973112194, previous record loss: 0.05584160961173781\n",
      "New record loss is SAVED: 0.05307117973112194\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.14355398 Acc: 0.94627199\n",
      "test Loss: 0.05200729 Acc: 0.98223670\n",
      "New best model found!\n",
      "New record loss: 0.05200729471153038, previous record loss: 0.05307117973112194\n",
      "New record loss is SAVED: 0.05200729471153038\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.14442108 Acc: 0.94521083\n",
      "test Loss: 0.05114995 Acc: 0.98275479\n",
      "New best model found!\n",
      "New record loss: 0.051149950668311704, previous record loss: 0.05200729471153038\n",
      "New record loss is SAVED: 0.051149950668311704\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.14019414 Acc: 0.94688634\n",
      "test Loss: 0.04961361 Acc: 0.98312486\n",
      "New best model found!\n",
      "New record loss: 0.04961361258976086, previous record loss: 0.051149950668311704\n",
      "New record loss is SAVED: 0.04961361258976086\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.13533066 Acc: 0.94979056\n",
      "test Loss: 0.05018375 Acc: 0.98268078\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.13554042 Acc: 0.95029321\n",
      "test Loss: 0.04857840 Acc: 0.98260676\n",
      "New best model found!\n",
      "New record loss: 0.04857839595483168, previous record loss: 0.04961361258976086\n",
      "New record loss is SAVED: 0.04857839595483168\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.13274908 Acc: 0.95152192\n",
      "test Loss: 0.04712022 Acc: 0.98430908\n",
      "New best model found!\n",
      "New record loss: 0.04712021577863149, previous record loss: 0.04857839595483168\n",
      "New record loss is SAVED: 0.04712021577863149\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.13795304 Acc: 0.94906451\n",
      "test Loss: 0.04679770 Acc: 0.98312486\n",
      "New best model found!\n",
      "New record loss: 0.046797697391049706, previous record loss: 0.04712021577863149\n",
      "New record loss is SAVED: 0.046797697391049706\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.13732108 Acc: 0.95006981\n",
      "test Loss: 0.04672365 Acc: 0.98438310\n",
      "New best model found!\n",
      "New record loss: 0.046723648448743604, previous record loss: 0.046797697391049706\n",
      "New record loss is SAVED: 0.046723648448743604\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.13692709 Acc: 0.94867355\n",
      "test Loss: 0.04507691 Acc: 0.98497521\n",
      "New best model found!\n",
      "New record loss: 0.045076908787307, previous record loss: 0.046723648448743604\n",
      "New record loss is SAVED: 0.045076908787307\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.13497383 Acc: 0.95079587\n",
      "test Loss: 0.04472332 Acc: 0.98504922\n",
      "New best model found!\n",
      "New record loss: 0.04472332109855221, previous record loss: 0.045076908787307\n",
      "New record loss is SAVED: 0.04472332109855221\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.13317381 Acc: 0.95034906\n",
      "test Loss: 0.04698305 Acc: 0.98216268\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.13297370 Acc: 0.95185702\n",
      "test Loss: 0.04346557 Acc: 0.98497521\n",
      "New best model found!\n",
      "New record loss: 0.043465573452184235, previous record loss: 0.04472332109855221\n",
      "New record loss is SAVED: 0.043465573452184235\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.13394809 Acc: 0.95107512\n",
      "test Loss: 0.04298621 Acc: 0.98541929\n",
      "New best model found!\n",
      "New record loss: 0.0429862143597153, previous record loss: 0.043465573452184235\n",
      "New record loss is SAVED: 0.0429862143597153\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.13394189 Acc: 0.95046077\n",
      "test Loss: 0.04192492 Acc: 0.98534527\n",
      "New best model found!\n",
      "New record loss: 0.04192492100495293, previous record loss: 0.0429862143597153\n",
      "New record loss is SAVED: 0.04192492100495293\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.13351896 Acc: 0.94912036\n",
      "test Loss: 0.04300644 Acc: 0.98445711\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.12614220 Acc: 0.95375593\n",
      "test Loss: 0.04031441 Acc: 0.98615943\n",
      "New best model found!\n",
      "New record loss: 0.0403144070319597, previous record loss: 0.04192492100495293\n",
      "New record loss is SAVED: 0.0403144070319597\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.13088452 Acc: 0.95118682\n",
      "test Loss: 0.04000589 Acc: 0.98689956\n",
      "New best model found!\n",
      "New record loss: 0.0400058850575365, previous record loss: 0.0403144070319597\n",
      "New record loss is SAVED: 0.0400058850575365\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.12659412 Acc: 0.95358838\n",
      "test Loss: 0.03853792 Acc: 0.98682555\n",
      "New best model found!\n",
      "New record loss: 0.03853791545710928, previous record loss: 0.0400058850575365\n",
      "New record loss is SAVED: 0.03853791545710928\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.12929162 Acc: 0.95196872\n",
      "test Loss: 0.03854663 Acc: 0.98726963\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.13035407 Acc: 0.95062832\n",
      "test Loss: 0.03681082 Acc: 0.98726963\n",
      "New best model found!\n",
      "New record loss: 0.03681081819463358, previous record loss: 0.03853791545710928\n",
      "New record loss is SAVED: 0.03681081819463358\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.12280189 Acc: 0.95476124\n",
      "test Loss: 0.03784025 Acc: 0.98689956\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.12562156 Acc: 0.95286233\n",
      "test Loss: 0.03516417 Acc: 0.98793576\n",
      "New best model found!\n",
      "New record loss: 0.03516416675128204, previous record loss: 0.03681081819463358\n",
      "New record loss is SAVED: 0.03516416675128204\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.12299162 Acc: 0.95386763\n",
      "test Loss: 0.03451334 Acc: 0.98808378\n",
      "New best model found!\n",
      "New record loss: 0.03451334142957654, previous record loss: 0.03516416675128204\n",
      "New record loss is SAVED: 0.03451334142957654\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.12811519 Acc: 0.95168947\n",
      "test Loss: 0.03514491 Acc: 0.98778773\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.12438093 Acc: 0.95191287\n",
      "test Loss: 0.03422608 Acc: 0.98837984\n",
      "New best model found!\n",
      "New record loss: 0.034226081448901616, previous record loss: 0.03451334142957654\n",
      "New record loss is SAVED: 0.034226081448901616\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.12615321 Acc: 0.95208042\n",
      "test Loss: 0.03464522 Acc: 0.98830582\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.12579988 Acc: 0.95219213\n",
      "test Loss: 0.03472326 Acc: 0.98800977\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.12230835 Acc: 0.95336498\n",
      "test Loss: 0.03427491 Acc: 0.98867589\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.12191116 Acc: 0.95509634\n",
      "test Loss: 0.03430106 Acc: 0.98837984\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.12304267 Acc: 0.95548729\n",
      "test Loss: 0.03402518 Acc: 0.98926800\n",
      "New best model found!\n",
      "New record loss: 0.034025183347314455, previous record loss: 0.034226081448901616\n",
      "New record loss is SAVED: 0.034025183347314455\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.12744230 Acc: 0.95347668\n",
      "test Loss: 0.03356203 Acc: 0.98904596\n",
      "New best model found!\n",
      "New record loss: 0.03356203377874402, previous record loss: 0.034025183347314455\n",
      "New record loss is SAVED: 0.03356203377874402\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.12300775 Acc: 0.95208042\n",
      "test Loss: 0.03385245 Acc: 0.98874991\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.12613253 Acc: 0.95347668\n",
      "test Loss: 0.03355202 Acc: 0.98897195\n",
      "New best model found!\n",
      "New record loss: 0.03355202058333073, previous record loss: 0.03356203377874402\n",
      "New record loss is SAVED: 0.03355202058333073\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.12444279 Acc: 0.95319743\n",
      "test Loss: 0.03335591 Acc: 0.98941603\n",
      "New best model found!\n",
      "New record loss: 0.03335590759615116, previous record loss: 0.03355202058333073\n",
      "New record loss is SAVED: 0.03335590759615116\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.12377923 Acc: 0.95381178\n",
      "test Loss: 0.03347543 Acc: 0.98852787\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.12277123 Acc: 0.95291818\n",
      "test Loss: 0.03397896 Acc: 0.98904596\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.12619168 Acc: 0.95543144\n",
      "test Loss: 0.03369733 Acc: 0.98837984\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.12955826 Acc: 0.95196872\n",
      "test Loss: 0.03408353 Acc: 0.98823181\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.12639487 Acc: 0.95353253\n",
      "test Loss: 0.03332654 Acc: 0.98882392\n",
      "New best model found!\n",
      "New record loss: 0.03332654143268029, previous record loss: 0.03335590759615116\n",
      "New record loss is SAVED: 0.03332654143268029\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.13044495 Acc: 0.95090757\n",
      "test Loss: 0.03349298 Acc: 0.98919399\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.12658628 Acc: 0.95124267\n",
      "test Loss: 0.03351531 Acc: 0.98897195\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.12111425 Acc: 0.95537559\n",
      "test Loss: 0.03268098 Acc: 0.98934202\n",
      "New best model found!\n",
      "New record loss: 0.03268097988216437, previous record loss: 0.03332654143268029\n",
      "New record loss is SAVED: 0.03268097988216437\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.12538896 Acc: 0.95258308\n",
      "test Loss: 0.03291722 Acc: 0.98919399\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.12214223 Acc: 0.95370008\n",
      "test Loss: 0.03405681 Acc: 0.98823181\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.12311324 Acc: 0.95610165\n",
      "test Loss: 0.03318865 Acc: 0.98926800\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.12530163 Acc: 0.95263893\n",
      "test Loss: 0.03345106 Acc: 0.98874991\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.12621815 Acc: 0.95258308\n",
      "test Loss: 0.03317080 Acc: 0.98904596\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.12229483 Acc: 0.95481709\n",
      "test Loss: 0.03322136 Acc: 0.98874991\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.12403371 Acc: 0.95330913\n",
      "test Loss: 0.03359948 Acc: 0.98823181\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.12304868 Acc: 0.95543144\n",
      "test Loss: 0.03246316 Acc: 0.98986011\n",
      "New best model found!\n",
      "New record loss: 0.03246315562785065, previous record loss: 0.03268097988216437\n",
      "New record loss is SAVED: 0.03246315562785065\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.12513499 Acc: 0.95297403\n",
      "test Loss: 0.03280346 Acc: 0.98941603\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.12692385 Acc: 0.95358838\n",
      "test Loss: 0.03286589 Acc: 0.98934202\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.12136718 Acc: 0.95476124\n",
      "test Loss: 0.03263794 Acc: 0.98956406\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.12438741 Acc: 0.95336498\n",
      "test Loss: 0.03320940 Acc: 0.98845385\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.12336918 Acc: 0.95414689\n",
      "test Loss: 0.03221402 Acc: 0.98971209\n",
      "New best model found!\n",
      "New record loss: 0.032214020241370926, previous record loss: 0.03246315562785065\n",
      "New record loss is SAVED: 0.032214020241370926\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.12435939 Acc: 0.95336498\n",
      "test Loss: 0.03289104 Acc: 0.98882392\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.12516177 Acc: 0.95498464\n",
      "test Loss: 0.03244708 Acc: 0.98926800\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.12647710 Acc: 0.95358838\n",
      "test Loss: 0.03337341 Acc: 0.98889794\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.12650060 Acc: 0.95252723\n",
      "test Loss: 0.03226335 Acc: 0.98949005\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.12086592 Acc: 0.95515219\n",
      "test Loss: 0.03375780 Acc: 0.98874991\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.12455295 Acc: 0.95258308\n",
      "test Loss: 0.03324227 Acc: 0.98882392\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 157, in pil_loader\n    return img.convert('RGB')\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/Image.py\", line 934, in convert\n    self.load()\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/ImageFile.py\", line 272, in load\n    raise_ioerror(err_code)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/ImageFile.py\", line 58, in raise_ioerror\n    raise IOError(message + \" when reading image file\")\nOSError: unrecognized data stream contents when reading image file\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3b6c1da7f9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                  \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                  \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                                  \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECK_POINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                                                  ) \n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ea80bc396f0a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, checkpoint)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CV/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 157, in pil_loader\n    return img.convert('RGB')\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/Image.py\", line 934, in convert\n    self.load()\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/ImageFile.py\", line 272, in load\n    raise_ioerror(err_code)\n  File \"/home/linh/.conda/envs/CV/lib/python3.6/site-packages/PIL/ImageFile.py\", line 58, in raise_ioerror\n    raise IOError(message + \" when reading image file\")\nOSError: unrecognized data stream contents when reading image file\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights/EfficientNet_B3_AP_Covid-19_V3.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
