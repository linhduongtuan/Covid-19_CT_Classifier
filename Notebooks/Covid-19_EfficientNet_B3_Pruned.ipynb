{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dEaGqShwJKT"
   },
   "source": [
    "# Trong đại dịch Covid-19 có nguồn gốc từ Wuhan, Trung Quốc đã làm ảnh hưởng tới cuộc sống của nhân loại, cướp đi sinh mạng của ít nhất 200.000 người vô tội và sẽ còn tiếp tục tăng trong thời gian tới.\n",
    "## Để phục vụ công tác chẩn đoán bệnh, các nhà khoa học đã tìm cách áp dụng trí thông minh nhân tạo vào trong việc xử lí và chẩn đoán ảnh CT và X quang chụp phổi để đánh giá tổn thương và phân loại viêm phổi do các nguyên nhân khác nhau, trong đó có Covid-19.\n",
    "## Trong bài này, mình sử dụng dataset tại đây: https://covidresearch.ai/datasets/dataset?id=2. Theo như tìm hiểu về cơ sở dữ liệu này, có lẽ nó được tiếp thu từ 2 nghiên cứu trước đó là bài báo này https://arxiv.org/abs/2003.11597 (địa chỉ github: https://github.com/ieee8023/covid-chestxray-dataset) và bài báo này https://arxiv.org/abs/2003.09871 (https://github.com/lindawangg/COVID-Net).\n",
    "## Gần đây có bài báo công bố sử dụng mạng EfficientNet (bài báo về mạng tại đây https://arxiv.org/pdf/1905.11946.pdf) để chẩn đoán dataset này cho kết quả có độ nhạy và độ đặc hiệu cao hơn hẳn các kết quả trước đó. Các bạn có thể tham khảo bài báo này tại đây: https://arxiv.org/pdf/2004.05717.pdf. Kết quả bài báo chỉ ra rằng họ đã thêm vào mạng EfficientNet_B0 một số lớp để cải thiện khả năng phân loại. Tuy nhiên bài báo sử dụng Framework là Keras, còn trong bài lặp lại thí nghiệm này, mình sử dụng Framework là PyTorch với đóng góp rất lớn của anh Ross Wightman khi xây dựng các mạng thần kinh tích chập sâu cho công việc phân loại ảnh (các bạn có thể tham khảo code tại đây https://github.com/rwightman/pytorch-image-models).\n",
    "### Bên cạnh việc sử dụng Framework khác với bài báo gốc, mình cũng có 1 số thay đổi như mình dùng hàm tối ưu là SGD thay vì ADAM, và mình bổ thêm kĩ thuật Augmentation (ở đây mình dùng thêm kĩ thuật RandAugmentation tại bài báo này https://arxiv.org/abs/1909.13719) để nâng cao độ chính xác.\n",
    "### Mình cũng đã thử huấn luyện dataset này với các mạng khác nhau, tuy nhiên kết quả phân loại có lẽ vẫn hiệu quả nhất với mạng EfficientNet_B0.\n",
    "### Tuy nhiên, để mô hình này có thể sử dụng trong thực tiễn, chắc chắn cần phải tiến hành internal và external validity qua nhiều bước khác nhau. Thêm vào đó, chúng ta hoàn toàn có thể nghĩ đến kĩ thuật ensemble voting để tăng tính chính xác cho công cụ chẩn đoán!\n",
    "# For fun, mình xây dựng thử nền tảng web dùng cho chẩn đoán các ảnh X quang vùng ngực xem bệnh nhân có nhiễm Covid-19 hay không. Các bạn có thể tham khảo tại địa chỉ github của minh [https://github.com/linhduongtuan/Covid-19_Xray_Classifier/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40178,
     "status": "ok",
     "timestamp": 1588213047201,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "rPwL9bdoBNzQ",
    "outputId": "553f83f0-cbf1-48d5-a184-4f4c8ff055ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import fnmatch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from timm.models.layers.activations import *\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from randaugment import RandAugment, ImageNetPolicy, Cutout\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179460,
     "status": "ok",
     "timestamp": 1588213186502,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "yyGpxuktB96O",
    "outputId": "584ea32f-dbe1-4465-8e60-e0f4e5c96a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'normal', 'pneumonia']\n",
      "{'train': 13511, 'test': 1489}\n",
      "cuda:0\n",
      "{0: 'COVID-19', 1: 'normal', 2: 'pneumonia'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 300, 300])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/linh/Downloads/Covid-19'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/test'\n",
    "\n",
    "# Define your transforms for the training and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(300),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        ImageNetPolicy(),\n",
    "        Cutout(size=16),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(340),\n",
    "        transforms.CenterCrop(300),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "batch_size = 50\n",
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
    "              for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "print(dataset_sizes)\n",
    "print(device)\n",
    "\n",
    "### we get the class_to_index in the data_Set but what we really need is the cat_to_names  so we will create\n",
    "_ = image_datasets['train'].class_to_idx\n",
    "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
    "print(cat_to_name)\n",
    "    \n",
    "# Run this to test the data loader\n",
    "images, labels = next(iter(data_loader['test']))\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226470,
     "status": "ok",
     "timestamp": 1588213233519,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "N350JAHpu8c3",
    "outputId": "96a2d095-f78f-4ca5-eb0c-c5390e367831"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def showimage(data_loader, number_images, cat_to_name):\\n    dataiter = iter(data_loader)\\n    images, labels = dataiter.next()\\n    images = images.numpy() # convert images to numpy for display\\n    # plot the images in the batch, along with the corresponding labels\\n    fig = plt.figure(figsize=(number_images, 4))\\n    for idx in np.arange(number_images):\\n        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\\n        img = np.transpose(images[idx])\\n        plt.imshow(img)\\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\\n        \\n#### to show some  images\\nshowimage(data_loader['test'], 20, cat_to_name)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def showimage(data_loader, number_images, cat_to_name):\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy() # convert images to numpy for display\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(number_images, 4))\n",
    "    for idx in np.arange(number_images):\n",
    "        ax = fig.add_subplot(2, number_images/2, idx+1, xticks=[], yticks=[])\n",
    "        img = np.transpose(images[idx])\n",
    "        plt.imshow(img)\n",
    "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
    "        \n",
    "#### to show some  images\n",
    "showimage(data_loader['test'], 20, cat_to_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226461,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "L9jdFtBjSAE6",
    "outputId": "f0f393c5-4369-422c-9aef-fc290ccc941d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('efficientnet_b3_pruned', pretrained=True)\n",
    "#model.fc #show fully connected layer for ResNet family\n",
    "model.classifier #show the classifier layer (fully connected layer) for EfficientNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226454,
     "status": "ok",
     "timestamp": 1588213233520,
     "user": {
      "displayName": "DUONG TUAN LINH",
      "photoUrl": "",
      "userId": "10844282398210252241"
     },
     "user_tz": -420
    },
    "id": "w6QP4CFPBNzg",
    "outputId": "6beb0600-5fdf-4ae6-a216-40c32a13bb9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11973863\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# define `classifier` for ResNet\n",
    "# Otherwise, define `fc` for EfficientNet family \n",
    "#because the definition of the full connection/classifier of 2 CNN families is differnt\n",
    "fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(1536, 1000, bias=True)),\n",
    "\t\t\t\t\t\t\t     ('BN1', nn.BatchNorm2d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('dropout1', nn.Dropout(0.7)),\n",
    "                                 ('fc2', nn.Linear(1000, 512)),\n",
    "\t\t\t\t\t\t\t\t ('BN2', nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t\t ('swish1', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('dropout2', nn.Dropout(0.5)),\n",
    "\t\t\t\t\t\t\t\t ('fc3', nn.Linear(512, 128)),\n",
    "\t\t\t\t\t\t\t\t ('BN3', nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
    "\t\t\t\t\t\t\t     ('swish2', Swish()),\n",
    "\t\t\t\t\t\t\t\t ('fc4', nn.Linear(128, 3)),\n",
    "\t\t\t\t\t\t\t\t ('output', nn.Softmax(dim=1))\n",
    "\t\t\t\t\t\t\t ]))\n",
    "# connect base model (EfficientNet_B0) with modified classifier layer\n",
    "model.fc = fc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Nadam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.01,momentum=0.9,\n",
    "                      nesterov=True,\n",
    "                      weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#show our model architechture and send to GPU\n",
    "model.to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count = count_parameters(model)\n",
    "print(\"The number of parameters of the model is:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPNx-TodPpVA"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i % 1000 == 999:\n",
    "                    print('[%d, %d] loss: %.8f' % \n",
    "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':                \n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                print(f'New best model found!')\n",
    "                print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'best_val_loss': best_loss,\n",
    "                            'best_val_accuracy': best_acc,\n",
    "                            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "                            }, \n",
    "                            CHECK_POINT_PATH\n",
    "                            )\n",
    "                print(f'New record loss is SAVED: {epoch_loss}')\n",
    "\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vcXkJFOlP4NJ",
    "outputId": "e47fadb8-c292-4051-8a56-bbdc5868abe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.57924735 Acc: 0.80541781\n",
      "test Loss: 0.40812838 Acc: 0.87172599\n",
      "New best model found!\n",
      "New record loss: 0.408128382336621, previous record loss: inf\n",
      "New record loss is SAVED: 0.408128382336621\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.33930197 Acc: 0.86840352\n",
      "test Loss: 0.19955631 Acc: 0.92813969\n",
      "New best model found!\n",
      "New record loss: 0.19955631019766495, previous record loss: 0.408128382336621\n",
      "New record loss is SAVED: 0.19955631019766495\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.31696220 Acc: 0.87869144\n",
      "test Loss: 0.17098053 Acc: 0.94157152\n",
      "New best model found!\n",
      "New record loss: 0.1709805269312186, previous record loss: 0.19955631019766495\n",
      "New record loss is SAVED: 0.1709805269312186\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.29617005 Acc: 0.88616683\n",
      "test Loss: 0.16119386 Acc: 0.95231699\n",
      "New best model found!\n",
      "New record loss: 0.16119386051588205, previous record loss: 0.1709805269312186\n",
      "New record loss is SAVED: 0.16119386051588205\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.28257061 Acc: 0.89164385\n",
      "test Loss: 0.16536717 Acc: 0.94224312\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.27791956 Acc: 0.89423433\n",
      "test Loss: 0.21531906 Acc: 0.93351242\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.25937604 Acc: 0.90378210\n",
      "test Loss: 0.22204798 Acc: 0.92411014\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.25473108 Acc: 0.90148768\n",
      "test Loss: 0.15394498 Acc: 0.94492948\n",
      "New best model found!\n",
      "New record loss: 0.1539449754269272, previous record loss: 0.16119386051588205\n",
      "New record loss is SAVED: 0.1539449754269272\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.25564970 Acc: 0.90474428\n",
      "test Loss: 0.16254352 Acc: 0.95298858\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.24834712 Acc: 0.90740878\n",
      "test Loss: 0.14945178 Acc: 0.94694426\n",
      "New best model found!\n",
      "New record loss: 0.14945177616310087, previous record loss: 0.1539449754269272\n",
      "New record loss is SAVED: 0.14945177616310087\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.24022803 Acc: 0.90859300\n",
      "test Loss: 0.15025187 Acc: 0.94291471\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.24375595 Acc: 0.90681667\n",
      "test Loss: 0.15093249 Acc: 0.94761585\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.24040652 Acc: 0.91125749\n",
      "test Loss: 0.13426765 Acc: 0.95097381\n",
      "New best model found!\n",
      "New record loss: 0.1342676539321807, previous record loss: 0.14945177616310087\n",
      "New record loss is SAVED: 0.1342676539321807\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.23641582 Acc: 0.91022130\n",
      "test Loss: 0.16023769 Acc: 0.94828744\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.23255037 Acc: 0.91214566\n",
      "test Loss: 0.19960015 Acc: 0.93955675\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.23295515 Acc: 0.91118348\n",
      "test Loss: 0.14814395 Acc: 0.95097381\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.22582449 Acc: 0.91606839\n",
      "test Loss: 0.15714936 Acc: 0.95366017\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.22732210 Acc: 0.91266376\n",
      "test Loss: 0.15677509 Acc: 0.94895903\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.22066860 Acc: 0.91762268\n",
      "test Loss: 0.15877416 Acc: 0.94022834\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.22419868 Acc: 0.91695655\n",
      "test Loss: 0.12577317 Acc: 0.95366017\n",
      "New best model found!\n",
      "New record loss: 0.12577317180731218, previous record loss: 0.1342676539321807\n",
      "New record loss is SAVED: 0.12577317180731218\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.21936575 Acc: 0.91791873\n",
      "test Loss: 0.13447650 Acc: 0.95030222\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.21776287 Acc: 0.91673451\n",
      "test Loss: 0.14579945 Acc: 0.94761585\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.21979620 Acc: 0.91577233\n",
      "test Loss: 0.17460104 Acc: 0.93888516\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.22490785 Acc: 0.91414403\n",
      "test Loss: 0.14001162 Acc: 0.95097381\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.21538188 Acc: 0.91969506\n",
      "test Loss: 0.16788269 Acc: 0.94627267\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.21975182 Acc: 0.91673451\n",
      "test Loss: 0.18829512 Acc: 0.92881128\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.21706023 Acc: 0.91828880\n",
      "test Loss: 0.15195335 Acc: 0.95231699\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.21263240 Acc: 0.91828880\n",
      "test Loss: 0.12820408 Acc: 0.95366017\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.20454792 Acc: 0.92354378\n",
      "test Loss: 0.12709024 Acc: 0.95433177\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.20861270 Acc: 0.92206350\n",
      "test Loss: 0.12156597 Acc: 0.95634654\n",
      "New best model found!\n",
      "New record loss: 0.12156596933938738, previous record loss: 0.12577317180731218\n",
      "New record loss is SAVED: 0.12156596933938738\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.18919229 Acc: 0.92805862\n",
      "test Loss: 0.11279998 Acc: 0.95567495\n",
      "New best model found!\n",
      "New record loss: 0.11279998249590116, previous record loss: 0.12156596933938738\n",
      "New record loss is SAVED: 0.11279998249590116\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.18251123 Acc: 0.93249944\n",
      "test Loss: 0.11414724 Acc: 0.96104768\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.17588253 Acc: 0.93775442\n",
      "test Loss: 0.10872635 Acc: 0.96239087\n",
      "New best model found!\n",
      "New record loss: 0.1087263524632393, previous record loss: 0.11279998249590116\n",
      "New record loss is SAVED: 0.1087263524632393\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.17465396 Acc: 0.93671823\n",
      "test Loss: 0.11006691 Acc: 0.95836132\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.16863864 Acc: 0.93856857\n",
      "test Loss: 0.11316766 Acc: 0.95500336\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.17353066 Acc: 0.93531197\n",
      "test Loss: 0.11203033 Acc: 0.95970450\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.16833871 Acc: 0.93679224\n",
      "test Loss: 0.10937017 Acc: 0.96104768\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.17126771 Acc: 0.93731034\n",
      "test Loss: 0.11210868 Acc: 0.96037609\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.16601642 Acc: 0.94086300\n",
      "test Loss: 0.11203912 Acc: 0.96239087\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.16708618 Acc: 0.93627415\n",
      "test Loss: 0.11247407 Acc: 0.96171927\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.17087561 Acc: 0.93568204\n",
      "test Loss: 0.11068140 Acc: 0.96104768\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.15976141 Acc: 0.94064096\n",
      "test Loss: 0.11419612 Acc: 0.96037609\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.16501004 Acc: 0.94012286\n",
      "test Loss: 0.11186049 Acc: 0.95970450\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.16303914 Acc: 0.93886463\n",
      "test Loss: 0.11138376 Acc: 0.95701813\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.16311030 Acc: 0.94034490\n",
      "test Loss: 0.11337035 Acc: 0.95903291\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.15955107 Acc: 0.94167715\n",
      "test Loss: 0.11184780 Acc: 0.96239087\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.15867708 Acc: 0.93886463\n",
      "test Loss: 0.11390298 Acc: 0.95970450\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.16452737 Acc: 0.93990082\n",
      "test Loss: 0.11490628 Acc: 0.95903291\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.15990902 Acc: 0.93908667\n",
      "test Loss: 0.12222717 Acc: 0.95433177\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.15664595 Acc: 0.94241729\n",
      "test Loss: 0.11953064 Acc: 0.96037609\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.14954950 Acc: 0.94634002\n",
      "test Loss: 0.11290779 Acc: 0.96440564\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.15543725 Acc: 0.94241729\n",
      "test Loss: 0.11252390 Acc: 0.96306246\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.15347919 Acc: 0.94360151\n",
      "test Loss: 0.11852165 Acc: 0.95701813\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.15786912 Acc: 0.94263933\n",
      "test Loss: 0.11627305 Acc: 0.95701813\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.15734815 Acc: 0.94189919\n",
      "test Loss: 0.11624695 Acc: 0.95500336\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.15172336 Acc: 0.94434165\n",
      "test Loss: 0.11258399 Acc: 0.96306246\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.15160507 Acc: 0.94456369\n",
      "test Loss: 0.11662622 Acc: 0.96171927\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = '/home/linh/Downloads/Covid-19/weights/EfficientNet_B3_Pruned_Covid-19.pth'\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")\n",
    "if checkpoint == None:\n",
    "    CHECK_POINT_PATH = CHECK_POINT_PATH\n",
    "model, best_val_loss, best_val_acc = train_model(model,\n",
    "                                                 criterion,\n",
    "                                                 optimizer,\n",
    "                                                 scheduler,\n",
    "                                                 num_epochs = 100,\n",
    "                                                 checkpoint = None #torch.load(CHECK_POINT_PATH)\n",
    "                                                 ) \n",
    "                                                \n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Covid-19_EfficientNet_B0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
